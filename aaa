INTEGRATION, the VLSI journal 93 (2023) 102048
Available online 16 June 2023
0167-9260/Â© 2023 Elsevier B.V. All rights reserved.
Contents lists available at ScienceDirect
Integration, the VLSI Journal
journal homepage: www.elsevier.com/locate/vlsi
AI/ML algorithms and applications in VLSI design and technologyâœ©
Deepthi Amuru a,âˆ—
, Andleeb Zahra a
, Harsha V. Vudumula a
, Pavan K. Cherupally a
,
Sushanth R. Gurram a
, Amir Ahmad b
, Zia Abbas a
a Center for VLSI and Embedded Systems Technology (CVEST), International Institute of Information Technology, Hyderabad
(IIIT-H), Gachibowli, Hyderabad, 500032, India
b College of IT, UAE University, Al Ain, 15551, Abu Dhabi, United Arab Emirates
A R T I C L E I N F O
Keywords:
Artificial intelligence (AI)
Machine learning (ML)
Manufacturing
CMOS
VLSI design
A B S T R A C T
An evident challenge ahead for the integrated circuit (IC) industry is the investigation and development of
methods to reduce the design complexity ensuing from growing process variations and curtail the turnaround
time of chip manufacturing. Conventional methodologies employed for such tasks are largely manual, timeconsuming, and resource-intensive. In contrast, the unique learning strategies of artificial intelligence (AI)
provide numerous exciting automated approaches for handling complex and data-intensive tasks in very-largescale integration (VLSI) design and testing. Employing AI and machine learning (ML) algorithms in VLSI design
and manufacturing reduces the time and effort for understanding and processing the data within and across
different abstraction levels. It, in turn, improves the IC yield and reduces the manufacturing turnaround time.
This paper thoroughly reviews the AI/ML automated approaches introduced in the past toward VLSI design
and manufacturing. Moreover, we discuss the future scope of AI/ML applications to revolutionize the field of
VLSI design, aiming for high-speed, highly intelligent, and efficient implementations.
1. Introduction
A dramatic revolution has been triggered in the field of electronics
by the advent of complementary metalâ€“oxideâ€“semiconductor (CMOS)
transistors in the integrated circuit (IC) industry, leading to the era of
semiconductor devices. Thenceforth, CMOS technology has been the
predominant technology in the field of microelectronics. The number of
transistors fabricated on a single chip has increased exponentially since
the 1960s [1,2]. The continuous downscaling of transistors over many
technological generations has improved the density and performance of
these devices [3], leading to tremendous growth in the microelectronics
industry. The realization of complex digital systems on a single chip
is enabled by modern very-large-scale integration (VLSI) technology.
The rising popularity of portable electronics in recent years has led to
a substantial surge in need for power-efficient designs incorporating
advanced features. Highly advanced and scalable VLSI circuits meet
the ever-increasing demand in the electronics industry. Continuous
device downscaling is one of the major driving forces of IC technology
advancement with improved device performance. Currently, devices
are being scaled down to the sub-3-nm-gate regime and beyond.
Aggressive downscaling of CMOS technology has created many challenges for device engineers and new opportunities. The semiconductor
âœ© A preprint of this article is available at https://doi.org/10.48550/arXiv.2202.10015.
âˆ— Corresponding author.
E-mail addresses: deepthi.amuru@research.iiit.ac.in (D. Amuru), andleeb.zahra@research.iiit.ac.in (A. Zahra), amirahmad01@gmail.com (A. Ahmad),
zia.abbas@iiit.ac.in (Z. Abbas).
process complexity increases as the transistor dimensions decrease. As
we approach atomic dimensions, simple scaling eventually stops. Despite their small size, devices can experience a variety of performance
issues, such as increased leakage [4â€“6], decreased gain, and heightened sensitivity to manufacturing process variations [7]. The profound
increase in process variations significantly impacts the circuit operation, leading to a variable performance in identical-sized transistors. It
further impacts the propagation delay of the circuit, which behaves as
a stochastic random variable, thereby complicating the timing-closure
techniques and strongly affecting the chip yield [8]. Increasing process
variations in the nanometer regime is one of the major causes of parametric yield loss. Multi-gate field-effect transistors (FETs) [9] are more
tolerant to process variations than CMOS transistors. However, their
performance parameters are also affected by aggressive scaling [10,11].
Advanced and affordable design techniques with finer optimization must be adopted in the VLSI design flow to maintain future
performance trends in circuits and systems. The turnaround time of
a chip depends on the performance of electronic design automation
(EDA) tools in overcoming design constraints. The traditional rulebased methodologies in EDA take longer to yield an optimal solution
for the set design constraints. In addition, to a certain level, the
https://doi.org/10.1016/j.vlsi.2023.06.002
Received 26 February 2023; Received in revised form 28 May 2023; Accepted 10 June 2023
Integration 93 (2023) 102048
2
D. Amuru et al.
Fig. 1. Different areas of VLSI Technology reviewed in the paper.
conventional solutions employed for such tasks are largely manual;
thus, they are time-critical and resource intensive, resulting in time-tomarket delays. Moreover, once the data are fed back, it is difficult and
time-consuming for the designers to understand the underlying functionalities, i.e., the root cause of issues, and apply fixes if required. This
difficulty increases under the impact of the process and environmental
variations [7,12].
Artificial intelligence (AI) has provided prominent solutions to
many problems in various fields. The principle of AI is based on human
intelligence, interpreted in such a way that a machine can easily mimic
it and execute tasks of varying complexity. Machine learning (ML) is
a subset of AI. The goals of AI/ML are learning, reasoning, predicting,
and perceiving. AI/ML can quickly identify the trends and patterns in
large volumes of data, enabling users to make relevant decisions. AI/ML
algorithms can handle multidimensional and multivariate data at high
computational speeds. These algorithms continuously gain experience
and improve the accuracy and efficiency of their predictions. Further,
they facilitate decision-making by optimizing the relevant processes.
Considering the numerous advantages of AI/ML algorithms, their applications are endless. Over the last decade, AI/ML strategies have been
extensively applied in VLSI design and technology.
VLSIâ€“computer-aided design (CAD) tools are involved in several
stages of the chip design flow, from design entry to full-custom layouts. Design and performance evaluation of highly complex digital
and analog ICs depends on the CAD toolsâ€™ capability. Advancement
of VLSIâ€“CAD tools is becoming increasingly challenging and complex with the tremendous increase in transistors per chip. Numerous opportunities are available in semiconductor and EDA technology
for developing/incorporating AI/ML solutions to automate processes
at various VLSI design and manufacturing levels for quick convergence [13,14]. These intelligent learning algorithms are steered and
designed to achieve relatively fast turnaround times with efficient,
automated solutions for chip fabrication.
This work thoroughly attempts to summarize the literature on
AI/ML algorithms for VLSI design and modeling at different abstraction
levels. It is the first paper that provides a detailed review encompassing
circuit modeling to system-on-chip (SoC) design, along with physical
design, testing, and manufacturing. We also briefly present the VLSI
design flow and introduction to artificial intelligence for the benefit of
the readers.
We organized the paper as follows. Section 2 briefly discusses
the existing review articles on AI/MLâ€“VLSI. An overview of artificial
intelligence and machine learning and a brief on different steps in
the VLSI design and manufacturing are presented in Sections 3 and 4,
respectively. A detailed survey of AI/ML-CAD-oriented work in circuit
simulation at various abstraction levels (device level, gate level, circuit
level, register-transfer level (RTL), and post-layout simulation) is presented in Section 5. A review of AI/ML algorithms at the architecture
level and SoC level is reported in Sections 6 and 7. A survey of
the learning strategies proposed in physical design and manufacturing
(lithography, reliability analysis, yield prediction, and management)
is discussed in Sections 8 and 9, respectively. The AI/ML approaches
proposed in testing are reported in Section 10. Sources of training data
for AI/ML-VLSI are presented in Section 11, followed by challenges and
opportunities for AI/ML approaches in the field of VLSI in Section 12.
2. Existing reviews
The impact of AI on VLSI design was first demonstrated in 1985
by Robert. S. Kirk [15]. He briefly explained the scope and necessity
for AI techniques in CAD tools at different levels of VLSI design. His
paper included a brief on the existing VLSIâ€“AI tools and stressed the
importance of incorporating the expanded capabilities of AI in CAD
tools. The advantages of incorporating AI in the VLSI design process
and its applications are briefed in [16,17]. Khan et al. [17] focused
on the applications of AI in the IC industry, particularly in expert
systems; different knowledge-based systems, such as design automation
assistant, design advisor by NCR, and REDESIGN, being used in the
VLSI industry. Rapid developments in AI/ML have drawn the attention
of researchers who have made numerous pioneering efforts to design,
develop, and apply learning strategies to VLSI design and manufacturing. The implementation of neural networks (NNs) for digital and
analog VLSI circuits and knowledge-based systems has been reported
in [18]. The scope for the joint optimization of physical design with
data analytics and ML is reviewed in [19].
Many recent applications and opportunities for ML in physical
design are reviewed in [20]. Beerel et al. [21] stated the challenges and
opportunities associated with ML-based algorithms in asynchronous
CAD/VLSI; they proposed the development of an ML-based recommendation tool, called DesignAdvisor, that monitors and records the
actions taken by various designers during the usage of standard RTL,
logic synthesis, and place route tools. The DesignAdvisor chooses the
best action for a given scenario by running powerful training engines.
Subsequently, it is deployed and used by circuit designers to obtain
design recommendations. Overall, the design advisor focuses more on
asynchronous CAD/ML tools. Stratigopoulos et al. reviewed IC testing
by demonstrating various ML techniques in the field of testing and
provided recommendations for future practitioners [22].
Elfadel et al. [23] discussed in detail various ML methods used in the
fields of physical design; yield prediction; failure, power, and thermal
analysis; and analog design. Khailany et al. [24] highlighted the application of ML in chip design. They focused on ML-based approaches
Integration 93 (2023) 102048
3
D. Amuru et al.
in micro-architectural design space exploration, power analysis, VLSI
physical design, and analog design to optimize the prediction speed and
tape-out time. They proposed an AI-driven physical design flow with a
deep reinforcement learning (DRL) optimization loop to automatically
explore the design space for high-quality physical floorplans, timing
constraints, and placements, which can achieve good-quality results,
downstream clock-tree synthesis (CTS), and routing steps.
ML in EDA is currently gaining the attention of researchers and
research communities. Employing ML in IC design and manufacturing
augments the designers by reducing their time and effort in data analysis, optimizing the design flow, and improving time to market [25].
Rapp et al. presented a comprehensive presentation of state of the art
on ML for CAD at different abstract levels [26]. Interestingly, the paper
also presents a meta-study of ML usage in CAD to capture the overall
trend of suitable ML algorithms at various levels of the VLSI cycle. As
per the meta-study, the trend for ML-CAD is shifting toward Physical
design with NN-implementations compared to other abstraction levels
and algorithms. The paper also discusses open challenges while employing ML for CAD, such as the problem of combinatorial optimization,
limited availability of training data, and practical limitations. However,
the reviews and summaries have been presented only for the last five
years, limited to five key conferences and journals. Another survey [27]
summarizes ML-CAD works in a well-tabulated manner covering many
abstraction levels in digital/analog design flow. However, there needed
to be more focus on challenges and future directions. In [28], a comprehensive review of Graphical Neural Networks (GNNs) for EDA is
presented, highlighting the areas of logic synthesis, physical design,
and verification. As graphs are an intuitive way of representing circuits,
netlists, and layout, GNN can easily fit into EDA to solve combinatorial
optimization problems at various levels and improve the QoR (Quality
of Results) [29]. A review of ML achievements in placement and routing
with benchmark results on benchmark ISPD 2015 datasets is presented
in [30].
Recently, a brief review of recent machine learning and deep learning techniques incorporated in analog and digital VLSI, including physical design, is discussed in [31]. VLSI Computer-Aided Design at different abstraction levels from a machine-learning perspective is presented in [32]. In [33], applications, opportunities, and challenges of
reinforcement learning to EDA, mainly macro chip placement, analog transistor sizing, and logic synthesis, are discussed with practical
implementations.
The reviews mentioned above break down to provide a detailed
discussion of the AI/ML approaches proposed in the literature, mainly
covering all the abstraction levels of the digital VLSI design flow. This
review summarizes the literature on AI/ML algorithms for VLSI design
and modeling at different abstraction levels. We also discuss the challenges, opportunities, and scope for incorporating automated learning
strategies at various levels in the semiconductor design flow. The design
abstraction levels covered in this review under different sections are
shown through a dendrogram in Fig. 1. A concise VLSI design flow
with the traditional commercial CAD tools used in the industry and
the surrogate AI/ML techniques proposed by researchers is given in
Fig. 2. Fig. 6 provides a summary of the AI/ML techniques proposed
in the literature for VLSI circuit simulation for estimating circuit performance parameters, such as the transistor characteristics, statistical
static timing analysis (SSTA), leakage power, power consumption, and
post-layout behavior.
In the following sections, we present a brief background of AI/ML
and a brief description of the different stages of the VLSI design flow.
3. Brief on VLSI design flow
A traditional digital IC design flow has many hierarchical levels, as
shown in Fig. 2; the flowchart covers a generalized design flow, including the front-end and back-end of full-custom/semi-custom IC designs.
The design specifications abstractly describe the functionality, interface, and overall architecture of the digital circuit to be designed. They
include block diagrams providing the functional description, timing
specifications, propagation delays, package type required, and design
constraints. They also act as an agreement between the design engineer
and vendor. The architectural design level comprises the systemâ€™s basic
architecture. It includes decisions such as reduced instruction set computing/complex instruction set computing (RISC/CISC) processors and
the number of arithmetic logic units (ALUs) and floating-point units.
The outcome of this level is a micro-architectural specification that
contains the functional descriptions of subsystem units. Architects can
estimate the design performance and power based on such descriptions.
The behavioral design level is the next; it provides the functional
description of the design, often written using Verilog HDL/VHDL. The
behavioral level comprises a high-level description of the functionality,
hiding the underlying implementation details. The timing information
is checked and validated in the next level, i.e., the RTL description (register transfer level). A high-level synthesis (HLS) tool can automatically
convert C/C++-based system specifications to HDL. Alternatively, the
logic synthesis tool produces the netlist, i.e., a gate-level description for
the high-level behavioral description. The logic synthesis tool ensures
that the gate-level netlist meets the timing, area, and power specifications. Logic verification is performed through testbench/simulation.
Formal verification and scan insertion through design for testability
(DFT) are performed at this stage to examine the RTL mapping [34].
Next, system partitioning, which divides large and complex systems
into small modules, is performed, followed by floor planning, placement, and routing. The primary function of the floor planner is to
estimate the required chip area for standard cell/module design implementation and is responsible for improving the design performance.
The place and route tool place the sub-modules, gates, and flip-flops,
followed by CTS (clock tree synthesis) and reset routing. Subsequently,
the routing of each block is performed. After placement and routing,
layout verification is performed to determine if the designed layout
conforms to the electrical/physical design rules and source schematic.
These processes are implemented using tools such as design rule check
(DRC) and electrical rule check (ERC). After the post-layout simulation,
where parasitic resistance and capacitance extraction and verification
are performed, the chip moves to the sign-off stage [35]. GDS-II is the
resultant file sent to the semiconductor foundries for IC fabrication.
IC fabrication involves many advanced and complex physical and
chemical processes that must be performed with utmost precision.
It comprises numerous stages, from wafer preparation to reliability
testing. A detailed description of each stage is presented in [36]. In
brief, silicon crystals are grown and sliced to produce wafers. The
wafers must be polished to near perfection to achieve extremely small
dimensions of VLSI devices. The fabrication process comprises several
steps, including the deposition and diffusion of various materials on the
wafer. The layout data from the GDS-II file is converted into photolithographic masks, one for each layer. The masks define the spaces on the
wafer where certain materials need to be deposited, diffused, or even
removed. During each step, one mask is used. Several dozen masks may
be used to complete the fabrication process. Lithography is the step that
involves mask preparation and verification as well as the definition of
different materials in specific areas of the IC. It is a crucial step during
fabrication and is repeated numerous times at different stages. It is the
step most affected by the downscaling of technology nodes and the
increase in process variations. After the chip is fabricated, the wafer
is diced, and individual chips are separated. Subsequently, each chip is
packaged and tested to validate the design specifications and functional
behavior. Post-silicon validation is the last step in IC manufacturing and
is used to detect and fix bugs in ICs and systems after production [37].
Integration 93 (2023) 102048
4
D. Amuru et al.
Fig. 2. Modern chip design flow.
4. Brief on AI/ML algorithms
In modern times, statistical learning plays a crucial role in nearly
every emerging field of science and technology. The vast amount of
data generated and communicated within each field can be mined for
learning patterns and dependencies among the parameters for future
analyses and predictions. The statistical learning approach can be
applied to solve many real-world problems. AI is a technology that
enables a machine to simulate human behavior. ML and deep learning
are the two main subsets of AI. ML allows a machine to automatically
learn from past data without explicit programming. Deep learning is the
prime subset of ML (Fig. 3(a)). ML includes learning and self-correction
when new data is introduced. ML can handle structured and semistructured data, whereas AI can handle structured, semi-structured, and
unstructured data. ML can be divided into three main types: supervised,
unsupervised, and reinforcement learning. Supervised learning is performed when the output label is present for every element in the given
data. Unsupervised learning is performed when only input variables are
present in the data. The learning that involves data with a few labeled
samples and the rest is unlabeled is referred to as semi-supervised
learning [38].
4.1. Supervised learning
Supervised learning is further divided into two classes: classification and regression. Classification is a form of data analysis that
extracts models describing important data classes. Such models, called
classifiers, predict discrete categorical class labels [39]. In contrast,
regression is used to predict missing or unavailable numerical data
rather than discrete class labels. Regression analysis is a statistical
methodology generally used for the numeric prediction of continuousvalued functions [40]. The term prediction refers to both numeric
and class-label predictions. The classification/regression process can
be viewed as a learning function to predict a mapping of ğ‘Œ = ğ‘“(ğ‘‹)
where ğ‘Œ is a set of output variables for ğ‘‹ input variables. The mapping
function is estimated for predicting the associated class label ğ‘¦ of a
given new tuple ğ‘‹ (Fig. 3(b)). The most considerable drawback of
supervised learning is that it requires a massive amount of unbiased
Integration 93 (2023) 102048
5
D. Amuru et al.
Fig. 3. (a) Overview of Artificial Intelligence techniques (b) Learning function of classification/regression algorithms (c) Deep learning training and prediction.
labeled training data, which is hard to produce in specific applications
such as VLSI. Most popular regression and classification algorithms
include linear, polynomial, and ridge regressions; decision trees (DT);
random forest (RF); support vector machines (SVMs); and ensembled
learning [41,42].
Supervised learning algorithms are employed at various abstraction
levels for different VLSI applications. A few examples are device modeling, variation-aware statistical analysis, static timing analysis [27],
aging-related degradation analysis, performance, and power prediction
at the circuit and chip level [18], and routing congestion estimation
in physical design [21]. We conduct a comprehensive review of these
applications in successive sections.
4.2. Unsupervised learning
In contrast to supervised learning, unsupervised learning does not
require a label for each training tuple. Hence, it requires less effort
to generate the data than supervised learning. However, point estimates/desired output for a required input vector is harder to achieve
with unsupervised learning. It is employed to identify unknown patterns in the data. Clustering and dimensionality reduction through
principal component analysis and other methods are powerful applications associated with unsupervised learning. Clustering involves
grouping or segmenting objects into subsets or â€˜â€˜clustersâ€™â€™ such that the
objects in each cluster are more closely related to one another than to
the objects of different clusters. For a more detailed discussion, refer
to [43]. Common clustering algorithms include K-nearest neighbors
(KNN), K-means clustering, hierarchical Clustering, and agglomerative
clustering [44].
VLSI applications in unsupervised learning include pattern recognition and clustering for analyzing the system performance [23], logic
synthesis [28], automatic data path extraction, clock tree synthesis, hotspot pattern detection in lithography masks [20], reliability analysis,
scan chain diagnosis [22], etc.
4.3. Semi-supervised learning
Semi-supervised learning acts as a bridge between supervised and
unsupervised methodologies. It is useful when training data has limited labeled samples and a large set of unlabeled samples. It works
effectively to automate data labeling. It works better than supervised/unsupervised learning alone in some applications. The training
starts with limited labeled data and then applies algorithms to model
the unlabeled dataset with pseudo labels in the next step. Then, the
labeled data is linked with pseudo labels and later with unlabeled data
to improve the accuracy [45,46]. However, many efforts are needed
to converge both parts of the semi-supervised methodology in certain
complex applications.
One of the best examples of semi-supervised learning in VLSI is
hotspot detection during lithography [21], as it produces desired results
with less labeled training data, which is a critical requirement in
physical design.
4.4. Reinforcement learning
Reinforcement learning is an area of machine learning that maps
the situations to actions to maximize a numerical reward signal; it is
focused on goal-directed learning based on interactions [47]. It does
not rely on examples of correct behavior as in the case of supervised
learning or does not try to find a hidden pattern as in unsupervised
learning. Reinforcement learning is trying to learn from experience
and find an optimum solution that maximizes a reward signal. Design,
placement, and routing path optimization [26], automated floor planning and efficient test pattern generation in ICs based on recommender
systems save enormous time and human effort and improve overall
performance.
4.5. Deep learning
Deep learning is a subset of ML and is particularly suitable for bigdata processing. Deep learning enables the computer to build complex
concepts from simple concepts [48]. A feed-forward network or multilayer perceptron (MLP) is an essential example of a deep learning model
or artificial neural network (ANN) (Fig. 3(c)). An MLP is a mathematical function mapping a set of input and output values. The function is
formed by composing many simple functions. A shallow neural network
(SNN) is an NN with one or two hidden layers. A network with tens to
hundreds of such layers is called a deep neural network (DNN). DNNs
extract features layer by layer and combines low-level features to form
high-level features; thus, they can be used to find distributed expressions of data [49]. Compared with shallow neural networks (SNNs),
DNNs have better feature expression and the ability to model complex
mapping. Frequently used DNNs include deep belief networks, stacked
autoencoder (SAE), and deep convolution NNs (DCNNs) [48]. Recently,
DNNs have revolutionized the field of computer vision. DCNNs are
suitable for computer vision tasks [50]. Other popular deep learning
techniques include recurrent NNs (RNNs) [51]; generative adversarial
networks (GANs) [52,53]; and DRL (deep reinforcement learning) [54].
Refer to the research works mentioned in Fig. 2 for implementation
details of these algorithms.
Automated latent feature extraction through autoencoders, CNNs,
or other deep learning architectures, dimensionality reduction while
analyzing the lithography masks, and generative models for artificial
Integration 93 (2023) 102048
6
D. Amuru et al.
data generation [33] are some of the prominent applications of deep
learning in VLSI.
Rapid development in several fields of AI/ML is increasing the scope
for solution creation to address many divergent problems associated
with IC design and manufacturing. In the following sections, we discuss
the applications of AI/ML at different abstraction levels of VLSI design
and analysis, starting with circuit simulation.
5. AI at the circuit simulation
Simulation plays a vital role in IC device modeling. Performance
evaluation of designed circuits through simulations is becoming quite
challenging in the nanometer regime due to increasing process and
environmental variations [55â€“57]. The ability to discover functional
and electrical performance variations early in the design cycle can
improve the IC yield, which depends on the simulation toolsâ€™ capability.
By assimilating the automated learning capabilities offered by AI/ML
algorithms in E-CAD tools, the turnaround time and performance of
the chip can be revamped with reduced design effort. Researchers have
proposed surrogate methodologies targeting the characterization of the
leakage power, total power, dynamic power, propagation delay, and
IR-drop estimation ranging from stack-level transistor models to the
subsystem level [58]. Different AI/ML algorithms have been explored
for circuit modeling at different abstraction levels, including linear
regression (LR), polynomial regression (PR), response surface modeling
(RSM), SVM, ensembled techniques, Bayes theorem, ANNs, and pattern recognition models [59]. The following subsections describe the
learning strategies proposed in the literature for VLSI device/circuit
characterization at different abstraction levels.
5.1. Device level
Parametric yield estimation of the circuit and device modeling
at the transistor level is the primary focus area at this level. Parametric yield estimation of statistical-aware VLSI circuits is not new;
this process has been evolving along with ML algorithms since the
1980s. Statistical parametric yield estimation was proposed [60] for
determining the overall parametric yield of MOS circuits. Alvarez et al.
and Young et al. proposed a statistical design analysis through a response surface methodology (RSM) for computer-aided VLSI device
design [61,62]. The proposed models have been successfully applied
to optimize the BiCMOS transistor design. RSM has inspired industrial
experimentation since its development in the 1950s. Refer to [63,64]
for a comprehensive review of RSM. Khan et al. [65] proposed the
multivariate polynomial regression (MPR) method for approximating
the early voltage and MOSFET characteristics in saturation; they considered a curve-fitting approach using the least-squares method in MPR
for simplifying the complexity in BSIM3, and BSIM4 equations [66] to
calculate the MOSFET characteristics realistically.
Considering the drastic decrease in the dimensions of technology
nodes, conducting a thorough analysis of the characteristics at the
device level is of utmost necessity. The randomness in the behavior of
transistors due to the inter-die and intra-die variations in the process
causes exponential changes in the device currents, particularly in the
sub-threshold [56]. Statistical sampling techniques are more effective
than conventional corner-based methods for estimating the effect of the
process parameters on the device [67]. The datasets generated from the
statistical sampling techniques are best suited for learning strategies.
The development of AI/ML algorithms for analyzing device parameters
at different technology nodes facilitates the optimization of the device
parameters and estimating the parametric yield at very high computational speeds. Owing to this fact, an ML-based Tikhonov regularization
(TR) approach is implemented to analyze the impact of the process
on ğ‘‰ğ‘‡ ğ» in GaN-based high electron mobility transistors (HEMTs) [68].
In [69], neural network-based variability analysis of ferroelectric fieldeffect transistor (FeFET) with raw data in the form of polarization maps
from the metrology as inputs is proposed. High/low threshold voltage,
on-state current, and sub-threshold slope are sampled as outputs from
the model. The experiments show that ML predictions are 106
times
faster and > 98% accurate compared to TCAD simulations. A hybrid
analytical and deep-learning-assisted MOSFET I-V (currentâ€“voltage)
modeling is proposed in [70]. For modeling the I-V characteristics of a
12 nm gate length GAAFET (Gate-all-around transistor) technology, a
3-layer NN with 18 neurons was employed.
Performance evaluation of FinFET devices and circuits designed
at 7 nm and above is becoming challenging. Accurate estimation of
the reliability of these devices prior to manufacturing is another concern [71]. Identifying the trend in ML applications for device modeling
from RSM to ANNs over the years and noticing the future requirements in advanced technologies, we propose inductive transfer learning [72,73] as a promising technique for investigating the device
behavior in forthcoming technology nodes from the knowledge of
existing technology nodes.
Given a source domain, ğ·ğ‘†
, a corresponding source task, ğ‘‡ğ‘†
, a
target domain, ğ·ğ‘‡
, and a target task, ğ‘‡ğ‘‡
, the objective of transfer
learning is to enable the learning of the target conditional probability
distribution, ğ‘ƒ (ğ‘Œğ‘‡
|ğ‘‹ğ‘‡
) in ğ·ğ‘‡ with the information gained from ğ·ğ‘† and
ğ‘‡ğ‘† where ğ·ğ‘† â‰  ğ·ğ‘‡ or ğ‘‡ğ‘† â‰  ğ‘‡ğ‘‡
. In most cases, a limited number
of labeled target examples are assumed to be available, exponentially
smaller than the number of labeled source examples. Fig. 4 shows the
proposed methodology for developing a learning system using transfer
learning to analyze the behavior of devices in upcoming technology
nodes.
5.2. Gate level
Researchers have explored the application and development of
AI/ML techniques for gate-level circuit design and evaluation. Fig. 5
shows generalized modeling of statistical aware circuit simulation at
the gate level. Down the line, RSM modeling was popular for estimating
process variation effects on the circuit design. Mutlu et al. presented a
detailed analysis of the development of RSMs to estimate the process
variation effects on the circuit design [74]. Basu et al. [75] developed
a library of statistical intra-gate variation tolerant cells by building
RSM-based gate-delay models with reduced dimensions; the developed,
optimized standard cells can be used for chip-level optimization to realize the timing of critical paths. In [76,77] RSM learning models were
developed via a combination of statistical design of experiment (DoE)
and an automatic selection algorithm for the SSTA of the gate-level
library-cell characterization of VLSI circuits. Their models considered
the threshold voltage (ğ‘‰ğ‘¡â„) and current gain (ğ›½) as model parameters
for a compact transistor model characterization of power, delay, and
output transitions. In [76], the RSM and linear sensitivity approaches
were proposed to increase the analysis speed by one and two orders of
magnitude, respectively, when compared to that of Monte Carlo (MC)
simulations, albeit at the cost of a decrease in accuracy of up to 2% and
7% respectively. In [77], on average, s-DoE has an error of 0.22% at the
tails of 3ğœ distribution compared to the 10x error given by sensitivity
analysis by cadence encounter library characterizer (ELC).
Miranda et al. [78] also proposed a variation-aware statistical design of experiments approach (s-DoE) for predicting the parametric
yield of static random access memory (SRAM) circuits under process
variability. Their approach achieved an accuracy of approximately two
orders of magnitude better than that for the sensitivity analysis in
the tail response under 3ğœ process variations and a CPU time 10â€“100
times less than that in MC simulations. The case studies in the article
demonstrate the advantage of s-DoE in choosing the region of interest
in the distribution to improve accuracy while reducing the number
of simulations. Under similar lines, Chaudhuri et al. [79] developed
accurate RSM-based analytical leakage models for 22 nm shortedgate and independent-gate FinFETs using a central composite rotatable
design to estimate the leakage current in FinFET standard cells by
Integration 93 (2023) 102048
7
D. Amuru et al.
Fig. 4. Block diagram of the proposed inductive transfer learning for device modeling at upcoming lower technology nodes.
considering the process variations. Their results agreed well with the
quasi-MC simulations performed in TCAD using 2D cross-sections.
Exploration of possible patterns in simulated data and reuse of the
data across various stages of circuit design was of great interest. In
this fashion, Cao et al. [80] proposed a robust table-lookup method for
estimating the gate-level circuit leakage power and switching energy of
all possible states using the Bayesian interface (BI) and neural networks
(NNs). Their model uses pattern recognition by classifying the possible
states based on the average power consumption values using NNs. The
idea is centered on using the statistical information on a circuitâ€™s available SPICE power data points to characterize the correlation between
the state-transition patterns and power consumption values of the
circuit. Such correlated pattern information is further utilized to predict
the power consumption of any seen and unforeseen state transition in
the entire state-transition space of the circuit. The estimation errors
obtained using NNs always exhibit normal distributions, with much
smaller variations than benchmark curves. Moreover, the estimation
error decreases with the number of clusters and complexity of the
NNs when appropriate features are extracted. Additionally, the time
required to train and validate the NNs is negligible compared to the
computing time required to generate statistical distributions using the
SPICE environment.
Applying BI, Yu et al. [81] proposed a novel nonlinear analytical
timing model for statistical characterization of the delay and slew
of standard library cells in bulk silicon, SOI technologies, and nonFinFET and FinFET technologies, using a limited combination of output
capacitance, input slew rate, and supply voltage. Utilizing the Bayesian
inference framework, they extract the new timing model parameters
using an ultra-small set of additional timing measurements from the
target technology, achieving a 15Ã— runtime speedup in simulation runs
without compromising accuracy, which is better than the traditional
lookup table approach. They employed ML to develop priors of timing
model coefficients using old libraries and sparse sampling to provide
the additional data points required for building the new library in the
target technology.
Over time, polynomial regression was another important analytical
modeling technique. A statistical leakage estimation through PR was
proposed by [82]. Experimental results on the MCNC benchmark [83]
show that the leakage estimation is five times more efficient than
Wilkinsonâ€™s approach [84] with no accuracy loss in mean estimation
and about 1% in standard deviation. On these lines, Moshrefi et al. [85]
proposed an accurate, low-cost Burr distribution as a function for delay
estimation at varying threshold voltages Â±10% from mean. The samples
are generated at the 90, 45, and 22 nm technology nodes. Statistical
data from MATLAB were applied to HSPICE for simulations to obtain
delay variations. The relation between the threshold voltage and delay
variations was determined as a fourth-order polynomial equation. In
addition to the mean and variance of the estimated distributions, the
maximum likelihood was considered the third parameter, forming a
three-parameter probability density function. The proposed Burr distribution benefits with one more degree of freedom to the normal
distribution [86], and with lower error distribution.
The AI/ML predictive algorithms are intermittently applied for the
processâ€“voltageâ€“temperature (PVT) variation-aware library-cell characterization of digital circuit design and simulation. The accurate performance modeling of digital circuits is becoming difficult with the
acute downscaling of transistor dimensions in the deep sub-micrometer
regime [87,88]. To address the concern regarding the performance
modeling of digital circuits in the sub-micrometer regime, Stillmaker
et al. [89] developed polynomial equations for curve-fitting the measurements of the CMOS circuit delay, power, and energy dissipation
based on HSPICE simulated data using predictive technology models
(PTMs) [90] at technology nodes ranging from 180 nm to 7 nm.
Second-order and third-order polynomial models were developed with
iterative power, delay, and energy measurement experiments, attaining
a coefficient of determination (R2score [91]) of 0.95. The scaling
models proposed in [89,92] are more accurate for comparing devices
at different technology nodes and supply voltages than the classical
scaling methods.
Development of MPR and ANN models for measuring the PVT-aware
(process voltage temperature) leakage in CMOS and FinFET digital logic
cells was reported in [91], and [93] respectively. [91] also models total
power with the same MPR model. The developed models demonstrated
high accuracy with < 1% error w.r.t. the HSPICE simulations. Amuru
et al. [94] reported a PVT-aware estimation of leakage power and
propagation delay with a Gradient boosting algorithm, which yields a
< 1% error in estimations with 104
times improvement in computational
speed compared to HSPICE simulations. These characterized librarycell estimations can be used for estimating the overall leakage power
and propagation delay of complex circuits, avoiding the relatively
long simulation runs of traditional compilers. Bhavesh et al. [95]
propose an estimation of power consumption of the MOSFET-based
digital circuits using regression algorithms. PMOS-based Resistive Load
Inverter (RLI), NMOS-based RLI, and CMOS-based NAND gate layout
are employed at 90 nm MOS technology to create the dataset. The
feature vectors extracted for modeling are capacitance, resistance, number of MOSFET, their respective width and length, and the average
power consumption of the respective layout. As per the experimental
results, Extra tree and polynomial regressors demonstrate better performance over Linear, RF, and DT regressors. GPU-based circuit analysis is
required at the present state of complex-circuit analysis. Recently, XTPRAGGMA, a tool to eliminate false aggressors and accurately predict
crosstalk-induced delta delays using GPU-accelerated dynamic gatelevel simulations and machine learning, is proposed in [96]. It shows
a speedup of 1800x compared to SPICE-based simulations.
Integration 93 (2023) 102048
8
D. Amuru et al.
Fig. 5. Generalized statistical aware modeling for VLSI circuit simulation.
An accurate yield estimation in the early stage of the design cycle
can positively impact the cost and quality of IC manufacturing [97,98].
Comprehensive analysis of VLSI circuitsâ€™ delay and power characteristics being designed at the sub-nanometer scale under expanding PVT
variations is extremely important for parametric yield estimation. As
reported earlier, accurate predictions are made by well-trained AI/ML
algorithms, such as PR, ANNs, GB, and BI, with power and delay
estimations that are very close to those of the most-reliable HSPICE
models. Incorporating such efficient ML models in EDA tools for librarycell characterization at the transistor level and gate level facilitates the
performance evaluation of complex VLSI circuits at very high computational speeds, facilitating the analysis of the yield. These advanced
computing EDA tools drastically improve the turnaround time of the
IC.
5.3. Circuit level
Statistical characterization of VLSI circuits under process variations
is essential for avoiding silicon re-spins. Similar to gate-level, explorations for the design of ML-based surrogate models at the circuit level
were reported in the literature. Hou et al. [99] reported the power
estimation of VLSI circuits using NNs. Trained NNs can estimate the
power using the input/output (I/O) and cell number without requiring
circuit information such as net structures. This approach requires the
power estimation results of benchmark circuits to train the target NN.
Limited experimental results have shown that this method can give
acceptable results with a specific net structure at a considerably high
speed. Stockman et al. [100] discussed a novel approach for predicting
power consumption based on memory activity counters, exploiting
the statistical relationship between power consumption and potential
variables. The proposed ML models for the prediction include support
vector regression (SVR), genetic algorithms, and NNs. They showed that
a NN with two hidden layers and five nodes per layer is the best predictor among the chosen ML models, with a mean square error of 0.047. In
addition, they explained that the ML approaches are significantly less
costly and less complex than a hardware solution, with reduced run
time. Janakiraman et al. [101] proposed an efficient ANN model for
characterizing the voltage- and temperature-aware statistical analysis
of leakage power. Trained transistor-level stack models used for circuit
leakage estimation. The designed model showed 100x improvement in
runtime with < 1% and < 2% error in the mean and standard deviation
of Monte-Carlo statistical leakage estimations. The complexity of the
comprehensive model is reported as ğ‘‚(ğ‘) on par with existing linear
and quadratic models [84,102â€“104].
Garg et al. presented SVM-based macro models for characterizing
transistor stacks of CMOS gates with an average increase in the runtime
of 17Ã— compared to those of the HSPICE computations for estimating
the leakage power [105]. Kahng et al. [106] proposed a hybrid surrogate model that combines the predictions of ANN and SVM models to
estimate the incremental delay due to the signal integrity aware path
delay in a 28-nm FDSOI technology, demonstrating a worst-case error
of < 10 ps. An accurate power estimation of CMOS VLSI circuit using
Random Forest (RF) that performs better than NNs is proposed in [107].
Results show a good agreement with ISCASâ€™89 Benchmark circuits. A
fast and efficient ResNet-based digital circuit optimization framework
for leakage and delay is proposed in [108]. Results on 22 nm Metal
Gate High-K digital cells show 36.7% and 18.8% reduction in delay
and leakage using a genetic algorithm.
5.4. RTL level
The effect of process variability on guard bands and its mitigation
are detailed in [109]. Jiao et al. [110] proposed a supervised-learning
model for the bit-level static timing error prediction modeled at the
RTL level, aiming for a guard-band reduction in error-resilient applications. They considered floating-point pipelined circuits in their
analysis. The circuitâ€™s behavior was characterized by timing errors
using the Synopsys design and Synopsys IC compilers as frontend and
backend design tools, respectively. Synopsys prime-time was used for
voltage and temperature scaling, followed by a post-layout simulation
with the SDF back-annotation in Mentor Graphics ModelSim to extract
the bit-level timing error information. The logistic regression model
shows an average accuracy of 95% at various voltage/temperature
corners and unseen workload, with an average guard-band reduction
of 10%. ML-based power estimation techniques at the RTL level that
outperform commercial RTL tools [111â€“114] were proposed in [115].
Their experiments recommend CNN over ridge regression, gradient
tree boosting, and multi-layer perceptron for accurate power estimations. The average power estimation from the RTL simulations using a
GNN [116], GRANNITE, was presented in [117]. GRANNITE achieved
> 18.7ğ‘‹ speedup when compared to traditional per-cycle gate-level
simulations.
The AI/ML strategies can be extended to the circuit and RTL level to
build macrocell models for parametric yield estimation and optimization. The models built using ANNs, CNNs, and deep learning techniques
Integration 93 (2023) 102048
9
D. Amuru et al.
Fig. 6. Summary of proposed AI/ML algorithms in literature for circuit simulation parameter estimation/performance evaluation.
are helpful for complex cell design optimization and power-delay product prediction as they are less dependent on the complete circuit
description. Another critical bottleneck is the generation of big data for
ML algorithms. ML algorithms require a large amount of simulated data
to accurately develop I/O relationships, which is possible at some levels
of digital circuits and their applications. The concept of GANs can help
address this concern. Generative models aim to estimate the training
dataâ€™s probability distribution and generate samples belonging to the
same data distribution manifold [118]. GAN-based semi-supervised
method architectures for the regression task proposed recently [119]
strengthen the possibilities of applying GAN to the regression tasks of
digital circuits. Different measures and techniques need to be explored
to keep the quantization error introduced by these networks in check.
5.5. Post layout simulation
ML models also facilitate the efficient use of resources in repeated
dynamic IR-drop simulations. The model proposed in [120] reduces
the training time by building small-region models for cell instances
for IR-drop violations instead of building a global model for the entire
chip. Further, ML models work on the regional clusters to extract
the required features and predict the violations. Experiments on validated industry designs show that the XGBoost model outperforms
CNNs for IR-drop prediction, requiring less than 2 min for each ECO
iteration. Zhiyao Xie et al. [121] developed a fast design independent dynamic IR-drop estimation technique named PowerNet based on
CNNs. Design-dependent, ML-based IR-drop estimation techniques are
proposed in [120,122â€“125].
Han et al. [126] proposed an ML-based tool called Golden Timer
eXtension (GTX) for sign-off timing analysis. Using the proposed tool,
they attempted to predict the timing slack between different timing
tools and the correlation between the sign-off tool and implementation tool across multiple technology nodes. The poor yield due to
the inaccurate timing estimation by the STA sign-off, particularly at
nodes below 16 nm at low voltages, can be improved using surrogate
tools, supporting advanced processes for accurate timing calibration.
ML techniques in chip design and manufacturing, notably addressing
the effect of process variations on chip manufacturing at the sub-22-nm
regime, are discussed in [127]. The authors discuss pattern-matching
techniques integrated with ML techniques for pre-silicon HD, postsilicon variation extraction, bug localization, and learning techniques
for post-silicon time tuning. [128] reviews some of the on-chip power
grid design solutions using AI/ML approaches. It thoroughly discusses
Integration 93 (2023) 102048
10
D. Amuru et al.
Power grid analysis using probabilistic, heuristic, and machine-learning
approaches. It further recommends that it is necessary to obtain the
electromigration-aware aging prediction of the power grid networks
during the design phase itself.
Power delivery networks (PDNs) supply low-noise power to the
active components of the ICs. As the supply voltage scaled down, the
variations in power supply voltage increased, affecting the systemâ€™s
performance, especially at higher frequencies. The effects of this power
supply noise can be minimized with a proper design of impedancecontrolled PDN. The probability of system failure increases with the
PDN ratio (The ratio of the actual impedance of the PDN to the
target impedance). It can be minimized by efficiently selecting and
placing decoupling capacitors on the board and/or the package. A fast
ML-based surrogate-assisted meta-heuristic optimization framework for
decoupling capacitor optimization is proposed in [129].
Further, a low-cost machine learning-based chip performance prediction framework using on-chip resources is proposed [130]. It predicts the maximum operating frequency of chips for speed binning with
an accuracy of over 90% w.r.t Automatic Test equipment (ATE). Experimental results on 12 nm industrial chips show that linear regression
is more suitable with less training time and model size than XGBoost.
It also proposes a sensor selection method to minimize the area overhead on on-chip sensors. Sadiqbatcha et al. [131] propose RealMaps,
a framework for real-time estimation of full-chip heatmaps using an
LSTM-NN (Long short-term memory) model with existing embedded
temperature sensors and system-level utilization information. The experiments to identify the dominant spatial features through 2D spatial
DCT (Discrete Cosine Transform) shows that only 36 DCT coefficients
are required to maintain sufficient accuracy. Fig. 6 presents a summary
of the AI/ML algorithms proposed in the literature to address VLSI
circuit simulation.
As reported earlier, AI and ML can be incorporated into EDA tools
and methodologies at various stages of circuit simulation to address different statistical/parameter estimations, including the leakage power,
total power, propagation delay, and effects induced due to aging, yield,
and power consumption. Assimilation of these automated learning
strategies into VLSI circuit design and simulation will revolutionize the
field of CADâ€“VLSI considering the numerous related advantages.
Nevertheless, it is crucial to thoroughly analyze the constraints of
AI/ML models to leverage the advantages of these automated methods
fully. The lack of transparency and interpretability in ML models is a
significant disadvantage. Since many models do not offer insights into
the connections between inputs and outputs or the decision-making
process, it can pose challenges for design engineers to comprehend
the underlying relationships and make adjustments accordingly. This
hampers their ability to iterate through the model to accommodate any
required modifications effectively. Another significant issue revolves
around the reliance of ML model efficacy on extensive data. Generating
substantial data for ML algorithms during VLSI design or manufacturing
remains a challenging task unless the data collection process becomes
automated across various levels of abstraction in the future. Generating
or collecting a significant volume of simulated data is currently feasible
only at certain stages of circuit design. However, artificial data generation techniques such as GANs and autoencoders, along with knowledge
transfer across different nodes and abstraction levels using transfer
learning concepts, can partially address the data requirements. ML
models, especially those using supervised or unsupervised algorithms,
have the potential to generate biased outcomes if the balance between
bias and variance within the dataset is not carefully examined. Hence, it
is crucial to gather and utilize diverse data that encompasses all aspects
of the design. Likewise, overfitting and underfitting are frequent challenges in training ML algorithms. To mitigate these issues, techniques
such as regularization, cross-validation, and ensemble methods can be
employed. Additionally, it is essential to address security vulnerabilities
in AI/ML modeling, as these automated processes are susceptible to
potential attacks. Evaluating and implementing appropriate countermeasures against security threats must be done prior to deploying such
models.
6. AI in architectures
Design of VLSI architectures became dynamic with the evolution of
AI/ML techniques [132,133]. Advances in NN algorithms and innovations in high bandwidth and high-performance semiconductor designs
have paved a new way to address the challenges in hardware implementations of advanced real-time applications. Over the last few
decades, different architectures have inspired the advancement of VLSI
technology. Most design developments/improvements are motivated by
the need for edge applications with high processing speeds, improved
reliability, low implementation cost, and time-to-market windows. The
architectural designs proposed in the literature are majorly for the
application domains of image processing and signal processing, speech
processing, IoT, and automobile.
This survey presents a broad review of VLSI architectural modifications at the memory and systolic array architectures in this section
and at the SoC level in the next section to provide the authors with an
overview and scope of research in VLSI architectures for ML.
6.1. Memory systems
Memory systems are one of the computational systemsâ€™ essential and
dominant components. Different scalable memory architectures have
been designed for the real-time processing of ML algorithms in various
IoT (Internet of Things) and embedded system applications. Various AI
applications involve large datasets and demand a faster interface between the computing unit and memory. Different memory architectures
were proposed in the past, addressing data movement and processing
issues. Kang et al. [134] proposed deep embedding of computation
in an SRAM parallel processing architecture for pattern recognition
in 256 Ã— 256 images; their model enables multi-row read access and
analog signal processing without degrading the system performance.
Their method employs two models: multi-row READ and the analog
sum of absolute difference (SAD) computation. This architecture differs
from conventional architecture as a data path between the processor
and memory is not required. The SAD is computed at different locations
of the array in parallel with multiple windows of the template pattern.
For high-performance computations, Zhang et al. [132] proposed a 6TSRAM array that stores an ML classifier model, which is an ultra-low
energy detector for image classification. The prototype is a 128 Ã— 128
SRAM array that operates at 300 MHz, with an accuracy equivalent to
that of a discrete SRAM/digital-MAC system.
Gonugondla et al. [135] presented a robust, deep-in-memory ML
classifier with a stochastic gradient descent based on an on-chip trainer
using a standard 16 kB 6T-SRAM bit-cell array. In-memory computing
is a technology that uses memory devices assembled in an array to
execute MAC operations [136]. The Multiply-Accumulate (MAC) unit,
which acts as the core of an embedded processor, plays a vital role
in determining its performance. Essentially, the MAC unit performs
the fundamental task of multiplying two numbers and subsequently
adding the product to an accumulator. Kang et al. [133] worked on
deep in-memory architecture (DIMA) as a substitute for the regular
von Neuman architecture for realizing energy and latency-efficient ML
SoCs. This architecture was employed mainly for targeted applications,
such as IoT and autonomous driving, which require computing heavy
ML algorithms. DIMA eliminates the need for separate computation and
memory by implanting the conventional memory periphery with the
computation hardware. The design employs 6T SRAM with a changeless bit-cell structure to maintain the storage density. In [137], MAC
circuit architecture in a 2Tâ€“1C configuration (two MoS2 FETs and one
metalâ€“insulatorâ€“metal capacitor) is the core module for the convolution
operation in an artificial neural network. The memory portion of this
circuit is similar to Dynamic Random Access Memory (DRAM) but with
a longer retention time owing to the ultralow leakage current of the
MoS2 transistors.
Integration 93 (2023) 102048
11
D. Amuru et al.
Wang et al. [138] discussed parallel digital VLSI architecture for
combined SVM training and classification. In this parallel architecture,
a multi-layer system bus and multiple distributed memories fully utilize
parallelism. Before this, many SVMs were developed and discussed
in [139â€“141], primarily focusing on the 90-nm technology node. Distinctively, Wang et al. in [138] developed the SVM on a 45-nm node on
a commercial GPU with an enhanced speedup of 29Ã— compared with
traditional SVMs on digital hardware.
A part of the computation tasks can be performed inside the memory
to solve the data movement issue, thus avoiding the memory access
bottleneck and accelerating the application performance significantly.
Such architectures are processing in-memory (PIM) architectures. [142]
proposes NNPIM, a novel PIM architecture, to accelerate NNâ€™s interface inside the memory. The memory architecture combines crossbar
memory architecture for faster operations, optimization techniques to
improve the NN performance and reduce energy consumption, and
weight sharing mechanism to reduce the computational requirement of
NNs. [143,144] are some of the significant state-of-the-art DRAM PIM
architectures.
Another evolved computing technique is near-memory processing
(NMP). Near-memory processing incorporates the memory and logic
chips in 3D storage packages to provide high bandwidth. Schuiki
et al. [145] proposed a near-memory architecture for training DNNs.
This model was developed for accelerating DNN training instead of
interference. The training engine, NTX, was used to train the DNNs
at scale. They explored the RISC-V cores and NTX coprocessor by
reducing the overhead on the main processor by seven times. The NTX
combined with the RISC-V processor core offers a shared memory space
with single-cycle access on a 128-kB tightly coupled data memory. The
architecture employs a hybrid memory cube as the memory module for
training the DNNs in data centers.
In [146], a general-purpose vector architecture for migration of
ML kernels for near-data processing (NDP) to achieve high speedup
with low energy consumption is presented. Their architecture shows
a speedup of up to 10x for KNN, 11Ã— for MLP, and 3Ã— for convolution
when processing near-data compared to a high-performance Ã— 86 baseline. The work also includes an NDP intrinsics library that supports validating NDP architectures based on large vectors. A machine-learning
framework is proposed in [147] to effectively predict the suitable NSP
system (among an HBM-based (High Bandwidth Memory) NDP system,
an HMC-based (Hybrid Memory Cube) NDP system, and a conventional
DDR4-based system) for a given application based on the rankings in
performance for a given workload. Kaplan et al. worked on K-means
and KNN algorithm evaluation for processing in-storage acceleration
of ML (PRINS) [148], a system employing resistive content addressable
memory (ReCAM). This architecture functions both as a storage and
a massively parallel associative processor. This design works better
than the von Neumann architecture model in managing the bottleneck
between the storage and main memory. These algorithms outperformed
CPU, GPU, and field-programmable gate array (FPGA) in fetching timeaccessing data from the main memory. The ReCAM is more efficient
than traditional CAMs as it implements line-by-line execution of the
truth table of the expression. PRINS enhances the power efficiency and
performance compared to other hardware for both K-means and KNN
evaluation.
A survey on the architectural aspects, dimensions, challenges, and
limitations of In-memory computing processing-in-memory (CIM) is
presented in [149]. A robust and area-efficient CIM approach with
6T foundry bit-cells that has improved dynamic voltage range for dot
product computations, withstanding bit-cell ğ‘‰ğ‘¡ variations, and eliminating any read disturb issues is proposed in [150]. Recent stateof-the-art works on CIM chips are presented in [151]. As per their
research, the SRAM-based CIM solution can be a potential choice for AI
processors than NVM-based (non-volatile memory) CIMs. NVM-based
CIMs or memristive devices include resistive random-access memory
(RRAM), magneto-resistance RAM (MRAM), and phase-change memory
(PCM) [152,153]. A survey on the memristive simulation frameworks,
their comparisons, and future modeling is highlighted in [154].
In the past, Cheng et al. [155] introduced the training-in-memory
architecture for the memristor-based DNN named TIME. It reduced the
computation time of the regular training systems. This architecture
supports not only interference but also backpropagation and update
during the training of the NNs. It is based on metal-oxide resistive
random access memory, which enhances performance and efficiency.
The main module is divided into three subarrays: full-function, buffer,
and memory. The full-function subarray manages the memory and
training operations such as interference, backpropagation, and update.
The memory subarray manages data storage, and the buffer subarray
holds the intermediate data for the full-function subarray. This architecture improves energy efficiency in deep reinforcement learning and
supervised learning.
A thorough survey on hardware accelerators is outside the scope
of this paper. Interested readers can refer to [156], a review of accelerators and similar works [157â€“159]. However, we could provide the
overview of different design aspects at the architecture level to speed
up the ML computations
6.2. Systolic arrays
A systolic array is a subset of the data-flow architecture comprising
several identical cells, with each cell locally connected to its nearest
neighbor. A wavefront of computation is propagated in the array with a
throughput proportional to the I/O bandwidth. Systolic arrays are finegrained and highly concurrent architectures. The progress of IoT-based
smart applications has exponentially increased the demand for deep
learning algorithms and, in turn, systolic array-based architectures.
In these lines, an automatic design space exploration framework
for CNN-based systolic array architecture implementations on an FPGA
under high resource utilization and at higher speeds was proposed
in [160,161]. They utilize analytical models to provide in-depth resource estimation and performance analysis. However, systolic array
implementations on FPGAs are affected much by the sparsity problem of deep neural networks. Researchers earlier worked toward this
problem. An approach of packing sparse convolutional neural networks
into a denser format for efficient implementations using systolic arrays
is proposed in [162]. However, these designs create irregular sparse
models that fail to exploit the data-reuse rate feature of the systolic
array. Structured pruning was introduced in [163,164] to overcome
the problem associated with the data-reuse rate that produces DNNs
compatible with the synchronous and rhythmic flow of data from
memory to the systolic arrays.
Further, [165] propose Eridanus, an approach for structural pruning
the zero-values in sparse DNN models before implementing them on
systolic arrays. The approach examines the correlation among all the
filters to extract the locally-dense blocks, the widths of which match the
width of the target systolic array, thus reducing the sparsity problem.
Similarly, optimization for the systolic array architecture of deep learning accelerators for sparse CNN models on FPGA platforms is necessary
as the zeros in the filter matrix of CNN occupy the computation units
resulting in sub-optimal efficiency. A sparse matrix packing method
with bit-map representation that condenses sparse filters to reduce
the computation required for systolic array accelerators is proposed
in [166].
Many systolic array architectural modifications were proposed in
the literature addressing specific applications. In [167], an MLP training accelerator as a systolic array on Xilinx U50 Alveo FPGA card is
proposed to address the attack detection on a massive amount of traffic
logs in network intrusion detection in a short time. The processing
speed per power consumption was 11.5 times better than the CPU
and 21.4 times better than the GPU. An approximate systolic array
architecture combines timing error prediction and approximate computing to relax the timing constraints of MACs [168]. The proposed
Integration 93 (2023) 102048
12
D. Amuru et al.
array on CIFAR-10 image classification could obtain a 36% energy
reduction with only a 1% accuracy loss. A reconfigurable systolic
ring architecture to reduce on-chip memory requirement and power
consumption [169].
Matrix multiplication is one of the primary computations in most
computing architectures. [170] proposes a novel systolic array based on
factoring and radix-8 multipliers to significantly reduce the area, delay,
and power from the conventional radix-4 design providing the same
functionality. FusedGCN [171], a systolic architecture that computes
the triple matrix multiplication to accelerate graph convolutions. It
supports compressed sparse representations and tiled computations
without losing the regularity of a systolic architecture. Recently, a
hybrid accumulator factored systolic array based on partial factoring of carry propagate adder is proposed [172] with a significant
improvement in area, delay, and power.
The functional safety of the accelerators is another critical concern.
Faults manifested due to manufacturing defects in the data paths of
GPU/TPU accelerated DNNs on systolic arrays may lead to a functional
safety violation. An extensive functional safety assessment of a DNN
accelerator exposed to faults in the data path is presented in [173].
From the directions of the state-of-the-art works, it demands systolic
array architectures that are more flexible, with more data-flow strategies and multiple data transmission modes in the future to handle the
increasing depths of deep neural networks.
To summarize, we discuss two major VLSI architectures in this section â€” memory systems and systolic arrays. Architectural modifications
to SRAM array assembles based on in-memory computing and parallel
architectural implementations to speed up the traditional ML algorithms through PIM, NMP, and NDP, especially to train complex neural
networks, are discussed. Followed by that, deep learning algorithm
implementations as systolic arrays catering to the needs of specific
embedded applications are discussed. The architectural modifications
addressing sparse matrix representations and structural pruning to
improve the data reuse in systolic arrays are discussed.
Both architectures include bulk and fast computations determining
the overall performance of the application. Depending on the application, necessary changes need to be made to these architectures catering
to their specifications.
7. AI at the SOC
Artificial intelligence, more specifically deep learning, is feasible
in most hardware applications due to the advancements in computing
and semiconductor fields. Many attempts have been made to replicate
the human brain in next-generation applications, often referred to as
neuromorphic computing. Several critical modifications are made to
the SoC architectures to incorporate deep-learning capabilities. These
design modifications impact general-purpose SoC designs and specialized systems that include specialized processing technologies with
heterogeneous and massive parallel matrix computations, innovative
memory architectures, and high-speed data connectivity.
AI-SoC models must be compressed to ensure their operation at
constrained memory architectures in mobile, communications, automobile, and IoT edge applications. The model compression is performed
through controlled pruning without compromising accuracy. However,
power, latency, and other areas could be trade-offs. Therefore, the architectural modifications are to be carefully chosen with the combined
efforts on memory and datapath subsystems.
FPGA (Field Programmable Gate Array) is one of the widespread
and commercially available programmable logic devices to accelerate
the computing power of AI on hardware [138,165]. FPGA became a
robust device for hardware accelerators because of its low cost, high
energy efficiency, reusability, and flexibility. ASIC (Application Specific Integrated Circuits) are at their best for implementing specialized
applications.
NNs are biologically inspired and perform parallel computations.
Digital units such as DSP models, floating-point units, ALUs, and highspeed multipliers can be effectively implemented using NN techniques.
The fundamental advantage of NNs for digital applications is that highspeed circuits can be realized efficiently because of the almost constant
operation time, regardless of the increasing number of bits in the
circuit. Exploiting the parallelism in NN computations also provides a
balance between using internal and off-chip memory.
Many ML and deep learning applications were reported in the past
for the performance evaluation of SoCs. Joseph et al. [174] developed
empirical models for processors using LR to characterize the relationship between processor response and micro-architectural parameters.
Lee et al. [175], Yun et al. [176] proposed power estimation models
established via regression analysis for accurate performance prediction
and power of microprocessor applications in the micro-architectural
design space. The model proposed in [175] reduces the simulation cost
with increased profiling efficiency and improved performance by effectively assessing and modeling the sensitivity according to the number
of samples simulated for the model formulation and finding fewer than
4000 sufficient samples from a design space of approximately 22 billion
points. Depending on the application, 50%â€“90% of predictions achieve
error rates of < 10%. The maximum outlier error percent reported is
approximately 20%â€“33%. Wherein hierarchical Clustering is employed
in [176] to determine the best predictors among the ten considered
events. The proposed model shows an average estimation error of approximately 4% between the actual and estimated power consumptions
when applied to an Intel-XScale-architecture-based PXA320 mobile
processor.
An investigation and comparative analysis on the application of Machine Learning algorithms for logic synthesis of incompletely-specified
functions is presented in [177]. Periodic performance monitoring of
SoCs is essential for high-speed and energy-efficient computing systems. However, performance monitoring is dependent on the accurate
sampling of critical paths. These critical paths drastically vary with
PVT conditions, particularly at advanced nodes. Addressing this issue,
Wang et. Al [178] proposes a machine-learning-based SoC real-time
performance monitoring methodology incorporating physical parasitic
characteristics and PVT variations with unknown critical paths.
Several SoC architectures were reported targeting specific applications. MLSoC for multimedia content analysis (implemented in TSMC
90-nm CMOS technology) [179]. Jokic et al. [180] presents a complete end-to-end dual-engine SOC for face analysis that achieves >2X
improvement in energy efficiency compared to the state-of-art systems. The efficiency comes with the hierarchical implementation of
the Binary Decision Tree in the first level and more power-hungry
CNN in the next level, which can be triggered when needed. Machine
efficiency monitoring is significant to achieve high productivity, failure, and cost reduction. An SoC-based tool wear monitoring system
with a combination of signal processing, deep learning, and decision
making is proposed in [181]. The sensor fusion data collected from the
three-axial accelerometer and MEMS microphone, combined with the
measurement of tool flank wear at different scenarios using a camera,
is fed to a CNN to detect any machining variation. Extreme learning
machines (ELMs) are NN architectures to increase computational efficiency and performance for large data processing [182]. A low-cost
real-time neuromorphic hardware system of spiking Extreme Learning
Machine (ELM) with on-chip triplet-based reward-modulated spiketiming-dependent plasticity (R-STDP) learning capability is proposed
in [183].
A thorough timing analysis of an SoC is also essential to meet
the design specifications. In [184], ensemble learning-based timing
analysis in an SoC physical design was performed. Ensemble learning
is a combination of multiple machine learning models to improve
the performance of the base learners. Many floor plan files with different parameter settings, followed by slack time from Synopsys IC
Compiler tool as the label, were used for training supervised learning
Integration 93 (2023) 102048
13
D. Amuru et al.
Fig. 7. Physical design flow.
algorithms. The idea was to feedback on the prediction results at an
early stage to the physical design flow to modify the improper floorplan. Bigram-based multi-voltage aware timing path slack divergence
prediction [185] utilizes the classification and regression tree (CART)
approach. Experimental results show an accuracy of 95 to 97% in
predicting cell delays and endpoint timing slack.
CAD tools capable of delivering industrial-quality chip designs must
be tuned for optimal PPA (performance, power, area). A holistic approach that involves online and offline machine learning approaches
working together for industrial design flow tuning is proposed in [186].
The work highlights SynTunSys (STS), an online system that optimizes
designs and generates data for a recommender system that performs
offline training and recommendation. The work also proposes adaptable
Online & offline Systems for the future that dynamically adapts to the
trials originating from the online-learning algorithm and the recommender system in the due lifespan of the system across various STS
iterations. Addressing the challenges in meeting timing constraints in
modern ICs, Ajirlou et al. [187] proposed an additional ML pipeline
stage in the baseline pipelined RISC processor to classify instructions
into propagation delay classes and enhance temporal resource utilization. The critical challenges in deploying ML-based SoC design for real
design flows are presented in [188]. The work highlights the challenges
due to limited data, insufficient open-source benchmarks and datasets,
EDA tool-based data generation, and Synthetic data generation.
AI-SoC architectures are at the beginning of their capabilities with
tightly coupled processors and memory architectures. There is a long
way to reach their full capacity mimicking the human brain in edge
applications.
8. AI in physical design
VLSI Physical design has numerous combinatorial problems that
require many iterations to converge. Semiconductor technology scaling
has increased the complexity of these design problems with complex
design rules and design for manufacturing (DFM) constraints, making
it challenging to achieve optimal solutions [189]. Traditionally, these
issues/violations are detected and fixed manually. However, the traditional manual approach to design closure at advanced nodes is striving
hard to meet the market windows. In addition to that, the design
quality and manufacturing process in the later stages of the design flow
becomes extremely sensitive to the changes in the early stages, in turn
increasing the turnaround time and retarding the design closure. Thus,
the early-stage prediction of valid designs is critical, particularly at
the current technology nodes. Machine learning and pattern-matching
techniques provide reasonably good abstraction and quality of results
at several stages of physical design. They act as a bridge to connect each
step and provide valuable feedback to achieve early design closure.
Broadly, physical design can be divided into four stages: partitioning, floor planning, placement & clock tree synthesis, and routing
(Fig. 7). We review AI/ML approaches proposed by the researchers in
these stages through the following subsections.
8.1. AI for partitioning, floor planning and placement
Partitioning is one of the dominant areas of VLSI physical design.
The main objective of partitioning is to divide the complex circuit
into sub-blocks, design them individually, and then assemble them
separately to reduce the design complexity. Floor planning and placement are the other critical stages in the design flow for design quality
and design closure. Floor planning maps the logic description from
partitioning and the physical description to minimize chip area and
delay. The floor planning goals are arranging the chipâ€™s sub-blocks and
deciding the type and location of I/O pads, power pads, and power
and clock distributions. Placement determines the physical locations of
logic gates (cells) in the circuit layout, and its solution largely impacts
the subsequent routing and post-routing closure. Global placement,
legalization, and detailed placement are the three stages of placement.
The global placement provides the rough locations of standard cells,
and legalization removes any design rule violations and overlaps based
on the global placement solution. Detailed placement incrementally
improves the overall placement quality [190].
Chip floor planning is modeled as a reinforcement learning problem
in [191]. The state encoding provided to a neural network (NN) acting
as a reinforcement learning (RL) agent includes partial placement
information, such as the netlist, node, and edge characteristics, current
placement, and metadata of the netlist graph. The NN is trained to
optimize the next placement by maximizing the reward. The RL agent
iteratively places the macros, and a graph neural network (GNN)
architecture based on edges is developed using a dataset comprising
10,000 chip placements to define the reward policy. The method was
used to design the next generation of Googleâ€™s artificial intelligence
accelerators and has shown the potential to save thousands of hours
of human effort for each new generation. A machine learning-based
methodology to predict post P&R (place and route) slack of SRAMs
at the floor planning stage, given only a netlist, constraints, and floor
plan context tested on 28 nm foundry FDSOI technology shows a worstcase error of 224ps [192]. The proposed model could be useful for SoC
designers to avoid floorplans and constraints causing timing failures at
sign-off. Cheng et al. propose [193] regression methodology to quickly
evaluate routing congestion and half-perimeter wire length (HPWL) in
each macro placement during floor planning. The authors identified 16
distinct characteristics of HPWL and routing congestion and applied
them as features to train the regressors. They explored solutions using
different regression techniques â€” LR, DTR (decision tree regressor),
booster DTR, NN, and Poisson regression. Among these, DTR showed a
better performance. A multi-chip module (MCM) has many small chips
integrated into a package and joined by interconnects [194]. Multi-chip
partitioning is harder due to sparse search space. An RL solution for
partitioning ML models in MCM is presented in [195]. The proposed
multi-chip partitioning platform functions by mapping a computational
graph with fixed hardware accelerator constraints and variable backend
compiler constraints to a set of available chips. Through the use of a
constraint solver, an RL agent can ensure a valid partition, resulting
in quicker and more accurate partitions compared to traditional RL
models that struggle to generate sufficient valid rewards.
Moving to placement, high regularity of data paths is essential for
compact layout design during placement. However, the data paths are
frequently mixed with other circuits, such as random logic. For designs
with many embedded data paths, it is crucial to extract and place them
appropriately for high-quality placement. Existing analytical placement
techniques handle them sub-optimally [196]. However, modern placers
fail to handle data paths effectively due to technological constraints. ML
plays a crucial role in such scenarios. Ward et al. [197] proposed PADE
to demonstrate the capability of automatic datapath extraction for
large-scale designs mixed with random and datapath circuits. The effective features are extracted by analyzing the global placement netlist
to predict the direction of the datapath. PADE employs a combination
Integration 93 (2023) 102048
14
D. Amuru et al.
of SVM and NN for cluster classification and evaluation. Experimental results on hybrid benchmarks showed promising improvements in
half-perimeter and Steiner tree wire lengths. Wang et al. present a
connection vector-based and learning-based data path logic extraction
strategies [198]. SVM and CNN are employed for machine learning
based extraction. Results on MISPD 2011 data path benchmarks show
that both the strategies equally perform in classifying data path and
non-data path parts.
Chip placement is one of the chip design cycleâ€™s most timeconsuming and complex stages. AI will provide the necessary means to
shorten the chip design cycle, ultimately forming a symbiotic relationship between the hardware and AI, each promoting the advancement of
the other. To reduce the time required by the chip placement, Mirhoseini et al. proposed an approach that can learn from past experiences
and improve over time [199]. The authors posed placement as an RL
problem and trained an agent to place the nodes of a chip netlist onto a
chip canvas such that the final PPA is optimized while adhering to the
constraints imposed by the placement density and routing congestion.
The RL agent (policy network) sequentially places the macros, and
once all macros are placed, a force-directed method produces a rough
placement of the standard cells. This RL agent becomes faster and better
at chip placement as it gains experience on numerous chip netlists. The
results ensured that the proposed approach generates placements in
under 6 h, whereas the strongest baselines require human experts in
the loop, and the overall process may take several weeks. In [200],
quantum machine learning techniques are proposed for faster and
optimal solutions with low-error rates to VLSI placement problems.
A complete placement was achieved using the variational quantum
Eigen solver (VQE) [201] approach, tested on two circuits: a toy circuit
(comprising eight gates) and another circuit called â€˜â€˜Apteâ€™â€™, taken from
the MCNC benchmark suite [83]. Research on GPU acceleration for
placement and timing analysis achieved 500x speedup for static timing
analysis on a million-gate design harnessing the power of machine
learning techniques with heterogeneous parallelism [202].
Placement and routing are two highly dependent physical design
stages. Tight cooperation between them is highly recommendable for
optimized chip layout. Traditional placement algorithms that estimate
routability using pin delay or through wirelength models can never
meet their objectives due to increased manufacturing constraints, and
complex standard cell layouts [203]. A deep-learning model (CNN
based) to estimate the routability of a placement to quickly analyze
the degree of routing difficulty to be encountered by the detailed
router is presented in [204]. Unlike regression models, this approach
employs placement images sourced from ISPD 2016 Routing-Aware
Placement circuits, as well as supplementary circuits from Xilinx Inc.
These images are transformed into heatmaps and annotated based on
their routability using Xilinx Vivado Router. The resulting heatmaps
are then utilized as inputs for a CNN to forecast routability. This
research demonstrates that precise congestion images can effectively
predict placement routability, resulting in decreased CPU time and
cost for P & R. In [205], a CNN-based RL model is proposed for
detailed Placement, keeping optimal routability for current Placement.
A generalized placement optimization framework to meet the postlayout PPA metrics with a small runtime overhead is proposed in [206].
Given an initial placement, unsupervised learning discovers the critical
cell clusters for post-route PPA improvements from timing, power,
and congestion analysis. A directed-placement optimization followed
them. The approach is validated on industrial benchmarks in a 5 nm
technology node.
A machine-learning model for predicting the sensitivity of minimum valid block-level area of various physical layout factors that
provides 100x speedup compared to conventional design technology
co-optimization (DTCO) and system technology co-optimization (STCO)
approaches is proposed in [207]. This research suggests bootstrap
aggregation and gradient boosting techniques for block-level area sensitivity prediction from their experiments across various ML algorithms.
Further, [208] quotes MAGICAL (Fully automated analog layout from
netlists to GDSII, including automatic layout constraint generation,
placement, and routing), an open-source VLSI placement engine. Magical 1.0 is open-source. [209] presents automated floor planning by
exploration with different floor plan alternatives and placement styles.
RL is being proposed as the best solution for the physical design of
an IC as it does not depend on any external data or prior knowledge
for training and could produce unusual solutions based on the design
space exploration by the agent. Some RL approaches for placement
optimizations [210,211].
8.2. AI for clock tree synthesis(CTS)
Clock tree synthesis is one of the crucial steps in the VLSI physical
design. It is used to reduce clock skew and insertion delay. As the clock
network contributes a large percentage of the overall power in the final
full-chip design, it is vital to have an optimized clock tree that prevents
serious design problems, including excessive power consumption, high
routing congestion (caused when extra shielding techniques are used),
and protracted time closure. With the downscaling of devices, the
run time and complexity of existing EDA tools for accomplishing CTS
have increased. Highly efficient clock trees that optimize key-desired
parameters, such as the clock power, skew, and clock wire length, are
required. It is a very time-consuming process involving searching for
parameters in a wide range of candidate parameters. Several ML algorithms have been proposed to automate the prediction of clock-network
metrics.
Data mining tools such as the cubist data mining tool [212] are
used to achieve skew and insertion delay efficiently. In [213], statistical learning and meta-modeling methods (including surrogate models)
were employed to predict essential parameters, such as the clock power
and clock wire length, as shown in Fig. 8. In [214], the authors
implement a hierarchical hybrid surrogate model for CTS prediction,
mitigating parameter multi-collinearity challenges in relatively high
dimensions. They tackle the high-dimensionality problem by dividing
the architectural and floor planning parameters into two groups â€”
one with low multi-collinearity and the other with parameters that
exhibit large linear dependence. Later the models from these groups
are combined through least-squares regression (LSQR). [215] presents
an ANN-based transient clock power estimation that can be applied to
pre-CTS netlists. ANN utilizes a pre-CTS netlist, acquired after logic
synthesis or placement, to make predictions about the essential components of a clock tree. These predictions include the number of clock
buffers required and their corresponding workloads. By combining
these predicted clock tree components with switching information for
all signals during each clock cycle, it becomes possible to estimate
the transient power of clock buffers, clock gating cells, and flip-flops.
Additionally, this approach offers a significant reduction in the runtime required for estimating transient clock power compared to the
traditional method of running CTS alongside gate-level simulation.
Ray et al. [216] employ ML-based parameter tuning in multisource CTS to build a high-performance clock network with a quick
turnaround time. GAN-CTS, a conditional GAN framework for CTS
outcome prediction and optimization, outperforms commercial autogenerated clock tree tools in terms of clock power, skew, and wire
length (target CTS metrics) [217]. Design features are directly extracted
from placement layout images to perform practical CTS outcome predictions. The framework also employs RL to supervise the generator
of GAN toward clock tree optimization. A modified GAN-CTS [218]
employs a multitask learning technique to predict the target CTS
metrics using multi-output deep NN simultaneously. It achieves higher
accuracy in a shorter training time compared to the meta-learning
approach [217]. An RL-based solution reduces over 40% of the peak
current of a design at the CTS stage compared to the heuristic CTS
solutions utilized by physical design EDA tools [219].
Integration 93 (2023) 102048
15
D. Amuru et al.
Fig. 8. Meta modeling flow.
8.3. AI for routing
Routing lays physical connections to the circuit blocks and pins
assigned during the placement as per logical connectivity and design
rules. Global routing (GR) and detailed routing are two routing stages.
Global routing partitions the routing region into tiles and decides tileto-tile paths for all nets while attempting to optimize specific objectives
such as minimum wire length and timing budget. The actual geometric
layout of each net within the assigned routing regions is carried out in
the detailed routing stage [220].
Routing congestion [221] is the major bottleneck in the GR stage
which is caused when an overflow of net assignment occurs in a region.
Another area for improvement in routing is DRVs (detailed routing
violations). The heuristic and probabilistic approaches employed in
traditional GR solutions [222,223] suffer from scalability limitations
associated with advanced nodes. Early prediction of routing requirements enables the design engineers to create high-quality layouts faster.
Some research efforts were made to address these challenges using
machine-learning-based approaches.
MARS (multivariate adaptive regression splines), a non-parametric
flexible regression modeling for high-dimensional data, is used for
modeling routing congestion in [224]. Qi et al. [225] also utilize MARS
to construct a routing congestion model that directly estimates detailed
routing congestion through a mapping function that maps global routes
and layout data to detailed routing congestion. Router-friendly placement solutions can be obtained from congestion estimators. SVM for
classifying BEOL (back end of line) stack-specific placements routability
based on the DRVs/DRCs (design rule check) from P & R tools at
the post-route stage achieved significant improvements than employing
only congestion maps [226]. Xie et al. propose RouteNet [227] to
evaluate the overall routability of cell placement solutions without
global routing or predict the locations of DRC (Design Rule Checking)
hotspots. RouteNet is built over CNNs and shows 50% higher accuracy
than GR. An ML approach predicts any short violations before detailed
routing with placement and global routing congestion information and
sends it as feedback to the placement system for improvement [228â€“
230].
Fig. 9 shows the general procedure of ML-based routing shorts
prediction. In the routing step, each circuit under training is routed
using a detailed routing tool, and the locations of any identified shorts
are collected. In feature extraction, the circuit area is divided into
small tiles, and occurrences of shorts are investigated inside these tiles.
Each tile is described by a feature vector (appropriate features that
contribute to routing violations) and is considered as an instance. An
instance belongs to the negative (N) class and is labeled with the target
value of 0, i.e., there is no short in its tile area. An instance belongs to
the positive (P) class and is labeled with the target value of 1 if any
short violation is observed in its tile area after detailed routing. The
collected data are fed to a supervised-learning algorithm.
NTHU-Route [231] is an iterative rip-up and reroute method designed for global routing. It follows a sequential approach where each
net is routed independently without considering congestion. Once all
nets are routed, congested regions are identified, and nets within
those areas are rerouted to find less congested paths. The researchers
also propose refinement techniques to address bottlenecks encountered
during the rip-up and reroute process. This approach effectively reduces
overflow and minimizes wire length, resulting in cost and chip area
savings. Building upon NTHU-Route 2.0, Zhang et al. introduce a fast
neural network algorithm based on density and pins peaks to predict
congestion maps [232]. Experimental results using ISPD benchmarks
demonstrate that this method significantly enhances the speed of predicting routing congestion information and reduces runtime for the
global router, particularly in very large-scale integrated circuits. By
leveraging a GAN framework, a novel routing algorithm introduces a
substantial improvement in runtime without compromising the quality
of routing [233]. This algorithm combines heatmaps generated from
placement and netlist data using GAN and incorporates them into the
routing flow. As a result, it achieves high-quality global routing solutions for challenging designs that are difficult to route. This technique
effectively addresses the issue of converging on local optima during
the iterative rip-up and reroute process. Moreover, it is scalable and
exhibits a high level of accuracy.
A deep learning framework for predicting the shorts violations
by extracting useful features after placement and analyzing them
drastically decreases the prediction time and requirement of a global
router [234]. Interestingly, the framework considers a short prediction
problem as a binary classification problem with imbalanced data. The
results show that the model is 14x faster than NCTU-GR [235] for
smaller designs and up to 96x faster for larger designs. A CNN-based GR
congestion estimation algorithm [236] that utilizes the 3D congestion
information similar to [237] showed an incremental improvement
in the number of overflows, wire length, and vias. The congestion
heatmaps and placement information extracted from hyper-images of
the design feature extraction algorithm act as inputs to the congestion
model. Goswami et al. proposed a regression-based routing congestion
prediction problem for FPGAs [238]. The paper reports important features through thorough feature engineering for modeling the regression
algorithms â€” RF, MLP, LR, and MARS. On average, the proposed
methodology is 25 to 50 times faster than Xilinx Vivado-based routing
calculation tool, which reports actual congestion after detailed routing.
One solution for reducing the overall time of physical design is
to predict the circuit performance after physical design. Li and Franzon [239] proposed an ML approach using surrogate modeling (SUMO).
They employed surrogate models to predict the results after the GR
step. In the first stage, SUMO generates models for each output to
predict the GR results in the future. In the second stage, after analyzing
the linear relationships among thousands of GR results and detailed
routing results, these results were set as inputs and outputs in ML
models. These trained ML models precisely predict the after-detailed
routing results using the GR results. NNs and decision trees are the
most used ML models for this problem. A machine learning-based prerouting timing prediction approach [240] shows a closer match with
post-routing analysis from Synopsys PrimeTime. RF performed well in
their analysis compared to lasso and NN techniques.
Substrate routing automation framework through supervised learning algorithms is proposed in [241]. It combines manual and automated results as training data to a CNN for improved design cycle
and performance. A GNN-based congestion estimation approach that
can predict the detail routed lower metal layer congestion values from
a technology-specific gate-level netlist for every cell in a design is
Integration 93 (2023) 102048
16
D. Amuru et al.
proposed in [242]. The training dataset is built from the detail-routed
congestion maps by dividing them into discrete grids and assigning the
congestion value of each grid as the target value. Another GNN-based
routing short violations prediction at the placement stage is proposed
in [243]. The information is fed back to the placement system, and
a new placement result is generated with reduced DRC violations.
GraphSAGE (Graph sample and aggregate) is applied effectively to
combine the adjacency matrix with the features of each tile. [244]
presents a survey of the recent development of machine learningbased routing algorithms. XGBoost is employed to predict post-detailed
routing timing at the post-GR stage in [245]. When employed for
post-GR optimization, it improves the circuit performance.
Supervised learning, NNs in particular, and RL-based solutions reported in the literature produced many valuable feedbacks and solutions for different complex modeling tasks at various physical design
stages. However, The complexity of modern VLSI chips, consisting of
billions of transistors and interconnects, has led to increasingly long
design cycles that struggle to meet market timelines while maintaining
optimal chip quality. In particular, during the physical design phase,
designers face numerous constraints related to power, timing, area, and
cost, often requiring multiple iterations to achieve convergence. In this
context, AI/ML approaches offer solutions for accelerating designs and
enhancing the efficacy of IC physical design.
The primary advantages lie in the ability of AI/ML techniques
to handle complex designs and tasks efficiently, say, using neural
networks (NNs). They can automate iterative design exploration and
provide placement and routing suggestions to avoid congestion through
reinforcement learning (RL), which would otherwise be tedious and
time-consuming for a physical designer. By amalgamating various automated learning procedures, it becomes possible to create an automated
workflow from netlist to GDS-II, significantly reducing the need for
multiple design expertise and manual effort at various levels.
However, there are challenges associated with implementing an
accurate learning platform for physical design. These challenges include limited availability of training data, difficulty in encoding design
expertise related to placement and routing into ML architectures, and
the need for thorough collaboration between designers and ML engineers to verify the encoded information. Additionally, a suitable ML
model at this level requires feedback from routing to placement or floor
planning, necessitating design changes. It is crucial to restructure the
encoded information according to designer standards, as even a small
error at any stage can result in significant design changes, leading to
convergence issues or requiring extensive iterations. Therefore, careful
analysis and understanding of the flow of information is essential.
Furthermore, while many researchers have achieved promising results on benchmark designs, the extent to which ML models generalize
across different designs and applications remains uncertain. It is necessary to develop proper measures of generalization to estimate their
effectiveness accurately.
After placement and routing, the layout is generated and the design
is ready for fabrication.
9. AI in manufacturing
Numerous processes are involved in manufacturing an IC, including wafer preparation, epitaxy, oxidation, diffusion, ion implantation,
lithography, etching, and metallization [246]. All the steps are performed in highly sophisticated fabrication units with constant human
supervision. The fabricated ICs are packaged in special packages to
protect them from external/environmental damage.
Fig. 9. Typical flow of ML-based routing techniques.
9.1. AI for lithography
Most chip-manufacturing processes are complex chemical processes,
except for the lithography process. Lithography transforms layout data
into geometric patterns as masks and from masks to the resist material on the semiconductor. After the physical design, lithography is a
crucial step in chip manufacturing. Masks identify spaces on the wafer
where certain materials need to be deposited, diffused, or removed.
The fabrication process involves several dozen steps of deposition and
diffusion based on the circuitâ€™s complexity. During each step, one mask
is used. The exposure parameters required to achieve accurate pattern
transfer from the mask to the photosensitive layer primarily depend on
the wavelength of the radiation source and the dose required to achieve
the desired change in the properties of the photoresist. Identifying
defects during mask synthesis and verifying each lithography stage
before proceeding to the next stage is crucial for yield enhancement
but very difficult in the nanometer dimensions, mainly due to the
increased random variations in the process. Introduction/improvement
of automated procedures at various stages of lithography is necessary
to increase the manufacturing yield and reduce the cost and turnaround
time. Traditionally, this was a very laborious process; fortunately, the
introduction of ML has afforded many opportunities for increasing the
processing speed, particularly in mask synthesis and verification [247].
The need for Machine Learning in the lithography process is discussed in [248]. It also highlights various algorithms and their tradeoffs used for hotspot detection (HD), optical proximity correction
(OPC), sub-resolution assist feature (SRAF), phase shift masks (PSM),
and resist modeling. They also propose a Gaussian process to reduce
the false positive outcomes of ML algorithms. In detail, we discuss the
research on ML-Lithography in the following sub-sections.
9.1.1. At mask synthesis
Optical lithography is the most widely used technique in IC manufacturing, where a geometric mask is projected into a photo-resistcoated semiconductor through a photon-based technique. Mooreâ€™s law
has driven features to ever smaller dimensions, and the technology
has been scaled down to the limit of light wavelength. Consequently,
the printed patterns get distorted due to diffraction, resulting in process defects. Various resolution enhancement techniques (RETs) are
Integration 93 (2023) 102048
17
D. Amuru et al.
Fig. 10. Typical procedure of ML-based mask Synthesis Flow.
employed to improve the performance of photo-lithography. OPC and
SRAF insertion are the most used RETs to maximize the process window
and ensure accurate patterns on the wafer. However, these enhancement techniques suffer from an extremely long runtime owing to their
laborious iterative process. Many state-of-the-art methods use machine
learning to identify defective lithographic patterns. Fig Fig. 10 shows
the typical flow of ML-based mask synthesis.
LR was the first ML technique used in OPC. An LR model for
predicting the optimum starting point for a traditional-iterative-modelbased OPC has been proposed [249]. Using discrete cosine transform
coefficients from the lowpass-filtered 2 Ã— 2 um layout patterns as
inputs and creating separate models for normal edge, concave corner,
and convex corner fragments; they achieved a 32% reduction in the
runtime. When Luo [250] proposed a three-layer MLP to generate the
optimal mask pattern for OPC, deep learning came into use. Using
the steepest descent method to generate the training set, his model
drastically reduced computation time.
A hierarchical Bayes model (HBM) was proposed in [251], for OPC,
along with a new feature-extracting technique known as concentric
circle area sampling (CCAS). HBM provides a flexible model that is not
constrained by the linearity of the model parameters or the number of
samples; this model utilizes a Bayes inference technique to learn the
optimal parameters from the given data. All parameters are estimated
using the Markov chain MC method [252]. This approach has shown
better results than other ML techniques, such as LR and SVMs. Most ML
OPCs use local pattern densities or pixel values of rasterized layouts as
parameters, which are typically huge numbers. It leads to overfitting
and, consequently, reduced accuracy. Choi et al. [253] proposed the
usage of basic functions of polar Fourier transform (PFT) as parameters
of ML OPC. The PFT signals obtained from the layout are used as
input parameters for an MLP whose number of layers and neurons
are decided empirically. Experimental results show that this model
achieves an 80% reduction in the OPC time and a 35% reduction in
error.
ML is also explored in inverse lithography technology (ILT) [254],
a popular pixel-based OPC method. ILT treats the OPC as an inverse imaging problem and follows a rigorous approach to determine
the mask shapes that produce the desired on-wafer results. Jia and
Lam [255] developed a stochastic gradient descent model for mask
Fig. 11. Generative adversarial networks.
optimization that showed promising results in robust mask production.
Luo et al. [256] proposed an SVM-based layout retargeting method for
ILT for fast convergence. A solution to ILT was achieved through a
hybrid approach by combining physics-based feature maps [257] with
image space information as model inputs to DCNN (deep CNN) [258].
SRAFSs are small rectangular patterns on a mask that assist in
printing target patterns; they are not printed even though they are
on the mask. The process of SRAF generation is similar to OPC and
is computationally expensive. Recently, ML was applied to SRAF generation. Xu et al. [259] demonstrated an SRAF generation technique
with supervised-learning data for the first time. In their model, features
are extracted using CCAS and compacted to reduce training data size.
Logistic regression and SVM models were employed for training and
testing. Instead of using binary classification models, the author uses
the models as probability maxima. SRAFs are inserted at the grids with
the probability maxima. This model shows a drastic speedup in computation with less error. Shim et al. [260] used decision trees and logistic
regression for SRAF generation, which showed a 10Ã— improvement in
runtime.
Etching and mask synthesis are performed simultaneously. Recently,
ML has been used to predict the etch bias (over-etched or underetched). ANNs [261â€“263] have been used to predict the etch proximity
correction to compensate for the etch bias, yielding better accuracy
than traditional methods.
Although these ML models achieve high accuracy, they require a
large amount of data for training. In the field of lithography, where
the technology shrinks very rapidly, and old data cannot be used for
the new models, data generation is a very laborious task. One of
the solutions to this problem is to use transfer learning, [264] which
takes the data generated through old technology nodes and information
about the evolution of nodes, e.g., from 10 to 7 nm, and uses them
for model training. The authors also employ active data selection to
use the unlabeled data for training using Clustering. ResNet is used
along with these two active learning and transfer learning techniques,
yielding high accuracy with very few data samples for training.
GANs [53] are one of the hottest prospects in deep learning. Fig. 11
shows the general design of the primary optimization flow of a generative adversarial network. It contains two networks interacting with
each other. The first one is called the â€˜â€˜generatorâ€™â€™ and takes random
vectors as input and generates samples as close to the true dataset
distribution as possible. The second one is called the â€˜â€˜discriminatorâ€™â€™
and attempts to distinguish the true dataset from the generated samples. At convergence, ideally, the generator is expected to generate
samples with the same distribution as the true dataset. This technique was exploited in lithography modeling. GANs were used for OPC
where intermediate ILT results initialize the generator; this improves
the training process, allowing the network to produce an improved
mask [265]. In [266], CGAN was used for the generation of SRAF.
Conditional GAN is an extension of GAN, where the generator and
discriminator are conditioned on some auxiliary information, such as
Integration 93 (2023) 102048
18
D. Amuru et al.
Fig. 12. Examples of lithography hotspot patterns.
class labels or data, from other modalities. A new technique for data
preparation, i.e., a novel multi-channel heatmap encoding/decoding
scheme that maps layouts to images suitable for CGAN training while
preserving the layout details, was also proposed here. This model
achieves a 14Ã— reduction in computation costs compared to state-ofthe-art ML techniques. LithoGAN [267] is an end-to-end lithography
modeling approach where the mask pattern is directly mapped to the
resist pattern. Here, a CGAN is used to predict the shape of the resist
pattern, and a CNN is used to determine the center location of the resist
pattern. This technique overcomes the laborious process of building and
training a model for each stage, resulting in a reduction in computation
time of approximately 190 times compared to other ML techniques.
Different OPC engines work on different design patterns, each of
which has advantages and disadvantages. Compared to the modelbased OPC, ILTs generally promise good mask printability owing to
their relatively large solution space. However, this conclusion only
sometimes holds as ILTs need to solve a highly non-convex optimization
problem, which is occasionally challenging to converge. GAN yields
good results; however, it is difficult to train for some patterns. To
overcome these challenges, Yang et al. [268] proposed a heterogeneous
OPC flow, where a deterministic ML model decides the appropriate
OPC engine for a given pattern, taking advantage of both ILT and
model-based OPC with negligible overhead. They designed a classification model with a task-aware loss function to capture the design
characteristics better and achieve their objectives. Yang et al. [269]
also proposed an active-learning-based layout pattern sampling and HD
flow for effective, optimized pattern selection. The experiments show
that the proposed flow significantly reduces the lithography simulation
overhead with satisfactory detection accuracy.
E-beam lithography is another prominent patterning method to electronically transfer the layouts onto the wafer. Non-uniformities caused
by parallel e-beam maskless lithography result in variations within the
targets. Scatterometry measures the defects caused by simulated dose
variations in patterned multi-beam maskless lithography. An ML-based
scatterometry to quantify critical dimension (measured parameter for
variation detection) and sensitivity analysis in detecting beam defects is
proposed in [270]. A fast in-line EUV resist characterization using scatterometry in conjunction with machine learning algorithms is presented
in [271].
9.1.2. At mask verification
Due to complicated design rules and various RETs such as OPC
and SRAF, there may still be many lithographic hotspots that may
cause opens, shorts, and reductions in yield (Fig. 12). Therefore, detecting and removing these hotspots are critical for achieving a high
yield. Traditionally, pattern-matching techniques are widely used in
HD. Hotspot patterns are stored in a predefined library, and given a
new testing pattern; a hotspot is detected if it can be matched to the
existing patterns. This technique is accurate for already-known hotspot
patterns but does not work well for new, unknown patterns. ML-based
approaches show better accuracy for both seen and unseen patterns.
Early ML usage in lithography HD included classifiers such as simple
NNs (including ANNs) [272,273], which detect hotspots from given
patterns. Clustering algorithms were also extensively used, [274,275],
where a large dataset of hotspots is divided into multiple classes
using these algorithms, and pattern-matching techniques are used for
the detection of new hotspots. As the detected hotspots in the same
class share similar geometric shapes, it is expected that they can be
fixed using a standard fixing solution. False alarms are a critical issue
in HD in many ML methods. Researchers have been attempting to
overcome this challenge. Ding et al. [276] attempted to successfully
refine the SVM and ANN classifiers to identify the hotspot patterns
more accurately Topological classification is another method where
feedback learning is employed to reduce false alarms. Yu et al. [277]
classified the already-known hotspots and non-hotspot patterns into
clusters according to the topologies in their core regions. Subsequently,
they extracted critical features and constructed an SVM kernel with
multiple feedback learning from the mispredicted non-hotspots. Combining different ML techniques yields better outcomes most times.
Ding et al. [278] proposed a new algorithm that combines ML and
pattern matching, and Matsunawa et al. [279] used the AdaBoost
classifier; both approaches resulted in a significant reduction in false
alarms and outperformed many other ML techniques. Semi-supervised
learning [280] is also being used for detection; it leverages both labeled and unlabeled data, thereby reducing the dependence on labeled
training data. It is advantageous as obtaining labeled hotspot regions
is considerably more difficult. This method combines Classification and
Clustering, creating a multitasking network that groups the unlabeled
data with labeled data and then uses them for learning. It reduces the
pre-processing time and amount of labeled training data required.
CNNs are a widely used NN technique in image processing, classification, etc. HD is very similar to image classification; CNNs [281,282]
have recently been used in this field, yielding better accuracy than
other state-of-the-art ML approaches. Pooling layers are one of the
building blocks of CNNs. These layers reduce the number of parameters
and computation steps in the network by extracting the statistical
summary of the local regions of the previous layer, thereby reducing
the feature map dimension and drastically lowering the sensitivity of
the NN to small changes. However, in the HD process, these layers
may ignore small edge displacements and turn them into non-hotspot
regions. Yang et al. [283] proposed a pooling-free CNN architecture
that overcomes this defect, yielding increased accuracy. Online learning
is another method in ML where the model is trained and updated with
new data that is fed over time to build the predictor for future data.
This method can adapt over time and works well with new models;
thus, it can be used in HD. Although CNN has the potential to perform
well in HD, hotspot patterns are always minorities in the VLSI mask
design as less number of patterns are available for training, resulting
in an imbalanced training data set; this results in a model with high
false negatives.
Yang et al. [284] attempted to apply minority up-sampling and
random-mirror flipping before training the network and achieved better performance than state-of-the-art hotspot detectors. In this preprocessing technique, the training dataset is first augmented with a
mirror-flipped, 180â—¦
-rotated version of the original layout clips, followed by up-sampling. In this technique, overfitting can be reduced
through random mirroring. Zhang et al. [285] built an online learning
model with a novel critical feature extraction technique. They constructed an ensemble classifier using smooth boosting and modified
Naive Bayes. Their technique outperformed state-of-the-art methods in
terms of accuracy and false alarms. Subsequently, they extended this
technique to online learning, which yielded even better performance.
Ye [286] proposed litho-GPA, a Gaussian process assurance (confidence
value), provided along with each prediction. The framework also incorporated a set of weak classifiers and active data sampling for learning,
reducing the amount of training data and computations required.
Integration 93 (2023) 102048
19
D. Amuru et al.
Park et al. [287] propose an SVM model trained with lithographic
information that detects pinching and bridging hotspots during mask
transferring to wafer. They further incorporate domain knowledge of
lithographic information in the SVM kernel to accomplish an accurate
decision function to classify them into four categories â€” horizontal
bridging (HB), vertical bridging (VB), horizontal pinching (HP), and
vertical pinching (VP). A hybrid pattern matching-SVM classifier for
HD is presented in [288]. CNN-based HD is proposed in [289,290].
The framework in [290] also has a transfer learning scheme to reduce
the training sample requirement for modeling HD at a more advanced
node. A modified DNN by replacing pooling layers with convolution
layers for HD is proposed in [291]. They applied hotspot folding,
rotating, and mirror-flipping for highly imbalanced datasets to maximize the training samples. Addressing the challenge of an imbalanced
dataset in HD, [292] propose a dataset sampling technique based on
autoencoders. The autoencoders identify latent data features that can
reconstruct the input patterns, which are then grouped using Densitybased spatial clustering of applications with noise (DBSCAN). These
clustered patterns are sampled to reduce the training set size. An
automatic layout generation tool that can synthesize different layout
patterns given a set of design rules is proposed in [293]. The tool
supports via and uni-directional metal layer generation. It is robustness
in HD is tested using state-of-the-art ML models.
SONR (state of nature reduction) [294], a semi-supervised feature
vector-based ML tool for lithographic HD at different stages and cross
products based on known hotspots, is a fast and effective method to
optimize OPC verification flow and improve manufacturing yield. The
proposed workflow is available as Mentorâ€™s Calibre SONR tool. [295]
demonstrates an HD case study based on ADAPT, a framework for
the fast migration of machine learning models across different IC
technologies. It is an unsupervised Bayesian approach to significantly
reduce model cost and provide customized learning with fewer data
techniques and labeling strategies. An ML-based color defect detection
for after develop inspections in lithography exhibited more sensitivity
and specificity in a trial comparison against the reference method [296,
297].
Automation in SEM (scanning electron microscope) image preprocessing using dimensionality reduction and feature detection dramatically reduces the computation time of lithography patterning
[298]. A framework combining ML models for automatically mining
lithographic hotspots from massive SEM images detects hard defects
such as bridging and necking and soft defects such as scumming that
are hard to detect by manual inspection [299]. However, they propose
manual inspection on top of their framework for the final decision on
the detected hotspots. The solution proposed could reduce the workload
to a large extent compared with the traditional way. Recently, many
researchers have been searching for efficient solutions beyond ML [300,
301]. A circuit-based hybrid quantumâ€“classical machine learning using
variational quantum layers for lithography HD from SEM images is
proposed in [300]. The hybrid approach adds quantum circuits to
the conventional CNN for enhanced performance. Quantum computing
simulation has been performed with CuQuantum, an Nvidia software
development kit with optimized libraries and tools for accelerating
quantum computing workflows. Virtual metrology model using CNNs
to predict the overlay errors of the photo-lithography process [301].
Layout patterns play an essential role as resources for flows of
various DFM that we have already discussed. However, VLSI layout
pattern libraries are not readily available due to the long and iterative technology life cycle, which can slow down the technology
node development. However, significant effort has been devoted to
enlarging existing libraries by exploiting existing patterns, including
flipping, rotating, and using a random generator. These methods are
coupled with complex manuals for guidance and hardly increase the
layout diversity owing to their deterministic strategy. To address these
problems, Zhang et al. [302] proposed a pattern generation and legalization framework comprising two learning-based modules for pattern
topology generation and design rule legalization. In the generation
stage, a variational convolutional autoencoder (VCAE) [303] is designed to efficiently generate realistic pattern topologies via Gaussian
perturbation. For the legalization stage, a CGAN [304] model is used to
transform the generated samples from blurry patterns to smooth ones,
significantly reducing the DRC violation risks. Based on an adversarial
autoencoder, a pattern style detection tool is designed to examine the
pattern styles and filter out unrealistic generated patterns. A novel
confidence-aware deep learning model for post-fabrication wafer map
defect is proposed in [305]. The experiment results on industrial wafer
datasets demonstrate superior accuracy compared to the traditional
approach. The paper also discusses the scope of DL-based approaches
for manufacturing and yield in the near future.
Evidently, ML is no longer a novelty in chip fabrication. Chip
manufacturers will continue to leverage the technology as it matures.
ML provides solutions to many problems in lithography. However,
unlike areas such as image processing, where a large amount of data
is available, it is difficult and expensive to obtain enough data in VLSI
design for training robust and accurate models. Therefore, developing
techniques for improving modeling accuracy with a relaxed demand for
big data is critical to promote the widespread adoption of ML.
9.2. Reliability analysis
Over the last few decades, shrinking CMOS geometries have increased manufacturing defect levels and on-chip fault rates. Increased
fault rates have considerably impacted the performance and reliability
of circuits. The multiplied fault rates have necessitated an accurate and
robust reliability analysis. The fundamental reliability analysis evaluates logic circuit errors due to hot-carrier insertion, electro-migration,
NBTI (negative-bias temperature instability), and electrostatic ejection. Reliability engineers focus on correcting the functionality and
enhancing the circuitâ€™s lifetime.
Precise reliability analysis involves numerous mathematical equations. However, mathematical equations fall apart due to the complexity involved in the reliability estimation of large circuits with millions
of gates. Reliability engineers have worked on MC simulations, which
are rigorous and time-consuming. Therefore, the evolution of ML has
aided engineers in developing exhaustive and rapid reliability analysis
algorithms. Patel et al. [306] and Krishnaswamy et al. [307] developed
probabilistic transfer matrices, which perform simultaneous analyses
over all the possible I/O combinations. The major limitation of the
method is the large memory requirement for the matrices. Choudhury
et al. [308] recommended algorithms for reliability analysis based on
a single pass, observability, and a max-k gate. The methods are precise
for PVT and aging-related degradation. Beg et al. [309] presented an
NN-based nanocircuit reliability estimation method as an alternative to
traditional mathematical methods. This method is time efficient for the
analysis of the circuits.
Circuit aging is one of the essential concerns in the nanometer
regime in designing future reliable ICs. Several operating conditions,
such as temperature, voltage bias, and process parameters, influence
the performance degradation of the IC. NBTI is a significant phenomenon occurring at present and future technology nodes and contributes significantly to the performance degradation of an IC due to
aging. It shifts the threshold voltage during the lifetime, degrades the
device drive current, and degrades the device performance. It is necessary to evaluate the impact of NBTI on the performance of a circuit
under stress early in the design phase to incorporate appropriate design
solutions. Many researchers have contributed to the NBTI estimation
early in the design phase through ML algorithms.
Karmi et al. [310] proposed an aging prognosis approach based on
nonlinear regression models that map circuit operating conditions to
critical path delay estimation. The approach also considered the effects
due to process variations. The experiments showed that the impact of
IC aging on critical path delays could be accurately estimated through
Integration 93 (2023) 102048
20
D. Amuru et al.
nonlinear regression models. Such modeling facilitates the implementation of preventive actions before the circuit experiences aging-related
malfunctions. A gate-level timing prediction under dynamic on-chip
variations is proposed in [311]. The high-dimensional features added
to the statistical timing analysis for modeling the NBTI increase with
increasing circuit complexity. The proposed learning-based approach
efficiently captures these high-dimensional correlations and estimates
the NBTI-induced delay degradation, with a maximum absolute error of
4% across all the designs. SVR and random forest models were applied
to the timing estimation. Analysis and estimation of the impact of
NBTI-induced variations at multi-gate transistors in digital circuits are
becoming highly challenging [312]. A quick and accurate estimation
of process variation impact and device aging on the delay of any path
within a circuit is possible through GNNs [313].
Electro-migration is another concern successfully addressed by various AI strategies. A new data-driven learning-based approach for fast
2D analysis of electric potential and electric fields based on DNNs is
proposed in [314]. As an extension, Lamichhane et al. [315] proposed
an image-generative learning framework for electrostatic analysis for
VLSI dielectric aging estimation. It speeds up the analysis compared to
the conventional numerical method, COSMOL. Compared to the similar
CNN-based method, the proposed GAN-based approach gives 1.54x
more speedup with around similar accuracy.
Reliability analysis and failure prediction of 3D ICs has gained
considerable attention over the past few years. A study on the 3D Xray tomographic images combined with AI deep learning based on a
CNN for non-destructive analysis of solder interconnects demonstrates
an accuracy of 89.9% in predicting the interconnect operational faults
of solder joints of 3D ICs [316]. Adaptive lifetime prediction techniques (ADLPT) that minimize redundant prediction operations in 3D
NAND flash memories by exploiting reliability variation are presented
in [317].
Kundu et al. [318] confer the reliability issues of different AI/ML
hardware. The paper explores and analyzes the impact of DRAM faults
on the performance of the DNN accelerator by implementing MLP on
MNIST datasets. Further, they discussed the impact of the circuit and
transistor-level hazards such as PVT variations, runtime power supply
voltage noise and droop, circuit aging, and radiation-induced soft errors
on AI/ML accelerator performance. The accuracy impact on MAC units
due to these hazards has been estimated. The paper also highlights the
reliability issues of neuromorphic hardware and proposes RENEU, a
reliability-oriented approach to map machine learning applications to
it.
The ever-growing circuit complexity is also raising concerns about
hardware security. ML can aid in detecting hardware attacks and could
take necessary counter-attacks with suitable design [319]. Hardware
assurance and verification in manufactured ICs are also important to
identify hardware Trojans. Manual verification to identify such security
threats is becoming challenging at the present scale of circuit design.
Addressing these issues, [320] proposed CNN-based arithmetic circuit
classification, taking the image generated from a circuitâ€™s conjunctive
normal form description. However, the structural information of circuits is difficult to capture in the CNN framework. Resolving it, [321]
proposes a GNN framework for ASIC circuit netlist recognition, which
classifies circuits according to their structural similarity. Case studies
on four designs of adder circuits exhibit 98.3% accuracy. Several environmental, performance and process-related embedded instruments
(EI) are present in an SoC with a JTAG interface. The EI data is
systematically collected over time and analyzed using PCA (principal
component analysis) and a power-law-based degradation model to
predict the remaining valid lifetime of an SoC [322]. Liakos et al. [323]
proposed hardware trojan learning analysis (ATLAS) that identifies
hardware trojan-infected circuits using a gradient boosting model on
data from the gate-level netlists phase. The feature extraction was
based on the area and power analysis from Synopsis Design Compiler
NXT industrial tool. ATLAS model was trained and tested on all circuits available in the Trust-HUB benchmark suite. The experimental
results show that the classification performance is better than the
existing models. In [324], GNNs are proposed for reverse engineering
of gate-level netlists without manual intervention or post-processing.
The experimental results on EPFL benchmarks [325], the ISCAS-85
benchmarks, and the 74X series benchmark show an average accuracy
of 98.82% in terms of mapping individual gates to modules.
Numerical methods and MC simulations, which reliability engineers
widely use, have memory and timing constraint bottlenecks. The NNs
and Bayesian-statistical models are exhaustive and consume less memory. Recently, hyper-dimensional computing, an emerging alternative
to ML, has been proposed to address the circuit reliability issues [326].
The experiments to estimate transistor electrical characteristics and
manufacturing variability on industrial 14 nm FinFET Intel instruments
demonstrate 4x smaller error with 20x fewer training samples. Thus,
ML and more advanced models will play a significant role in reliability
estimation in the future.
9.3. Yield estimation and management
Many complex and interrelated components during the manufacturing process affect the yield of an IC. Yield learning and optimization are
critical for advanced IC design and manufacturing. A yield prediction
model is necessary to precisely evaluate the productivity of new wafer
maps because the yield is directly related to the productivity and
the design of the wafer map affects the yield [327]. Many statistical
approaches [328,329] for yield modeling and optimization have been
proposed since the 1980s; however, with the uncertainty in nanoscale
fabrication and the growing complexity of the process, large volumes
of data are being generated daily, traditional approaches have limits
in extracting the full benefits of the data. Even the most complex
sophisticated process results in poor exploitation of data. AI/ML could
aid in continuous quality improvement in a large and complex process.
Cycle time(CT) is one of the critical performance measures of the
semiconductor production line. It is a mandate to understand the key
factors influencing CT for its effective reduction and yield enhancement. A data-driven approach to predict the CT understanding the key
factors influencing it is proposed in [330,331]. The approach of data
mining and ML can be used for analyzing the extracted information and
knowledge from different stages of manufacturing for troubleshooting
and defect diagnosis, which decreases the turnaround time. Learning
approaches are proposed in [332] for yield enhancement. A backend
final test yield prediction at the wafer fabrication stage using a Gaussian Mixture Models (GMM) clustering approach through a weighted
ensembled regressor is proposed in [333]. Yield prediction at an early
stage helps in cost reduction and quality control. However, there are
some limitations to GMM â€” sensitive to initial guesses of parameters
and high chances of getting stuck at the local minimum. As an extension
to this, [334] propose a final test yield optimization approach through
wafer acceptance test parametersâ€™ inverse design.
Classification aids in minimizing wafer yield loss and package yield
improvement by thoroughly analyzing data across fab measurements,
wafer tests, and package tests [335]. In [336], an ROI-based (return on
investment) wafer productivity model using DNNs as a yield prediction
technique and differential evolution for optimization is proposed. The
DNNs are trained using geometric features of dies. A DNN approach
exploits spatial relationships among positions of dies on a wafer and
die-level yield variations collected from a wafer test to predict the yield
for pre-evaluating the productivity of new wafers [327].
ML is gradually being utilized in yield prediction and optimization
and is still in its early stages. There is scope for significant growth in
using various ML techniques in yield enhancement.
Integration 93 (2023) 102048
21
D. Amuru et al.
10. AI at testing
VLSI testing is the process of detecting possible faults in an IC after
chip fabrication. It is the most critical step in the VLSI design flow.
The earlier a defect is detected, the lesser the final product cost. Rule
of 10 [337] states that the cost of fault detection increases by order of
10 moving from one stage to the next in the IC design flow. Improving
the yield is a necessity for any company; shipping defective parts can
destroy a companyâ€™s reputation [338]. Almost 70% of the design development time and resources are spent on VLSI testing. Different stages of
the design flow involve different testing procedures. Broadly different
levels of testing are functional verification testing, acceptance testing,
manufacturing testing, wafer level testing, packaging level testing, and
so on [339]. We highlight the significant areas of testing that has AI/ML
contributions.
10.1. Functional verification
Functional verification is verifying that a design conforms to its
specifications. A set of input vectors is provided to the CUT (circuit
under test), and its output is compared to the golden output of the
specification for checking the possibility of faults. Functional verification [340] is very difficult because of the sheer volume of possible
test cases, even in a simple design. Manufacturers generally employ a
random test pattern generator [341], which provides significant fault
coverage; however, it may only cover some of the faults and has a very
long runtime. ML is being used to predict the best test set to achieve
the maximum fault coverage with a minimum number of test vectors.
In [342], the nearest neighbor algorithm was used to generate efficient
patterns for BIST (built-in self-test) test pattern generation, improving
the fault coverage. This algorithm detects random pattern-resistant
faults and produces test patterns directed toward them. Bayesian networks were used [343,344] to predict the test pattern set. A Bayesian
network is a graphical representation of the joint probability distribution for a set of variables. The Bayesian network model describes the
relations between the test directives and coverage space and is used
to achieve the required test patterns for a given coverage area. It was
further enhanced by clustering the coverage events and working on
them as a group. Hughes et al. [345] proposed an ML approach for
functional verification, where a NN model is used with RL to track
the coverage results of a simulation and, after that, to generate a set
of verification input data recommendations, which will increase the
probability of hitting functional coverage statements and identifying
hard-to-hit faults while adjusting itself.
The initial stage of the CUT greatly impacts the time taken and the
ability of stimuli generators to generate the requested stimuli successfully. Some initial states can lead to poor fault coverage, resulting in
faulty products. Bayesian networks are employed to automatically and
approximately identify the region of favorable initial states; however,
they require a certain level of human guidance to select one of the
initial states [346]. Identification of power-risky test patterns is also
essential, as excessive test power can lead to failure due to IR drop and
noise. However, simulation of all the patterns is impossible due to the
long runtime. Thus, the pre-selection and creation of a subset of patterns are crucial. Dhotre et al. [347] proposed a transient power activity
metric to identify potentially power-risky patterns. The method uses
the layout and power information to rank the patterns approximately
according to their power dissipation and subsequently uses a K-means
clustering to cluster all the instances with concentrated high switching
activity.
The application of ML can be extended to delay test measurements
as well. Wang et al. [348,349] proposed models for ğ¹ğ‘šğ‘ğ‘¥ prediction
based on the results of structural delay test measurements to determine the optimum conditions for improving the correlation between
the golden reference and potential low-cost alternative for measuring
the performance variability of the chip design. The performance and
robustness of the proposed methodology with a new dataset pruning method, called â€˜â€˜conformity checkâ€™â€™, is demonstrated on a highperformance microprocessor design using KNN, least-squares fit, ridge
regression, SVR, and Gaussian process regression (GPR) models. GPR
has proven effective in achieving accurate functional and system ğ¹ğ‘šğ‘ğ‘¥
prediction.
In [350], an explainable ML approach called Sentences in Feature
Subsets (SiFS) for test point insertion (TPI) is proposed. The proposed
ML methodology can also apply to human-readable classification in
EDA. An ANN-guided ATPG (automatic test pattern generation) [351,
352] proposed in the recent past reduces the backtracks for PODEM
and improves the backtraces, particularly in reconvergent fault-free
circuits with reduced CPU time. The training parameters include inputâ€“
output distances and testability values from cop (controllability and
observability program) for signal nodes. Unifying ANN for ATPG incurs
a one-time cost, after which ML imparted to ATPG can have long-term
benefits. Design2Vec, a deep architecture that learns representations of
RTL syntax and semantic abstractions of hardware designs using a GNN,
is proposed in [353]. These representations are applied to several tasks
within verification, such as test point coverage and new test generation
to cover desired points. Pattern identification and reordering method
are presented in [354]. An ML algorithm was used to select the most
effective test patterns, and then an optimal pattern sequence was
determined using the weighted SVMRANK (SVM Rank classification)
algorithm. Experiments show time-saving of 3.89 times at the expense
of 2% prediction accuracy. In [355], a KNN method is proposed to
divide the test patterns into valid and invalid patterns and then use only
valid patterns to reduce the test time. Experiments show that compared
to the traditional method; this methodology reduces the test time by
1.75 times. Chen et al. proposed an RL-based test program generation
technique for transition delay fault (TDF) detection [356].
Even though ML has shown significant progress and promise for
functional verification, more is needed to perfect accuracy and human
intervention. Intelligent data collection procedures and a novel feature
extraction scheme with MI should be inducted into CAD tools as initial
steps for IC testing to become fully automated.
10.2. Fault diagnosis
After functional verification, the following test procedure identifies
the fault location and type, called fault diagnosis. Traditionally, this
step is not fully automated, and the engineerâ€™s experience and intuition
play a part in developing the test strategies. At present, digital circuits
and systems are almost fully automated and have been extensively
explored. In contrast, analog circuits are more difficult to diagnose;
over the last few years, extensive research on analog fault diagnosis
has been conducted, and many ML models have been reported. Most
of these models focus on obtaining the output response of a circuit
for the test pattern; different pre-processing techniques are applied,
bore applying this data as input to the ML model, which attempts to
classify the fault. In [357], the Fourier harmonic components of the
CUT response are simulated from a sinusoidal input signal and supplied
to a two-layer MLP, which attempts to identify the fault. Additionally,
a selection criterion for determining the best components that describe
the circuit behavior under fault-free (nominal) and fault situations
is used and provided as input to a NN [358]. The NN, along with
clustering, classifies the faults into a fault dictionary.
In [359], wavelet transform along with PCA was used as a preprocessing technique to extract the optimal number of features from
the circuit node voltages. A two-layer NN was trained on these features
to the probability of input features belonging to different fault classes.
This model yields a 95%â€“98% accuracy on nonlinear circuits. It was
improved in [360] by dividing the circuit successively into modules.
At each stage of module subdivision, a NN is trained to determine the
sub-module that inherits the fault of interest from the parent module.
It led to an increase in the training efficiency of the NN, resulting
Integration 93 (2023) 102048
22
D. Amuru et al.
Fig. 13. A typical scan chain design.
in 100% accuracy in the classification. A novel anomaly detection
technique [361] for post-silicon bug diagnosis was proposed to detect
aberrant behaviors or anomalies and identify a bugâ€™s time and location.
This algorithm comprises two stages. In the first stage, it collects data
online by running multiple executions of the same test on the postsilicon platform, after which it records compact measurements of the
signal activity. In the second stage, it analyzes the data offline. The
authors measured the amount of time the signalâ€™s value was one during
the time step and applied ML to the measurements to locate the bug.
The fundamental goal of testing is to determine the defectsâ€™ root causes
and eliminate them.
Automatic defect classification, which has existed for several years,
has been revolutionized by ML in terms of speed and accuracy, although
ML-based defect analysis is still not ideal for industrial standards. Much
focus is needed on automated defect analysis to locate the root cause
of the defect.
10.3. Scan chain diagnosis
Scan chain structures are widely used in VLSI circuits under design
for testing. They increase the fault coverage and diagnosability by
enhancing the controllability and observability of the digital circuit
logic [362]. Fig. 13 shows the design of a preliminary scan chain.
During normal circuit operation, these structures function like a regular
flip-flop, and during testing, they shift and capture data at intermediate
nodes, aiding the identification of the fault location. However, the
circuit cannot be tested if a fault occurs in the scan chain. Therefore,
scan chain diagnosis is very crucial. Traditionally, many special-testerbased and hardware-based diagnostic techniques were used. Although
they provide high accuracy, they are computationally expensive and
time-consuming. Recently, software-based diagnostic methods have attracted significant attention; however, these methods do not provide
satisfactory results. ML is widely used in scan chain diagnosis to achieve
sufficient resolution and accuracy.
An unsupervised-learning model was proposed [363], where a
Bayesian model was employed for diagnosis. The failing probabilities
of each scan cell were supplied as input to the model, which partitioned
the scan cells into multiple clusters. After that, the defective scan cell is
found at the boundaries of adjacent clusters. This model yielded 100%
accuracy for both permanent and intermittent faults, although only for
single stuck-at faults. ANNs have come into use recently, providing
sufficient resolution and accuracy. For example, in [364], a coarse
global neural network was used to select several suspected scan cells
(affine group) from all the scan-chain cells, and a refined local neural
network to identify the final suspected scan cell in the affine group.
This successively refined focus increased the resolution and accuracy
but significantly increased the training time due to multiple networks.
A two-stage NN model was proposed to identify the exact location
of a stuck-at-fault and transition fault in [365]. The 1st stage ANN
trained with entire scan data with all faults predicts a scan window
with successive candidates. The 2nd stage ANN analyzes the fail data
locally to identify the exact fault location.
Liu et al. proposed RF classification to predict test chip design
exploration synthesis outcomes [366]. In [367], a DT-based screening
method is proposed to predict unreliable dies that would fail the
HTOL (high-temperature operating life) test [367]. The HTOL test is
a popular test to determine the deviceâ€™s intrinsic reliability and predict
the deviceâ€™s long-term failure rate and lifetime of the device [368]. SVM
and autoencoder-based early stage system level testing (SLT) failure
estimation reduces the testing cost by 40% with a minor impact on
defective parts per million (DPPM) [369]. In addition, adaptive test
methods that analyze the failing data and test logs, dynamically reorder
the test patterns and adjust the testing process bring down the testing
cost by several orders [370,371].
The state-of-the-art DL for IC test (GCNs (Graph Convolutional
Networks) and ANNs in particular) is discussed in [372]. The work
systematically investigates the robustness of ML metrics and models in
the context of IC testing and highlights the opportunities and challenges
in adopting them. A novel physics-informed neural network (PINN) to
model electrostatic problems for VLSI modeling applications achieves
an error rate of 9.3% in electric potential estimation without labeled
data and yields 5.7% error with the assistance of a limited number of
coarse labeling data [373]. The paper also highlights the implementation of ML models for data exploration for IC testing and reliability
analysis. In a survey of ML applications on analog and digital IC testing,
significant challenges and opportunities are presented [374].
In essence, the key discoveries in this section can be summarized as
follows: the utilization of pattern recognition techniques and clustering
algorithms assists in identifying potentially hazardous power patterns
by analyzing IC switching activity, which leads to the generation of
effective test patterns for improved fault coverage. The measurement
of performance variability in a chip design can be accomplished using
supervised learning algorithms. Among these algorithms, deep neural
networks, specifically GNNs, are highly suitable for fault simulation
analysis, fault detection, and diagnosis. Through automated learning
procedures that analyze failure data and test logs, it is possible to
estimate the device failure rate and lifetime. Successful implementation
of these labor-intensive manual testing procedures significantly reduces
chip testing costs by several orders of magnitude.
11. Sources of training data for AI/ML-VLSI
The techniques of AI/ML would aid in solving many challenges in
the IC industry. Nevertheless, the limited data availability for training
the necessary algorithms is a known difficulty in the VLSI domain.
However, there is a plethora of tools for designing, manufacturing and
testing VLSI circuits, but a systematical way of capturing relevant and
sufficient data for training AI/ML algorithms still needs to be solved.
A structured methodology for automated data capture across different
design levels needs to be incorporated into the IC design flow to resolve
the challenge of data scarcity to a certain extent.
This section presents a brief on sources of training data explored
and implemented in literature for future research interest (Table 1).
Integration 93 (2023) 102048
23
D. Amuru et al.
Table 1
Sources of training data in the literature.
Abstraction level Analysis/modeling Proposed models in
literature
Tools/datasets used for modeling
Transistor/Gate
Statistical static timing
analysis
and leakage modeling
Transistor model
characterization
and sensitivity analysis
HSPICE MC simulations [61,62,65,76]
TCAD Sentaurus [69,70]
Cadence encounter library characterizer [77,78]
Delay estimation MATLAB + HSPICE simulations [85]
HSPICE simulations [94,103,108]
Analytical leakage models TCAD quasi + MC simulations + MATLAB [79]
Leakage power estimation HSPICE MC simulations [80,82,91,93,94,103,108]
Delay, power, energy
dissipation
HSPICE MC simulations [89]
Total power HSPICE MC simulations [91]
Circuit/RTL Timing error/ Power
prediction
Power estimation
ISCASâ€™89 benchmark circuits [99]
ISCASâ€™85 benchmark circuits [101]
SPICE MC simulations [105,107]
Static timing error
prediction
Synopsys design compiler (frontend),
Synopsys IC compiler (backend), Synopsys
Prime-Time (voltage & Temperature scaling) [109]
Mentor Graphics ModelSim
(Post-layout simulation with SDF annotation) [110]
SPICE MC simulations [99,130]
IR drop estimation
Ansys RedHawk, ITC/IWS benchmark circuits
[122]
SPICE simulations [124]
IBM power grid benchmarks [128]
SOC/System
SOC design/ Reliability
analysis/ Hardware
attacks
Power prediction and
performance analysis
SPECjbb, SPEC2k benchmarks [174,176]
SPEC CPU 2000 benchmarks [175]
Dual engine SoC for face
analysis
FDDB benchmark [180]
Timing analysis Synopsys IC compiler [184]
Synopsys Design compiler [187]
Aging and reliability
analysis
Synopsys Design compiler (logic synthesis),
Synopsys Prime-Time (critical path extraction),
HSPICE MOSRA (NBTI) [310]
SPICE MC simulations [311,326]
COSMOL [314,315]
Circuit classification Bencgen tool [320]
Hardware trojan Synopsys Design Compiler NXT industrial tool,
Trust-HUB benchmark suite [323]
Reverse engineering EPFL, ISCAS-85, 74x series benchmarks [324]
Placement and
Routing
Placement/Routing
violations
Complete placement using
VQE
Open-source quantum computing library,
Qiskit by IBM, North Carolina placement circuit
benchmark suite [200]
Detailed routing violations Ehplacer (for placement),
Mentor Graphics Olympus router [228]
Routing congestion
prediction
Xilinx Vivado 2018.3 toolset (P & R),
Xilinx Ultrascale Virtex (mapping), ISPD 2016
FPGA placement benchmarks [238]
Pre-routing timing
prediction
ITCâ€™99 benchmarks, Cadence encounter,
Synopsys Prime-Time [240]
Lithography and
Testing
Mask
synthesis/verification,
functional verification,
ATPG
Hotspot detection ICCAD 2012 benchmarks [282,286]
Pattern generation Layout from ICCAD 2014,
CAD contest benchmark sets [302]
Post-fabrication wafer map
defect
WM-811K industrial wafer dataset [305]
Edge proximity correction Proteus 9 (parameter extraction) [261]
ANN-guided ATPG, IC
Testing
Mentorâ€™s Tessent 2019.1 DFT tool,
ABC (scanning and synthesizing benchmark
circuits), publicly available ISCAS & EPFL
benchmarks [372]
Integration 93 (2023) 102048
24
D. Amuru et al.
SIS is an interactive tool for synthesizing and optimizing sequential circuits that produce an optimized netlist in the target technology [375].
Benchmark circuits to analyze hardware security are available at TrustHUB [376]. The research community utilized EDA tools from Cadence,
Synopsys, and Mentor Graphics, while ISCAS and ISPD benchmarks
were used by many to generate training datasets and for testing/model
validation. Some data repositories, such as IEEE Dataport [377], might
be helpful to a certain extent. There is a drastic scarcity of publicly
available data, particularly at the physical design level, such as lithography mask images for pattern and fault detection. However, it is
necessary to reach a resolution within the VLSI industry to establish
a bench-marking system that ensures data is accessible to the public
for research and development purposes.
12. Challenges and opportunities for AI/ML in VLSI
AI/ML is gaining significance in the VLSI design process owing to intricate chip designs and the need for faster time to market. Within VLSI,
there are various challenges that can be addressed, including power
optimization, design productivity, manufacturing variability, and testability. At each stage of the design cycle, there are both obstacles and
potential for progress, highlighting the importance of identifying these
factors and exploring suitable automated methods. By harnessing the
capabilities of AI/ML in VLSI, designs can be improved, performance
can be enhanced, and new possibilities can be unlocked. This section
provides a comprehensive overview of the design challenges in VLSI
and the opportunities offered by AI/ML to innovate in this domain.
The dimensions of devices are decreasing; however, as we approach
atomic dimensions, many aspects of their performance deteriorate,
e.g., leakage increases (particularly in the sub-threshold region), gain
decreases, and sensitivity to fluctuations in manufacturing processes
increase drastically [7]. This results in less reliability and yield. The
growing process variability and environmental sources of variation in
nanometer technology are leading to the deterioration of the overall
circuit performance. Modeling these effects based on worst-case process
corners would no longer be valid as most parameters vary statistically.
Moreover, many parameters exhibit complex correlations and wide
variances.
Presently, the computationally efficient methods for estimating the
outputs corresponding to the inputs are some of the areas attracting significant interest in the field of circuit modeling of VLSIâ€“CAD.
To maximize chip reliability and yield, each design in VLSI is optimally tuned to consume and dissipate low power, occupy a minimum
area, and achieve high throughput. Device models coupled with circuit
simulation tools significantly improve design productivity, providing
insights for improving the design choices and circuit performance [7].
Accurate and fast estimation techniques are required during circuit
design and modeling to estimate and verify the effect of the process variations on the circuit output; this can aid the incorporation
of corrective measures/methodologies to improve the yield, thereby
guaranteeing the design quality. The primary challenge under process
variations is to identify the dominant parameters causing the variations, estimate the relationship between the dominant parameters and
the circuit performance parameters, develop models for performance
evaluation, and incorporate these models into design tools. This problem is more pronounced in the nanometer regime with the increased
complexity of digital design. Traditional models estimating the circuit performance comprise many parameters and have complicated
equations that significantly slow the simulation speed. At the current
technology nodes, there is a need for compact device models with
essential capabilities of scalability and universality (i.e., the ability
to support different technologies). Nevertheless, one can see many
opportunities to address these challenges.
Surrogate ML/AI models provide solutions to these problems. These
models forecast the device performance and can be easily extended to
circuit-level and system-level design and analysis. Such models have
been proposed in the literature to improve the turnaround time, and
yield of ICs [91,93]. This learning methodology can also be applied
for post-layout simulation and ECO, aiding the achievement of timing
closure [120]. Surrogate AI/ML models offer comparable simulation
rates to traditional EDA tools with reasonable accuracy. Potential risks
in the advanced silicon nodes can be estimated and analyzed with
prior design data using ML algorithms. These algorithms can better
capture complex electrical behavior in advanced technology nodes
than traditional EDA tools. The best methodologies for incorporating
computationally effective AI/ML models into VLSIâ€“CAD design tools
need to be explored.
Estimation and analysis of the subsystem behavior are also crucial
in IC technology. For instance, accurately estimating the subsystem
power consumption of commercial smartphones is necessary for various
applications in many research areas. In this regard, learning algorithms
will improve the end-to-end performance, promoting a high utilization
ratio and high data bandwidth [378]. Memory designs on nanometer
technology nodes are becoming increasingly challenging because they
are the smallest devices on the chip and are thus affected the most
in terms of functionality and yield. Increasing the inter-die and intradie variabilities will exacerbate the cell-stability concerns. The AI/ML
approach also increases the statistical analysis rate of memory designs.
The learning strategies of AI/ML have been extended to high-level
SoC designs [379] in the past. Kong and Bohr [380,381] discuss the
survey of design challenges in the nanometer regime. A vast network
of AI is employed in hardware acceleration to implement dynamic
high-level digital circuits onto the hardware. High-speed VLSI hardware
systems provide the necessary driving capability for AI/ML algorithms
to achieve their maximum potential. Dense NNs used extensively in
embedded systems, such as IoT sensors, cars, and cameras, need high
classification speeds, which are possible with high-speed hardware
accelerators [382,383]. In-memory-based computing by IBM [384]
demonstrates how the completion speed of tasks by ML can be increased significantly with reduced power consumption. The realization
of AI/ML learning algorithms in hardware reduces the learning time
and increases the speed of the prediction process by many orders
of magnitude. Interconnect datasets need to be built carefully with
many SoC parameters, floor planning, routing constraints, and clock
characteristics, creating ample design space for exploration. Current
GPUs offer high acceleration rates with parallel computing facilities and
superior performance for large design spaces.
Present ASIC design methodologies break down in light of the new
economic and technological realities; new design methodologies are
required for which the physical implementation of the design is more
predictable. A database and interface, from design-to-manufacturing to
effectively managing the parameter variability and increasing the data
volume, must be provided [380]. A paradigm shift in CAD tool research
is required to manage complex functional and physical variabilities.
The AI/ML algorithms can be unfolded to the CAD-tool methodologies
in physical design to manage the involved complexities. Data mining
approaches, such as clustering and classification, can be imbibed into
VLSI partitioning, paving a new route for recognizing hidden patterns
in data and predicting the relationships between the attributes that
enable forecasting outcomes [385].
Similarly, learning strategies can be applied to find cost-effective
solutions for placement and routing [23]. Reducing the design cost of
an IC is the primary driving force for downscaling [386]. The continued
shrinkage of logic devices has brought about new challenges in chip
manufacturing. It is increasingly difficult to resolve fine patterns and
place them accurately on the die, particularly at sizes below 20 nm. ML
techniques can be utilized in the chip-manufacturing-process-optimized
compact-patterning models in the lithography process, mask synthesis,
and correction and can be extended to physical verification to validate
the designâ€™s manufacturability. For automated recovery and repair and
big data debugging, the challenges in chip manufacturing need to be
addressed. Post-silicon validation is also possible using ML algorithms
Integration 93 (2023) 102048
25
D. Amuru et al.
with available training data from the pre-silicon stage. The cost of
testing a VLSI chip/subsystem can be reduced using AI algorithms. For
instance, finding an efficient solution for rearranging the test cases
using AI heuristic search algorithms can reduce power dissipation
during testing [387].
Having stated that there are many critical problems, such as high
variability and deteriorated reliability, a wide variety of AI and ML approaches â€” supervised/unsupervised/semi-supervised learning; NNs,
MLP structures [43,59,388] - CNNs and deep learning [389], provide
opportunities to solve the numerous problems and challenges in the
field of VLSI design. There is a trade-off between selecting suitable
algorithms and architectures with available training data and other
model constraints. Fitting these new techniques in the classical flow of
VLSI is another big challenge. Another issue is the availability of standardized, licensed ML algorithms with a thorough debugging facility.
High-yielding implementations are achievable by critically channeling
ML designersâ€™ domain knowledge with CAD designers.
The availability of limited training data can be maximally solved if
the data flow across the design cycle can be effectively captured and
explored. The chip designing industries should understand the importance of systematic generation, the capture of data, the incorporation
of distributed bid data systems for chip workflows, and data-driven
optimizations to accelerate the quality, cost, and time of results [390].
It could be beneficial to have benchmark datasets for AI/ML training for
future research and automated IC design flow development. To address
the dearth of training data, the critical challenge for employing AI/ML,
standardized statistical training data for circuit modeling should be
developed. Such open-source contributions in the VLSI community help
to address the challenges more effectively. Further, researchers can use
them as a baseline and rapidly progress [391].
Different abstraction levels in the design flow, ranging from circuit
design to chip fabrication and testing, inherently comprise numerous
models relating inputs to outputs. An enormous amount of data flows
across billions of devices or components integrated/to be integrated
on the chip [23]. The complex I/O relationships between the components, processes and various abstraction levels within each abstraction
level can be explored via AI/ML algorithms using the information
accumulated during different kinds of simulations/analyses. Further,
we need to analyze the data streams associated with file operations,
which clustering algorithms can use to deliver high application performance. AI/ML solutions can be employed in VLSIâ€“CAD for design-flow
optimization.
Future advancements in differential programming and quantum ML
approaches can lead to incredible breakthroughs in the EDA industry.
Declaration of competing interest
The authors declare the following financial interests/personal relationships which may be considered as potential competing interests:
Zia Abbas reports administrative support was provided by International
Institute of Information Technology, Hyderabad. Zia Abbas reports
a relationship with International Institute of Information Technology
Hyderabad that includes: employment.
Data availability
Data will be made available on request.
References
[1] J. Carballo, W.J. Chan, P.A. Gargini, A.B. Kahng, S. Nath, ITRS 2.0: Toward
a re-framing of the Semiconductor Technology Roadmap, in: 2014 IEEE 32nd
International Conference on Computer Design, ICCD, 2014, pp. 139â€“146, http:
//dx.doi.org/10.1109/ICCD.2014.6974673.
[2] G.E. Moore, Cramming more components onto integrated circuits, Reprinted
from Electronics, volume 38, number 8, April 19, 1965, 114 ff, IEEE SolidState Circuits Soc. Newslett. 11 (3) (2006) 33â€“35, http://dx.doi.org/10.1109/NSSC.2006.4785860.
[3] H..P. Wong, D.J. Frank, P.M. Solomon, C.H.J. Wann, J.J. Welser, Nanoscale
CMOS, Proc. IEEE 87 (4) (1999) 537â€“570, http://dx.doi.org/10.1109/5.752515.
[4] R. Vaddi, S. Dasgupta, R.P. Agarwal, Device and circuit design challenges in
the Digital Subthreshold Region for ultralow-power applications, VLSI Des. 2009
(2009) http://dx.doi.org/10.1155/2009/283702.
[5] D. Sylvester, H. Kaul, Power-driven challenges in nanometer design, IEEE Des.
Test 18 (6) (2001) 12â€“22, http://dx.doi.org/10.1109/54.970420.
[6] H. Iwai, Logic LSI technology roadmap for 22 nm and beyond, in: 2009
16th IEEE International Symposium on the Physical and Failure Analysis
of Integrated Circuits, 2009, pp. 7â€“10, http://dx.doi.org/10.1109/IPFA.2009.
5232710.
[7] B.H. Calhoun, Y. Cao, X. Li, K. Mai, L.T. Pileggi, R.A. Rutenbar, K.L. Shepard,
Digital circuit design challenges and opportunities in the era of nanoscale
CMOS, Proc. IEEE 96 (2) (2008) 343â€“365.
[8] M.H. Abu-Rahma, M. Anis, Variability in VLSI circuits: Sources and design
considerations, in: 2007 IEEE International Symposium on Circuits and Systems,
2007, pp. 3215â€“3218, http://dx.doi.org/10.1109/ISCAS.2007.378156.
[9] S. Chaudhuri, N.K. Jha, FinFET Logic circuit optimization with different FinFET
styles: Lower power possible at higher supply voltage, in: 2014 27th International Conference on VLSI Design and 2014 13th International Conference on
Embedded Systems, 2014, pp. 476â€“482, http://dx.doi.org/10.1109/VLSID.2014.
89.
[10] R.S. Rathore, A.K. Rana, R. Sharma, Threshold voltage variability induced by
statistical parameters fluctuations in nanoscale bulk and SOI FinFETs, in: 2017
4th International Conference on Signal Processing, Computing and Control,
ISPCC, 2017, pp. 377â€“380, http://dx.doi.org/10.1109/ISPCC.2017.8269707.
[11] A.R. Brown, N. Daval, K.K. Bourdelle, B. Nguyen, A. Asenov, Comparative
simulation analysis of process-induced variability in nanoscale SOI and bulk
trigate FinFETs, IEEE Trans. Electron Devices 60 (11) (2013) 3611â€“3617,
http://dx.doi.org/10.1109/TED.2013.2281474.
[12] M. Belleville, O. Thomas, A. Valentian, F. Clermidy, Designing digital circuits with nano-scale devices: Challenges and opportunities, SolidState Electron. 84 (2013) 38â€“45, http://dx.doi.org/10.1016/j.sse.2013.02.030,
URL: https://www.sciencedirect.com/science/article/pii/S0038110113000919.
Selected Papers from the ESSDERC 2012 Conference.
[13] L. Wang, M. Luo, Machine learning applications and opportunities in IC design
flow, in: 2019 International Symposium on VLSI Design, Automation and Test,
VLSI-DAT, 2019, pp. 1â€“3.
[14] C.K.C. Lee, Deep learning creativity in EDA, in: 2020 International Symposium
on VLSI Design, Automation and Test, VLSI-DAT, 2020, p. 1.
[15] R.S. Kirk, The impact of AI technology on VLSI design, in: Managing Requirements Knowledge, International Workshop on, IEEE Computer Society, 1985,
p. 125.
[16] G. Rabbat, VLSI and AI are getting closer, IEEE Circuits Dev. Mag. 4 (1) (1988)
15â€“18.
[17] M.Z.A. Khan, H. Saleem, S. Afzal, Application of VLSI in artificial intelligence,
2012.
[18] J. Delgado-Frias, W. Moore, VLSI for Artificial Intelligence, Vol. 68, 1989,
http://dx.doi.org/10.1007/978-1-4613-1619-0.
[19] L. Capodieci, Data analytics and machine learning for design-process-yield optimization in electronic design automation and IC semiconductor manufacturing,
in: 2017 China Semiconductor Technology International Conference, CSTIC,
2017, pp. 1â€“3.
[20] A.B. Kahng, Machine learning applications in physical design: Recent results and
directions, in: Proceedings of the 2018 International Symposium on Physical
Design, 2018, pp. 68â€“73.
[21] P.A. Beerel, M. Pedram, Opportunities for machine learning in electronic design
automation, in: 2018 IEEE International Symposium on Circuits and Systems,
ISCAS, 2018, pp. 1â€“5.
[22] H.-G. Stratigopoulos, Machine learning applications in IC testing, in: 2018 IEEE
23rd European Test Symposium, ETS, IEEE, 2018, pp. 1â€“10.
[23] I.A. Elfadel, D. Boning, X. Li, Machine Learning in VLSI Computer-Aided Design,
2019, http://dx.doi.org/10.1007/978-3-030-04666-8.
[24] B. Khailany, H. Ren, S. Dai, S. Godil, B. Keller, R. Kirby, A. Klinefelter,
R. Venkatesan, Y. Zhang, B. Catanzaro, et al., Accelerating chip design with
machine learning, IEEE Micro 40 (6) (2020) 23â€“32.
[25] C. Schuermyer, Deploying new nodes faster with machine learning for IC design
and manufacturing, in: 2019 International Symposium on VLSI Technology,
Systems and Application, VLSI-TSA, 2019, pp. 1â€“3, http://dx.doi.org/10.1109/
VLSI-TSA.2019.8804650.
[26] M. Rapp, H. Amrouch, Y. Lin, B. Yu, D.Z. Pan, M. Wolf, J. Henkel, MLCAD:
A survey of research in machine learning for CAD keynote paper, IEEE Trans.
Comput.-Aided Des. Integr. Circuits Syst. (2021) 1, http://dx.doi.org/10.1109/
TCAD.2021.3124762.
[27] G. Huang, J. Hu, Y. He, J. Liu, M. Ma, Z. Shen, J. Wu, Y. Xu, H. Zhang, K.
Zhong, X. Ning, Y. Ma, H. Yang, B. Yu, H. Yang, Y. Wang, Machine learning
for electronic design automation: A survey, ACM Trans. Des. Autom. Electron.
Syst. 26 (5) (2021) http://dx.doi.org/10.1145/3451179.
Integration 93 (2023) 102048
26
D. Amuru et al.
[28] D.S. Lopera, L. Servadei, G.N. Kiprit, S. Hazra, R. Wille, W. Ecker, A survey of
graph neural networks for electronic design automation, in: 2021 ACM/IEEE
3rd Workshop on Machine Learning for CAD, MLCAD, 2021, pp. 1â€“6, http:
//dx.doi.org/10.1109/MLCAD52597.2021.9531070.
[29] Y. Ma, Z. He, W. Li, L. Zhang, B. Yu, Understanding graphs in EDA: From
shallow to deep learning, in: Proceedings of the 2020 International Symposium
on Physical Design, Association for Computing Machinery, New York, NY, USA,
2020, pp. 119â€“126, http://dx.doi.org/10.1145/3372780.3378173.
[30] V. Hamolia, V. Melnyk, A survey of machine learning methods and applications
in electronic design automation, in: 2021 11th International Conference on
Advanced Computer Information Technologies, ACIT, 2021, pp. 757â€“760, http:
//dx.doi.org/10.1109/ACIT52158.2021.9548117.
[31] A. Malhotra, A. Singh, Implementation of AI in the field of VLSI: A review,
in: 2022 Second International Conference on Power, Control and Computing
Technologies, ICPC2T, 2022, pp. 1â€“5, http://dx.doi.org/10.1109/ICPC2T53885.
2022.9776845.
[32] M. Bansal, Priya, Machine learning perspective in VLSI computer-aided design at different abstraction levels, in: S. Shakya, R. Bestak, R. Palanisamy,
K.A. Kamel (Eds.), Mobile Computing and Sustainable Informatics, Springer,
Singapore, 2022, pp. 95â€“112.
[33] A.F. Budak, Z. Jiang, K. Zhu, A. Mirhoseini, A. Goldie, D.Z. Pan, Reinforcement learning for electronic design automation: Case studies and perspectives:
(invited paper), in: 2022 27th Asia and South Pacific Design Automation Conference, ASP-DAC, 2022, pp. 500â€“505, http://dx.doi.org/10.1109/ASP-DAC52403.
2022.9712578.
[34] L.-T. Wang, C.-W. Wu, X. Wen, VLSI Test Principles and Architectures: Design
for Testability, Morgan Kaufmann Publishers Inc. San Francisco, CA, USA, 2006.
[35] N. Weste, D. Harris, CMOS VLSI Design: A Circuits and Systems Perspective,
fourth ed., Addison-Wesley Publishing Company, USA, 2010.
[36] S.M.S.M. Sze, in: S. Sze (Ed.), VLSI Technology, second ed., in: McGraw-Hill Series in Electrical Engineering. Electronics and Electronic Circuits, McGraw-Hill,
New York, 1988.
[37] S. Mitra, S.A. Seshia, N. Nicolici, Post-silicon validation opportunities, challenges and recent advances, in: Proceedings of the 47th Design Automation
Conference, DAC â€™10, Association for Computing Machinery, New York, NY,
USA, 2010, pp. 12â€“17, http://dx.doi.org/10.1145/1837274.1837280.
[38] E. Alpaydin, Introduction to Machine Learning, MIT Press, 2020.
[39] J. Han, M. Kamber, J. Pei, 8 - classification: Basic concepts, in: J. Han, M.
Kamber, J. Pei (Eds.), Data Mining, third ed., in: The Morgan Kaufmann
Series in Data Management Systems, Morgan Kaufmann, Boston, 2012, pp.
327â€“391, http://dx.doi.org/10.1016/B978-0-12-381479-1.00008-3, URL: https:
//www.sciencedirect.com/science/article/pii/B9780123814791000083.
[40] S.B. Kotsiantis, I. Zaharakis, P. Pintelas, Supervised machine learning: A review
of classification techniques, in: Emerging Artificial Intelligence Applications in
Computer Engineering, Vol. 160, No. 1, Amsterdam, 2007, pp. 3â€“24.
[41] T.G. Dietterich, Machine-learning research, AI Mag. 18 (4) (1997) 97.
[42] T.G. Dietterich, Ensemble methods in machine learning, in: Multiple Classifier
Systems, Springer Berlin Heidelberg, Berlin, Heidelberg, 2000, pp. 1â€“15.
[43] T. Hastie, R. Tibshirani, J. Friedman, The elements of statistical learning â€“ data
mining, inference, and prediction, 2008.
[44] R. Xu, D. Wunsch, Clustering, Vol. 10, John Wiley & Sons, 2008.
[45] Semi-supervised classification using pattern clustering, in: Semi-Supervised
and Unsupervised Machine Learning, John Wiley & Sons, Ltd, 2013,
pp. 127â€“181, http://dx.doi.org/10.1002/9781118557693.ch4, URL: https://
onlinelibrary.wiley.com/doi/abs/10.1002/9781118557693.ch4.
[46] O. Chapelle, B. SchÃ¶lkopf, A. Zien, Introduction to semi-supervised learning, in:
Semi-Supervised Learning, 2006, pp. 1â€“12.
[47] R.S. Sutton, A.G. Barto, Reinforcement Learning: An Introduction, A Bradford
Book, Cambridge, MA, USA, 2018.
[48] I. Goodfellow, Y. Bengio, A. Courville, Deep Learning, MIT Press, 2016, http:
//www.deeplearningbook.org.
[49] H. Yi, S. Shiyu, D. Xiusheng, C. Zhigang, A study on deep neural networks
framework, in: 2016 IEEE Advanced Information Management, Communicates,
Electronic and Automation Control Conference, IMCEC, 2016, pp. 1519â€“1522,
http://dx.doi.org/10.1109/IMCEC.2016.7867471.
[50] E. Nishani, B. Ã‡iÃ§o, Computer vision approaches based on deep learning and
neural networks: Deep neural networks for video analysis of human pose
estimation, in: 2017 6th Mediterranean Conference on Embedded Computing,
MECO, 2017, pp. 1â€“4, http://dx.doi.org/10.1109/MECO.2017.7977207.
[51] B. Chandra, R.K. Sharma, On improving recurrent neural network for image
classification, in: 2017 International Joint Conference on Neural Networks,
IJCNN, 2017, pp. 1904â€“1907, http://dx.doi.org/10.1109/IJCNN.2017.7966083.
[52] A. Sinha, M. Jenckel, S.S. Bukhari, A. Dengel, Unsupervised OCR model
evaluation using GAN, in: 2019 International Conference on Document Analysis
and Recognition, ICDAR, 2019, pp. 1256â€“1261, http://dx.doi.org/10.1109/
ICDAR.2019.00-42.
[53] I. Goodfellow, J. Pouget-Abadie, M. Mirza, B. Xu, D. Warde-Farley, S. Ozair,
A. Courville, Y. Bengio, Generative adversarial nets, in: Advances in Neural
Information Processing Systems, 2014, pp. 2672â€“2680.
[54] A. Jeerige, D. Bein, A. Verma, Comparison of deep reinforcement learning
approaches for intelligent game playing, in: 2019 IEEE 9th Annual Computing
and Communication Workshop and Conference, CCWC, 2019, pp. 0366â€“0371,
http://dx.doi.org/10.1109/CCWC.2019.8666545.
[55] A. Zjajo, Stochastic Process Variation in Deep-Submicron CMOS, Springer, 2016.
[56] S. Shukla, S.S. Gill, N. Kaur, H. Jatana, V. Nehru, Comparative simulation
analysis of process parameter variations in 20 nm triangular FinFET, Act.
Passive Electron. Compon. 2017 (2017).
[57] Z. Abbas, M. Olivieri, Impact of technology scaling on leakage power in
nano-scale bulk CMOS digital standard cells, Microelectron. J. 45 (2) (2014)
179â€“195.
[58] M. Olivieri, A. Mastrandrea, Logic drivers: A propagation delay modeling
paradigm for statistical simulation of standard cell designs, IEEE Trans. Very
Large Scale Integr. (VLSI) Syst. 22 (6) (2013) 1429â€“1440.
[59] C.M. Bishop, Pattern Recognition and Machine Learning (Information Science
and Statistics), Springer-Verlag, Berlin, Heidelberg, 2006.
[60] P. Cox, P. Yang, S.S. Mahant-Shetti, P. Chatterjee, Statistical modeling for
efficient parametric yield estimation of MOS VLSI circuits, IEEE Trans. Electron
Devices 32 (2) (1985) 471â€“478.
[61] A.R. Alvarez, B.L. Abdi, D.L. Young, H.D. Weed, J. Teplik, E.R. Herald,
Application of statistical design and response surface methods to computeraided VLSI device design, IEEE Trans. Comput.-Aided Des. Integr. Circuits Syst.
7 (2) (1988) 272â€“288.
[62] D.L. Young, J. Teplik, H.D. Weed, N.T. Tracht, A.R. Alvarez, Application
of statistical design and response surface methods to computer-aided VLSI
device design II. Desirability functions and Taguchi methods, IEEE Trans.
Comput.-Aided Des. Integr. Circuits Syst. 10 (1) (1991) 103â€“115.
[63] R.H. Myers, D.C. Montgomery, G.G. Vining, C.M. Borror, S.M. Kowalski,
Response surface methodology: A retrospective and literature survey, J. Qual.
Technol. 36 (1) (2004) 53â€“77, http://dx.doi.org/10.1080/00224065.2004.
11980252.
[64] R. Myers, D. Montgomery, C. Anderson-Cook, Response Surface Methodology:
Process and Product Optimization using Designed Experiments, in: Wiley Series
in Probability and Statistics, Wiley, 2016, URL: https://books.google.co.in/
books?id=vOBbCwAAQBAJ.
[65] M.A.H. Khan, A.S.M.Z. Rahman, T. Muntasir, U.K. Acharjee, M.A. Layek,
Multiple polynomial regression for modeling a MOSFET in saturation to validate
the Early voltage, in: 2011 IEEE Symposium on Industrial Electronics and
Applications, 2011, pp. 261â€“266.
[66] Y.S. Chauhan, S. Venugopalan, M.A. Karim, S. Khandelwal, N. Paydavosi, P.
Thakur, A.M. Niknejad, C.C. Hu, BSIM â€” Industry standard compact MOSFET
models, in: 2012 Proceedings of the ESSCIRC, ESSCIRC, 2012, pp. 30â€“33,
http://dx.doi.org/10.1109/ESSCIRC.2012.6341249.
[67] Z. Abbas, M. Olivieri, Optimal transistor sizing for maximum yield in variationaware standard cell design, Int. J. Circuit Theory Appl. 44 (7) (2016)
1400â€“1424.
[68] T.-L. Wu, S.B. Kutub, Machine learning-based statistical approach to analyze
process dependencies on threshold voltage in recessed gate AlGaN/GaN MISHEMTs, IEEE Trans. Electron Devices 67 (12) (2020) 5448â€“5453, http://dx.doi.
org/10.1109/TED.2020.3032634.
[69] G. Choe, P.V. Ravindran, A. Lu, J. Hur, M. Lederer, A. Reck, S. Lombardo,
N. Afroze, J. Kacher, A.I. Khan, S. Yu, Machine learning assisted statistical
variation analysis of ferroelectric transistors: From experimental metrology to
predictive modeling, in: 2022 IEEE Symposium on VLSI Technology and Circuits
(VLSI Technology and Circuits), 2022, pp. 336â€“337, http://dx.doi.org/10.1109/
VLSITechnologyandCir46769.2022.9830392.
[70] M.-Y. Kao, H. Kam, C. Hu, Deep-learning-assisted physics-driven MOSFET
current-voltage modeling, IEEE Electron Device Lett. 43 (6) (2022) 974â€“977,
http://dx.doi.org/10.1109/LED.2022.3168243.
[71] M. Choi, X. Xu, V. Moroz, Modeling performance and thermal induced reliability issues of a 3nm FinFET logic chip operation in a fan-out and a
flip-chip packages, in: 2019 18th IEEE Intersociety Conference on Thermal
and Thermomechanical Phenomena in Electronic Systems, ITherm, 2019, pp.
107â€“112.
[72] S.J. Pan, Q. Yang, A survey on transfer learning, IEEE Trans. Knowl. Data Eng.
22 (10) (2010) 1345â€“1359.
[73] F. Zhuang, Z. Qi, K. Duan, D. Xi, Y. Zhu, H. Zhu, H. Xiong, Q. He, A
comprehensive survey on transfer learning, Proc. IEEE 109 (1) (2021) 43â€“76,
http://dx.doi.org/10.1109/JPROC.2020.3004555.
[74] A.A. Mutlu, M. Rahman, Statistical methods for the estimation of process
variation effects on circuit operation, IEEE Trans. Electron. Packag. Manuf. 28
(4) (2005) 364â€“375.
[75] S. Basu, P. Thakore, R. Vemuri, Process variation tolerant standard cell library
development using reduced dimension statistical modeling and optimization
techniques, in: 8th International Symposium on Quality Electronic Design,
ISQEDâ€™07, 2007, pp. 814â€“820.
[76] L. Brusamarello, G. Wirth, P. Roussel, M. Miranda, Fast and accurate statistical
characterization of standard cell libraries, Microelectron. Reliab. 51 (2011)
2341â€“2350, http://dx.doi.org/10.1016/j.microrel.2011.05.016.
Integration 93 (2023) 102048
27
D. Amuru et al.
[77] M. Miranda, P. Roussel, L. Brusamarello, G. Wirth, Statistical characterization
of standard cells using design of experiments with response surface modeling,
in: 2011 48th ACM/EDAC/IEEE Design Automation Conference, DAC, 2011, pp.
77â€“82.
[78] M. Miranda, P. Zuber, P. DobrovolnÃ½, P. Roussel, Variability aware modeling
for yield enhancement of SRAM and logic, in: 2011 Design, Automation Test
in Europe, 2011, pp. 1â€“6.
[79] S. Chaudhuri, P. Mishra, N.K. Jha, Accurate leakage estimation for FinFET standard cells using the response surface methodology, in: 2012 25th International
Conference on VLSI Design, IEEE, 2012, pp. 238â€“244.
[80] L. Cao, Circuit power estimation using pattern recognition techniques, in: Proceedings of the 2002 IEEE/ACM International Conference on Computer-Aided
Design, 2002, pp. 412â€“417.
[81] L. Yu, S. Saxena, C. Hess, I.A.M. Elfadel, D. Antoniadis, D. Boning, Statistical
library characterization using belief propagation across multiple technology
nodes, in: 2015 Design, Automation & Test in Europe Conference & Exhibition,
DATE, IEEE, 2015, pp. 1383â€“1388.
[82] L. Cheng, P. Gupta, L. He, Efficient additive statistical leakage estimation, IEEE
Trans. Comput.-Aided Des. Integr. Circuits Syst. 28 (11) (2009) 1777â€“1781,
http://dx.doi.org/10.1109/TCAD.2009.2030433.
[83] MCNC designersâ€™ manual, 1993, URL: https://www.carolana.com/NC/NC_
Manuals/NC_Manual_1993_1994.pdf.
[84] H. Chang, S. Sapatnekar, Full-chip analysis of leakage power under process variations, including spatial correlations, in: Proceedings. 42nd Design Automation
Conference, 2005, 2005, pp. 523â€“528, http://dx.doi.org/10.1109/DAC.2005.
193865.
[85] A. Moshrefi, H. Aghababa, O. Shoaei, Statistical estimation of delay in nanoscale CMOS circuits using Burr Distribution, Microelectron. J. 79 (2018)
30â€“37.
[86] T.-T. Liu, J.M. Rabaey, Statistical analysis and optimization of asynchronous
digital circuits, in: 2012 IEEE 18th International Symposium on Asynchronous
Circuits and Systems, IEEE, 2012, pp. 1â€“8.
[87] K.J. Kuhn, Considerations for ultimate CMOS scaling, IEEE Trans. Electron
Devices 59 (7) (2012) 1813â€“1828.
[88] K.J. Kuhn, CMOS transistor scaling past 32nm and implications on variation, in:
2010 IEEE/SEMI Advanced Semiconductor Manufacturing Conference, ASMC,
2010, pp. 241â€“246.
[89] A. Stillmaker, B. Baas, Scaling equations for the accurate prediction of CMOS
device performance from 180 nm to 7 nm, Integration 58 (2017) 74â€“81.
[90] Predictive technology model, 2012, URL: http://ptm.asu.edu/.
[91] D. Amuru, A. Zahra, Z. Abbas, Statistical variation aware leakage and total
power estimation of 16 nm VLSI digital circuits based on regression models, in:
A. Sengupta, S. Dasgupta, V. Singh, R. Sharma, S. Kumar Vishvakarma (Eds.),
VLSI Design and Test, Springer, Singapore, 2019, pp. 565â€“578.
[92] A. Stillmaker, Z. Xiao, B. Baas, Toward more accurate scaling estimates of CMOS
circuits from 180 nm to 22 nm, 2012.
[93] S. Gourishetty, H. Mandadapu, A. Zahra, Z. Abbas, A highly accurate machine
learning approach to modelling PVT variation aware leakage power in FinFET
digital circuits, in: 2019 IEEE Asia Pacific Conference on Circuits and Systems,
APCCAS, 2019, pp. 61â€“64.
[94] D. Amuru, M.S. Ahmed, Z. Abbas, An efficient gradient boosting approach for
PVT aware estimation of leakage power and propagation delay in CMOS/FinFET
digital cells, in: 2020 IEEE International Symposium on Circuits and Systems,
ISCAS, 2020, pp. 1â€“5.
[95] M.D. Bhavesh, N.A. Anilkumar, M.I. Patel, R. Gajjar, D. Panchal, Power
consumption prediction of digital circuits using machine learning, in: 2022 2nd
International Conference on Artificial Intelligence and Signal Processing, AISP,
2022, pp. 1â€“6, http://dx.doi.org/10.1109/AISP53593.2022.9760542.
[96] V.A. Chhabria, B. Keller, Y. Zhang, S. Vollala, S. Pratty, H. Ren, B. Khailany,
XT-PRAGGMA: Crosstalk pessimism reduction achieved with GPU gate-level
simulations and machine learning, in: 2022 ACM/IEEE 4th Workshop on
Machine Learning for CAD, MLCAD, 2022, pp. 63â€“69, http://dx.doi.org/10.
1109/MLCAD55463.2022.9900084.
[97] T. Chen, V.-K. Kim, M. Tegethoff, IC yield estimation at early stages of the
design cycle, Microelectron. J. 30 (8) (1999) 725â€“732.
[98] R.R. Rao, A. Devgan, D. Blaauw, D. Sylvester, Parametric yield estimation
considering leakage variability, in: Proceedings of the 41st Annual Design
Automation Conference, DAC â€™04, Association for Computing Machinery, New
York, NY, USA, 2004, pp. 442â€“447, http://dx.doi.org/10.1145/996566.996693.
[99] L. Hou, L. Zheng, W. Wu, Neural network based VLSI power estimation, in: 2006
8th International Conference on Solid-State and Integrated Circuit Technology
Proceedings, 2006, pp. 1919â€“1921.
[100] M. Stockman, M. Awad, R. Khanna, C. Le, H. David, E. Gorbatov, U. Hanebutte,
A novel approach to memory power estimation using machine learning, in: 2010
International Conference on Energy Aware Computing, IEEE, 2010, pp. 1â€“3.
[101] V. Janakiraman, A. Bharadwaj, V. Visvanathan, Voltage and temperature aware
statistical leakage analysis framework using artificial neural networks, IEEE
Trans. Comput.-Aided Des. Integr. Circuits Syst. 29 (7) (2010) 1056â€“1069.
[102] S. Narendra, V. De, S. Borkar, D. Antoniadis, A. Chandrakasan, Full-chip subthreshold leakage power prediction model for sub-0.18 /spl mu/m CMOS, in:
Proceedings of the International Symposium on Low Power Electronics and
Design, 2002, pp. 19â€“23.
[103] R.R. Rao, A. Devgan, D. Blaauw, D. Sylvester, Analytical yield prediction
considering leakage/performance correlation, IEEE Trans. Comput.-Aided Des.
Integr. Circuits Syst. 25 (9) (2006) 1685â€“1695.
[104] H. Chang, S.S. Sapatnekar, Prediction of leakage power under process uncertainties, ACM Trans. Des. Autom. Electron. Syst. 12 (2) (2007) 12â€“es, http:
//dx.doi.org/10.1145/1230800.1230804.
[105] L. Garg, V. Sahula, Variability aware support vector machine based macromodels for statistical estimation of subthreshold leakage power, in: 2012
International Conference on Synthesis, Modeling, Analysis and Simulation
Methods and Applications to Circuit Design, SMACD, 2012, pp. 253â€“256.
[106] A.B. Kahng, M. Luo, S. Nath, SI for free: machine learning of interconnect coupling delay and transition effects, in: 2015 ACM/IEEE International Workshop
on System Level Interconnect Prediction, SLIP, 2015, pp. 1â€“8.
[107] V. Govindaraj, B. Arunadevi, Machine learning based power estimation for
CMOS VLSI circuits, Appl. Artif. Intell. 35 (13) (2021) 1043â€“1055, http://dx.
doi.org/10.1080/08839514.2021.1966885.
[108] K. Agarwal, A. Jain, D. Amuru, Z. Abbas, Fast and efficient ResNN and Genetic
optimization for PVT aware performance enhancement in digital circuits, in:
2022 International Symposium on VLSI Design, Automation and Test, VLSI-DAT,
2022, pp. 1â€“4, http://dx.doi.org/10.1109/VLSI-DAT54769.2022.9768067.
[109] A. Rahimi, L. Benini, R.K. Gupta, Hierarchically Focused Guardbanding: An
adaptive approach to mitigate PVT variations and aging, in: 2013 Design,
Automation Test in Europe Conference Exhibition, DATE, 2013, pp. 1695â€“1700.
[110] X. Jiao, A. Rahimi, B. Narayanaswamy, H. Fatemi, J.P. de Gyvez, R.K. Gupta,
Supervised learning based model for predicting variability-induced timing
errors, in: 2015 IEEE 13th International New Circuits and Systems Conference,
NEWCAS, 2015, pp. 1â€“4.
[111] A. Bogliolo, L. Benini, G. De Micheli, Regression-based RTL power modeling,
ACM Trans. Des. Autom. Electron. Syst. 5 (3) (2000) 337â€“372, http://dx.doi.
org/10.1145/348019.348081.
[112] J.H. Anderson, F.N. Najm, Power estimation techniques for FPGAs, IEEE Trans.
Very Large Scale Integr. (VLSI) Syst. 12 (10) (2004) 1015â€“1027, http://dx.doi.
org/10.1109/TVLSI.2004.831478.
[113] S. Ahuja, D.A. Mathaikutty, G. Singh, J. Stetzer, S.K. Shukla, A. Dingankar,
Power estimation methodology for a high-level synthesis framework, in: 2009
10th International Symposium on Quality Electronic Design, 2009, pp. 541â€“546,
http://dx.doi.org/10.1109/ISQED.2009.4810352.
[114] D. Sunwoo, G.Y. Wu, N.A. Patil, D. Chiou, PrEsto: An FPGA-accelerated power
estimation methodology for complex systems, in: 2010 International Conference
on Field Programmable Logic and Applications, 2010, pp. 310â€“317, http://dx.
doi.org/10.1109/FPL.2010.69.
[115] Y. Zhou, H. Ren, Y. Zhang, B. Keller, B. Khailany, Z. Zhang, PRIMAL: Power
inference using machine learning, in: 2019 56th ACM/IEEE Design Automation
Conference, DAC, 2019, pp. 1â€“6.
[116] J. Zhou, G. Cui, Z. Zhang, C. Yang, Z. Liu, L. Wang, C. Li, M. Sun, Graph neural
networks: A review of methods and applications, 2019, arXiv:1812.08434.
[117] Y. Zhang, H. Ren, B. Khailany, GRANNITE: Graph neural network inference
for transferable power estimation, in: 2020 57th ACM/IEEE Design Automation
Conference, DAC, 2020, pp. 1â€“6, http://dx.doi.org/10.1109/DAC18072.2020.
9218643.
[118] E. Banijamali, A. Ghodsi, P. Popuart, Generative mixture of networks, in:
2017 International Joint Conference on Neural Networks, IJCNN, 2017, pp.
3753â€“3760, http://dx.doi.org/10.1109/IJCNN.2017.7966329.
[119] M. Rezagholiradeh, M.A. Haidar, Reg-gan: Semi-supervised learning based on
generative adversarial networks for regression, in: 2018 IEEE International
Conference on Acoustics, Speech and Signal Processing, ICASSP, 2018, pp.
2806â€“2810.
[120] Y. Fang, H. Lin, M. Sui, C. Li, E.J. Fang, Machine-learning-based dynamic
IR drop prediction for ECO, in: 2018 IEEE/ACM International Conference
on Computer-Aided Design, ICCAD, 2018, pp. 1â€“7, http://dx.doi.org/10.1145/
3240765.3240823.
[121] Z. Xie, H. Ren, B. Khailany, Y. Sheng, S. Santosh, J. Hu, Y. Chen, PowerNet:
Transferable dynamic IR drop estimation via maximum convolutional neural
network, in: 2020 25th Asia and South Pacific Design Automation Conference,
ASP-DAC, 2020, pp. 13â€“18, http://dx.doi.org/10.1109/ASP-DAC47756.2020.
9045574.
[122] S. Lin, Y. Fang, Y. Li, Y. Liu, T. Yang, S. Lin, C. Li, E.J. Fang, IR drop prediction
of ECO-revised circuits using machine learning, in: 2018 IEEE 36th VLSI Test
Symposium, VTS, 2018, pp. 1â€“6, http://dx.doi.org/10.1109/VTS.2018.8368657.
[123] Y. Yamato, T. Yoneda, K. Hatayama, M. Inoue, A fast and accurate per-cell
dynamic IR-drop estimation method for at-speed scan test pattern validation,
in: 2012 IEEE International Test Conference, 2012, pp. 1â€“8, http://dx.doi.org/
10.1109/TEST.2012.6401549.
[124] F. Ye, F. Firouzi, Y. Yang, K. Chakrabarty, M.B. Tahoori, On-chip voltagedroop prediction using support-vector machines, in: 2014 IEEE 32nd VLSI Test
Symposium, VTS, 2014, pp. 1â€“6, http://dx.doi.org/10.1109/VTS.2014.6818798.
Integration 93 (2023) 102048
28
D. Amuru et al.
[125] S. Kundu, M. Prasad, S. Nishad, S. Nachireddy, H. K, MLIR: Machine learning
based IR drop prediction on ECO revised design for faster convergence, in:
2022 35th International Conference on VLSI Design and 2022 21st International
Conference on Embedded Systems, VLSID, 2022, pp. 68â€“73, http://dx.doi.org/
10.1109/VLSID2022.2022.00025.
[126] S. Han, A.B. Kahng, S. Nath, A.S. Vydyanathan, A deep learning methodology to
proliferate golden signoff timing, in: 2014 Design, Automation Test in Europe
Conference Exhibition, DATE, 2014, pp. 1â€“6.
[127] C. Zhuo, B. Yu, D. Gao, Accelerating chip design with machine learning: From
pre-silicon to post-silicon, in: 2017 30th IEEE International System-on-Chip
Conference, SOCC, 2017, pp. 227â€“232, http://dx.doi.org/10.1109/SOCC.2017.
8226046.
[128] S. Dey, S. Nandi, G. Trivedi, Machine learning for VLSI CAD: A case study in
on-chip power grid design, in: 2021 IEEE Computer Society Annual Symposium
on VLSI, ISVLSI, 2021, pp. 378â€“383, http://dx.doi.org/10.1109/ISVLSI51109.
2021.00075.
[129] H. Vaghasiya, A. Jain, J.N. Tripathi, A machine learning based metaheuristic
technique for decoupling capacitor optimization, in: 2022 IEEE 26th Workshop
on Signal and Power Integrity, SPI, 2022, pp. 1â€“4, http://dx.doi.org/10.1109/
SPI54345.2022.9874924.
[130] M.-Y. Su, W.-C. Lin, Y.-T. Kuo, C.-M. Li, E.J.-W. Fang, S.S.-Y. Hsueh, Chip performance prediction using machine learning techniques, in: 2021 International
Symposium on VLSI Design, Automation and Test, VLSI-DAT, 2021, pp. 1â€“4,
http://dx.doi.org/10.1109/VLSI-DAT52063.2021.9427338.
[131] S. Sadiqbatcha, J. Zhang, H. Amrouch, S.X.-D. Tan, Real-time full-chip thermal
tracking: A post-silicon, machine learning perspective, IEEE Trans. Comput. 71
(6) (2022) 1411â€“1424, http://dx.doi.org/10.1109/TC.2021.3086112.
[132] J. Zhang, Z. Wang, N. Verma, In-memory computation of a machine-learning
classifier in a standard 6T SRAM array, IEEE J. Solid-State Circuits 52 (4) (2017)
915â€“924, http://dx.doi.org/10.1109/JSSC.2016.2642198.
[133] M. Kang, Y. Kim, A.D. Patil, N.R. Shanbhag, Deep in-memory architectures
for machine learningâ€“accuracy versus efficiency trade-offs, IEEE Trans. Circuits
Syst. I. Regul. Pap. 67 (5) (2020) 1627â€“1639, http://dx.doi.org/10.1109/TCSI.
2019.2960841.
[134] M. Kang, M.-S. Keel, N.R. Shanbhag, S. Eilert, K. Curewitz, An energy-efficient
VLSI architecture for pattern recognition via deep embedding of computation in
SRAM, in: 2014 IEEE International Conference on Acoustics, Speech and Signal
Processing, ICASSP, 2014, pp. 8326â€“8330, http://dx.doi.org/10.1109/ICASSP.
2014.6855225.
[135] S.K. Gonugondla, M. Kang, N. Shanbhag, A 42pJ/decision 3.12TOPS/W robust
in-memory machine learning classifier with on-chip training, in: 2018 IEEE
International Solid - State Circuits Conference, ISSCC, 2018, pp. 490â€“492.
[136] A. Sebastian, M. Le Gallo, R. Khaddam-Aljameh, E. Eleftheriou, Memory devices
and applications for in-memory computing, Nature Nanotechnol. 15 (7) (2020)
529â€“544, http://dx.doi.org/10.1038/s41565-020-0655-z.
[137] Y. Wang, H. Tang, Y. Xie, X. Chen, S. Ma, Z. Sun, Q. Sun, L. Chen, H.
Zhu, J. Wan, Z. Xu, D.W. Zhang, P. Zhou, W. Bao, An in-memory computing
architecture based on two-dimensional semiconductors for multiply-accumulate
operations, Nature Commun. 12 (1) (2021) 3347, http://dx.doi.org/10.1038/
s41467-021-23719-3.
[138] Q. Wang, P. Li, Y. Kim, A parallel digital VLSI architecture for integrated
support vector machine training and classification, IEEE Trans. Very Large Scale
Integr. (VLSI) Syst. 23 (8) (2015) 1471â€“1484.
[139] K. Kang, T. Shibata, An on-chip-trainable Gaussian-kernel analog support vector
machine, IEEE Trans. Circuits Syst. I. Regul. Pap. 57 (7) (2010) 1513â€“1524.
[140] T. Kuan, J. Wang, J. Wang, P. Lin, G. Gu, VLSI design of an SVM learning core
on sequential minimal optimization algorithm, IEEE Trans. Very Large Scale
Integr. (VLSI) Syst. 20 (04) (2012) 673â€“683, http://dx.doi.org/10.1109/TVLSI.
2011.2107533.
[141] M. Papadonikolakis, C. Bouganis, Novel cascade FPGA accelerator for support
vector machines classification, IEEE Trans. Neural Netw. Learn. Syst. 23 (7)
(2012) 1040â€“1052.
[142] S. Gupta, M. Imani, H. Kaur, T.S. Rosing, NNPIM: A processing in-memory
architecture for neural network acceleration, IEEE Trans. Comput. 68 (9) (2019)
1325â€“1337, http://dx.doi.org/10.1109/TC.2019.2903055.
[143] M. He, C. Song, I. Kim, C. Jeong, S. Kim, I. Park, M. Thottethodi, T.N. Vijaykumar, Newton: A DRAM-makerâ€™s accelerator-in-memory (AiM) architecture for
machine learning, in: 2020 53rd Annual IEEE/ACM International Symposium
on Microarchitecture, MICRO, 2020, pp. 372â€“385, http://dx.doi.org/10.1109/
MICRO50266.2020.00040.
[144] D. Chen, H. Jin, L. Zheng, Y. Huang, P. Yao, C. Gui, Q. Wang, H. Liu, H. He,
X. Liao, R. Zheng, A general offloading approach for near-DRAM processingin-memory architectures, in: 2022 IEEE International Parallel and Distributed
Processing Symposium, IPDPS, 2022, pp. 246â€“257, http://dx.doi.org/10.1109/
IPDPS53621.2022.00032.
[145] F. Schuiki, M. Schaffner, F.K. GÃ¼rkaynak, L. Benini, A scalable near-memory
architecture for training deep neural networks on large in-memory datasets,
IEEE Trans. Comput. 68 (4) (2019) 484â€“497.
[146] A.S. Cordeiro, S.R.d. Santos, F.B. Moreira, P.C. Santos, L. Carro, M.A.Z. Alves,
Machine learning migration for efficient near-data processing, in: 2021 29th
Euromicro International Conference on Parallel, Distributed and Network-Based
Processing, PDP, 2021, pp. 212â€“219, http://dx.doi.org/10.1109/PDP52278.
2021.00041.
[147] V. Iskandar, M.A. Abd El Ghany, D. Goehringer, Near-data-processing architectures performance estimation and ranking using machine learning predictors,
in: 2021 24th Euromicro Conference on Digital System Design, DSD, 2021, pp.
158â€“165, http://dx.doi.org/10.1109/DSD53832.2021.00033.
[148] R. Kaplan, L. Yavits, R. Ginosar, PRINS: Processing-in-storage acceleration of
machine learning, IEEE Trans. Nanotechnol. 17 (5) (2018) 889â€“896.
[149] S. Bavikadi, P.R. Sutradhar, K.N. Khasawneh, A. Ganguly, S.M. Pudukotai Dinakarrao, A review of in-memory computing architectures for machine learning
applications, in: Proceedings of the 2020 on Great Lakes Symposium on VLSI,
GLSVLSI â€™20, Association for Computing Machinery, New York, NY, USA, 2020,
pp. 89â€“94, http://dx.doi.org/10.1145/3386263.3407649.
[150] A. Biswas, H. Sanghvi, M. Mehendale, G. Preet, An area-efficient 6T-SRAM
based Compute-In-Memory architecture with reconfigurable SAR ADCs for
energy-efficient deep neural networks in edge ML applications, in: 2022 IEEE
Custom Integrated Circuits Conference, CICC, 2022, pp. 1â€“2, http://dx.doi.org/
10.1109/CICC53496.2022.9772789.
[151] L. Chang, C. Li, Z. Zhang, J. Xiao, Q. Liu, Z. Zhu, W. Li, Z. Zhu, S. Yang,
J. Zhou, Energy-efficient computing-in-memory architecture for AI processor:
device, circuit, architecture perspective, Sci. China Inf. Sci. 64 (6) (2021)
160403, http://dx.doi.org/10.1007/s11432-021-3234-0.
[152] W. Wan, R. Kubendran, S.B. Eryilmaz, W. Zhang, Y. Liao, D. Wu, S. Deiss, B.
Gao, P. Raina, S. Joshi, H. Wu, G. Cauwenberghs, H.-S.P. Wong, 33.1 A 74
TMACS/W CMOS-RRAM neurosynaptic core with dynamically reconfigurable
dataflow and in-situ transposable weights for probabilistic graphical models,
in: 2020 IEEE International Solid- State Circuits Conference, ISSCC, 2020, pp.
498â€“500, http://dx.doi.org/10.1109/ISSCC19947.2020.9062979.
[153] P. Chi, S. Li, C. Xu, T. Zhang, J. Zhao, Y. Liu, Y. Wang, Y. Xie, PRIME: A novel
processing-in-memory architecture for neural network computation in rerambased main memory, in: 2016 ACM/IEEE 43rd Annual International Symposium
on Computer Architecture, ISCA, 2016, pp. 27â€“39, http://dx.doi.org/10.1109/
ISCA.2016.13.
[154] C. Lammie, W. Xiang, M. Rahimi Azghadi, Modeling and simulating in-memory
memristive deep learning systems: An overview of current efforts, Array 13
(2022) 100116, http://dx.doi.org/10.1016/j.array.2021.100116, URL: https://
www.sciencedirect.com/science/article/pii/S2590005621000540.
[155] M. Cheng, L. Xia, Z. Zhu, Y. Cai, Y. Xie, Y. Wang, H. Yang, TIME: A training-inmemory architecture for memristor-based deep neural networks, in: 2017 54th
ACM/EDAC/IEEE Design Automation Conference, DAC, 2017, pp. 1â€“6.
[156] S. Dave, R. Baghdadi, T. Nowatzki, S. Avancha, A. Shrivastava, B. Li, Hardware
acceleration of sparse and irregular tensor computations of ML models: A survey
and insights, Proc. IEEE 109 (10) (2021) 1706â€“1752, http://dx.doi.org/10.
1109/JPROC.2021.3098483.
[157] W. Olin-Ammentorp, Y. Sokolov, M. Bazhenov, A dual-memory architecture
for reinforcement learning on neuromorphic platforms, Neuromorphic Comput.
Eng. 1 (2) (2021) 024003, http://dx.doi.org/10.1088/2634-4386/ac1a64.
[158] S. Hoffmann-Eifert, Nanoscale HfO2-based memristive devices for neuromorphic
computing, in: 2022 Device Research Conference, DRC, 2022, pp. 1â€“2, http:
//dx.doi.org/10.1109/DRC55272.2022.9855810.
[159] T. Tang, S. Li, L. Nai, N. Jouppi, Y. Xie, NeuroMeter: An integrated power, area,
and timing modeling framework for machine learning accelerators industry
track paper, in: 2021 IEEE International Symposium on High-Performance
Computer Architecture, HPCA, 2021, pp. 841â€“853, http://dx.doi.org/10.1109/
HPCA51647.2021.00075.
[160] X. Wei, C.H. Yu, P. Zhang, Y. Chen, Y. Wang, H. Hu, Y. Liang, J. Cong,
Automated systolic array architecture synthesis for high throughput CNN inference on FPGAs, in: 2017 54th ACM/EDAC/IEEE Design Automation Conference,
DAC, 2017, pp. 1â€“6, http://dx.doi.org/10.1145/3061639.3062207.
[161] H. Ahmad, M. Tanvir, M.A. Hanif, M.U. Javed, R. Hafiz, M. Shafique, Systimator: A design space exploration methodology for systolic array based CNNs
acceleration on the FPGA-based edge nodes, 2019, arXiv:1901.04986.
[162] H. Kung, B. McDanel, S.Q. Zhang, Packing sparse convolutional neural networks
for efficient systolic array implementations: Column combining under joint
optimization, in: Proceedings of the Twenty-Fourth International Conference
on Architectural Support for Programming Languages and Operating Systems,
ASPLOS â€™19, Association for Computing Machinery, New York, NY, USA, 2019,
pp. 821â€“834, http://dx.doi.org/10.1145/3297858.3304028.
[163] S. Han, H. Mao, W.J. Dally, Deep compression: Compressing deep neural
networks with pruning, trained quantization and huffman coding, 2016, arXiv:
1510.00149.
[164] P. Molchanov, S. Tyree, T. Karras, T. Aila, J. Kautz, Pruning convolutional
neural networks for resource efficient inference, 2017, arXiv:1611.06440.
[165] B. Asgari, R. Hadidi, H. Kim, S. Yalamanchili, ERIDANUS: Efficiently running
inference of DNNs using systolic arrays, IEEE Micro 39 (05) (2019) 46â€“54,
http://dx.doi.org/10.1109/MM.2019.2930057.
Integration 93 (2023) 102048
29
D. Amuru et al.
[166] C. Jiang, D. Ojika, B. Patel, H. Lam, Optimized FPGA-based deep learning
accelerator for sparse CNN using high bandwidth memory, in: 2021 IEEE 29th
Annual International Symposium on Field-Programmable Custom Computing
Machines, FCCM, 2021, pp. 157â€“164, http://dx.doi.org/10.1109/FCCM51124.
2021.00026.
[167] T. Senoo, A. Jinguji, R. Kuramochi, H. Nakahara, A multilayer perceptron
training accelerator using systolic array, in: 2021 IEEE Asia Pacific Conference
on Circuit and Systems, APCCAS, 2021, pp. 77â€“80, http://dx.doi.org/10.1109/
APCCAS51387.2021.9687773.
[168] N.-C. Huang, W.-K. Tseng, H.-J. Chou, K.-C. Wu, An energy-efficient approximate systolic array based on timing error prediction and prevention, in: 2021
IEEE 39th VLSI Test Symposium, VTS, 2021, pp. 1â€“7, http://dx.doi.org/10.
1109/VTS50974.2021.9441004.
[169] Y. Parmar, K. Sridharan, A resource-efficient multiplierless systolic array architecture for convolutions in deep networks, IEEE Trans. Circuits Syst. II 67 (2)
(2020) 370â€“374, http://dx.doi.org/10.1109/TCSII.2019.2907974.
[170] I. Ullah, K. Inayat, J.-S. Yang, J. Chung, Factored radix-8 systolic array for
tensor processing, in: 2020 57th ACM/IEEE Design Automation Conference,
DAC, 2020, pp. 1â€“6, http://dx.doi.org/10.1109/DAC18072.2020.9218585.
[171] C. Peltekis, D. Filippas, C. Nicopoulos, G. Dimitrakopoulos, FusedGCN: A systolic three-matrix multiplication architecture for graph convolutional networks,
in: 2022 IEEE 33rd International Conference on Application-Specific Systems,
Architectures and Processors, ASAP, 2022, pp. 93â€“97, http://dx.doi.org/10.
1109/ASAP54787.2022.00024.
[172] K. Inayat, J. Chung, Hybrid accumulator factored systolic array for machine
learning acceleration, IEEE Trans. Very Large Scale Integr. (VLSI) Syst. 30 (7)
(2022) 881â€“892, http://dx.doi.org/10.1109/TVLSI.2022.3170233.
[173] S. Kundu, S. Banerjee, A. Raha, S. Natarajan, K. Basu, Toward functional safety
of systolic array-based deep learning hardware accelerators, IEEE Trans. Very
Large Scale Integr. (VLSI) Syst. 29 (3) (2021) 485â€“498, http://dx.doi.org/10.
1109/TVLSI.2020.3048829.
[174] P. Joseph, K. Vaswani, M. Thazhuthaveetil, Construction and use of linear
regression models for processor performance analysis, in: The Twelfth International Symposium on High-Performance Computer Architecture, 2006, 2006,
pp. 99â€“108, http://dx.doi.org/10.1109/HPCA.2006.1598116.
[175] B.C. Lee, D.M. Brooks, Accurate and efficient regression modeling for microarchitectural performance and power prediction, in: Proceedings of the 12th
International Conference on Architectural Support for Programming Languages
and Operating Systems, in: ASPLOS XII, Association for Computing Machinery,
New York, NY, USA, 2006, pp. 185â€“194, http://dx.doi.org/10.1145/1168857.
1168881.
[176] H.-S. Yun, S.-J. Lee, Power prediction of mobile processors based on statistical
analysis of performance monitoring events, J. KIISE: Comput. Pract. Lett. 15
(7) (2009) 469â€“477.
[177] S. Rai, W.L. Neto, Y. Miyasaka, X. Zhang, M. Yu, Q.Y.M. Fujita, G.B. Manske,
M.F. Pontes, L.S. da Rosa Junior, M.S. de Aguiar, P.F. Butzen, P.-C. Chien,
Y.-S. Huang, H.-R. Wang, J.-H.R. Jiang, J. Gu, Z. Zhao, Z. Jiang, D.Z. Pan,
B.A. de Abreu, I. de Souza Campos, A. Berndt, C. Meinhardt, J.T. Carvalho, M.
Grellert, S. Bampi, A. Lohana, A. Kumar, W. Zeng, A. Davoodi, R.O. Topaloglu,
Y. Zhou, J. Dotzel, Y. Zhang, H. Wang, Z. Zhang, V. Tenace, P.-E. Gaillardon,
A. Mishchenko, S. Chatterjee, Logic synthesis meets machine learning: Trading
exactness for generalization, 2020, arXiv:2012.02530.
[178] D.-H. Wang, P.-J. Lin, H.-T. Yang, C.-A. Hsu, S.-H. Huang, M.P.-H. Lin, A novel
machine-learning based SoC performance monitoring methodology under widerange PVT variations with unknown critical paths, in: 2021 58th ACM/IEEE
Design Automation Conference, DAC, 2021, pp. 1370â€“1371, http://dx.doi.org/
10.1109/DAC18074.2021.9586155.
[179] T.-W. Chen, C.-S. Tang, S.-F. Tsai, C.-H. Tsai, S.-Y. Chien, L.-G. Chen, Terascale performance machine learning SoC (MLSoC) with dual stream processor
architecture for multimedia content analysis, IEEE J. Solid-State Circuits 45 (11)
(2010) 2321â€“2329, http://dx.doi.org/10.1109/JSSC.2010.2067910.
[180] P. Jokic, E. Azarkhish, R. Cattenoz, E. TÃ¼retken, L. Benini, S. Emery, A
sub-mW dual-engine ML inference system-on-chip for complete end-to-end faceanalysis at the edge, in: 2021 Symposium on VLSI Circuits, 2021, pp. 1â€“2,
http://dx.doi.org/10.23919/VLSICircuits52068.2021.9492401.
[181] C.-W. Hung, C.-H. Lee, C.-C. Kuo, S.-X. Zeng, Soc-based early failure detection
system using deep learning for tool wear, IEEE Access 10 (2022) 70491â€“70501,
http://dx.doi.org/10.1109/ACCESS.2022.3187043.
[182] A. Safaei, Q.M.J. Wu, Y. Yang, T. AkÄ±lan, System-on-a-chip (SoC)-based
hardware acceleration for extreme learning machine, in: 2017 24th IEEE
International Conference on Electronics, Circuits and Systems, ICECS, 2017, pp.
470â€“473, http://dx.doi.org/10.1109/ICECS.2017.8292050.
[183] Z. He, C. Shi, T. Wang, Y. Wang, M. Tian, X. Zhou, P. Li, L. Liu, N. Wu,
G. Luo, A low-cost FPGA implementation of spiking extreme learning machine
with on-chip reward-modulated STDP learning, IEEE Trans. Circuits Syst. II 69
(3) (2022) 1657â€“1661, http://dx.doi.org/10.1109/TCSII.2021.3117699.
[184] L. Bai, L. Chen, Machine-learning-based early-stage timing prediction in SoC
physical design, in: 2018 14th IEEE International Conference on Solid-State
and Integrated Circuit Technology, ICSICT, 2018, pp. 1â€“3, http://dx.doi.org/
10.1109/ICSICT.2018.8565778.
[185] V. Gotra, S.K.R. Reddy, Simultaneous multi voltage aware timing analysis
methodology for SOC using machine learning, in: 2020 IEEE 33rd International
System-on-Chip Conference, SOCC, 2020, pp. 254â€“257, http://dx.doi.org/10.
1109/SOCC49529.2020.9524780.
[186] M.M. Ziegler, J. Kwon, H.-Y. Liu, L.P. Carloni, Online and offline machine learning for industrial design flow tuning: (invited - ICCAD special session paper), in:
2021 IEEE/ACM International Conference on Computer Aided Design, ICCAD,
2021, pp. 1â€“9, http://dx.doi.org/10.1109/ICCAD51958.2021.9643577.
[187] A.F. Ajirlou, I. Partin-Vaisband, A machine learning pipeline stage for adaptive
frequency adjustment, IEEE Trans. Comput. 71 (3) (2022) 587â€“598, http:
//dx.doi.org/10.1109/TC.2021.3057764.
[188] S. Kapoor, P. Agarwal, L. Kostas, Challenges in building deployable machine
learning solutions for SoC design, in: 2022 IEEE Women in Technology Conference, WINTECHCON, 2022, pp. 1â€“6, http://dx.doi.org/10.1109/
WINTECHCON55229.2022.9832287.
[189] I.M. Elfadel, D.S. Boning, X. Li, Machine Learning in VLSI Computer-Aided
Design, Springer, 2019.
[190] Y. Lin, W. Li, J. Gu, H. Ren, B. Khailany, D.Z. Pan, ABCDPlace: Accelerated batch-based concurrent detailed placement on multithreaded CPUs and
GPUs, IEEE Trans. Comput.-Aided Des. Integr. Circuits Syst. 39 (12) (2020)
5083â€“5096, http://dx.doi.org/10.1109/TCAD.2020.2971531.
[191] A. Mirhoseini, A. Goldie, M. Yazgan, J.W. Jiang, E. Songhori, S. Wang, Y.-J.
Lee, E. Johnson, O. Pathak, A. Nazi, J. Pak, A. Tong, K. Srinivasa, W. Hang, E.
Tuncer, Q.V. Le, J. Laudon, R. Ho, R. Carpenter, J. Dean, A graph placement
methodology for fast chip design, Nature 594 (7862) (2021) 207â€“212, http:
//dx.doi.org/10.1038/s41586-021-03544-w.
[192] W.-T.J. Chan, K.Y. Chung, A.B. Kahng, N.D. MacDonald, S. Nath, Learning-based
prediction of embedded memory timing failures during initial floorplan design,
in: 2016 21st Asia and South Pacific Design Automation Conference, ASP-DAC,
2016, pp. 178â€“185, http://dx.doi.org/10.1109/ASPDAC.2016.7428008.
[193] W.-K. Cheng, Y.-Y. Guo, C.-S. Wu, Evaluation of routability-driven macro placement with machine-learning technique, in: 2018 7th International Symposium
on Next Generation Electronics, ISNE, 2018, pp. 1â€“3, http://dx.doi.org/10.
1109/ISNE.2018.8394712.
[194] A. Arunkumar, E. Bolotin, B. Cho, U. Milic, E. Ebrahimi, O. Villa, A. Jaleel, C.-J.
Wu, D. Nellans, MCM-GPU: Multi-chip-module GPUs for continued performance
scalability, in: 2017 ACM/IEEE 44th Annual International Symposium on
Computer Architecture, ISCA, 2017, pp. 320â€“332, http://dx.doi.org/10.1145/
3079856.3080231.
[195] X. Xie, P. Prabhu, U. Beaugnon, P.M. Phothilimthana, S. Roy, A. Mirhoseini, E.
Brevdo, J. Laudon, Y. Zhou, A transferable approach for partitioning machine
learning models on multi-chip-modules, 2021, arXiv. URL: https://arxiv.org/
abs/2112.04041. http://dx.doi.org/10.48550/ARXIV.2112.04041.
[196] S.I. Ward, D.A. Papa, Z. Li, C.N. Sze, C.J. Alpert, E. Swartzlander, Quantifying
academic placer performance on custom designs, in: Proceedings of the 2011
International Symposium on Physical Design, ISPD â€™11, Association for Computing Machinery, New York, NY, USA, 2011, pp. 91â€“98, http://dx.doi.org/10.
1145/1960397.1960420.
[197] S. Ward, D. Ding, D.Z. Pan, PADE: A high-performance placer with automatic
datapath extraction and evaluation through high-dimensional data learning, in:
DAC Design Automation Conference 2012, 2012, pp. 756â€“761.
[198] Y. Wang, D. Yeo, H. Shin, Effective datapath logic extraction techniques using
connection vectors, IET Circuits Devices Syst. 13 (6) (2019) 741â€“747.
[199] A. Mirhoseini, A. Goldie, M. Yazgan, J. Jiang, E. Songhori, S. Wang, Y.-J. Lee,
E. Johnson, O. Pathak, S. Bae, A. Nazi, J. Pak, A. Tong, K. Srinivasa, W. Hang,
E. Tuncer, A. Babu, Q.V. Le, J. Laudon, R. Ho, R. Carpenter, J. Dean, Chip
placement with deep reinforcement learning, 2020, arXiv:2004.10746.
[200] I. Turtletaub, G. Li, M. Ibrahim, P. Franzon, Application of quantum machine
learning to VLSI placement, in: Proceedings of the 2020 ACM/IEEE Workshop
on Machine Learning for CAD, Association for Computing Machinery, New York,
NY, USA, 2020, pp. 61â€“66.
[201] A. Peruzzo, J. McClean, P. Shadbolt, M.-H. Yung, X.-Q. Zhou, P.J. Love, A.
Aspuru-Guzik, J.L. Oâ€™Brien, A variational eigenvalue solver on a photonic
quantum processor, Nature Commun. 5 (1) (2014) http://dx.doi.org/10.1038/
ncomms5213.
[202] T.-W. Huang, Machine learning system-enabled GPU acceleration for EDA, in:
2021 International Symposium on VLSI Design, Automation and Test, VLSI-DAT,
2021, p. 1, http://dx.doi.org/10.1109/VLSI-DAT52063.2021.9427323.
[203] A.B. Kahng, Advancing placement, in: Proceedings of the 2021 International
Symposium on Physical Design, ISPD â€™21, Association for Computing Machinery,
New York, NY, USA, 2021, pp. 15â€“22, http://dx.doi.org/10.1145/3439706.
3446884.
[204] A. Alhyari, A. Shamli, Z. Abuwaimer, S. Areibi, G. Grewal, A deep learning
framework to predict routability for FPGA circuit placement, in: 2019 29th
International Conference on Field Programmable Logic and Applications, FPL,
2019, pp. 334â€“341, http://dx.doi.org/10.1109/FPL.2019.00060.
[205] S.F. Almeida, J. LuÃ­s GÃ¼ntzel, L. Behjat, C. Meinhardt, Routability-driven
detailed placement using reinforcement learning, in: 2022 IFIP/IEEE 30th
International Conference on Very Large Scale Integration, VLSI-SoC, 2022, pp.
1â€“2, http://dx.doi.org/10.1109/VLSI-SoC54400.2022.9939602.
Integration 93 (2023) 102048
30
D. Amuru et al.
[206] Y.-C. Lu, T. Yang, S.K. Lim, H. Ren, Placement optimization via PPA-directed
graph clustering, in: 2022 ACM/IEEE 4th Workshop on Machine Learning for
CAD, MLCAD, 2022, pp. 1â€“6, http://dx.doi.org/10.1109/MLCAD55463.2022.
9900089.
[207] C.-K. Cheng, C.-T. Ho, C. Holtz, D. Lee, B. Lin, Machine learning prediction
for design and system technology co-optimization sensitivity analysis, IEEE
Trans. Very Large Scale Integr. (VLSI) Syst. 30 (8) (2022) 1059â€“1072, http:
//dx.doi.org/10.1109/TVLSI.2022.3172938.
[208] D.Z. Pan, EDAML 2022 keynote speaker: Machine learning for agile, intelligent
and open-source EDA, in: 2022 IEEE International Parallel and Distributed
Processing Symposium Workshops, IPDPSW, 2022, p. 1181, http://dx.doi.org/
10.1109/IPDPSW55747.2022.00193.
[209] T.-C. Chen, P.-Y. Lee, T.-C. Chen, Automatic floorplanning for AI SoCs, in:
2020 International Symposium on VLSI Design, Automation and Test, VLSI-DAT,
2020, pp. 1â€“2, http://dx.doi.org/10.1109/VLSI-DAT49148.2020.9196464.
[210] Q. Cai, W. Hang, A. Mirhoseini, G. Tucker, J. Wang, W. Wei, Reinforcement learning driven heuristic optimization, 2019, arXiv. http://dx.doi.org/10.
48550/ARXIV.1906.06639. URL: https://arxiv.org/abs/1906.06639.
[211] A. Goldie, A. Mirhoseini, Placement optimization with deep reinforcement
learning, in: Proceedings of the 2020 International Symposium on Physical
Design, ISPD â€™20, Association for Computing Machinery, New York, NY, USA,
2020, pp. 3â€“7, http://dx.doi.org/10.1145/3372780.3378174.
[212] A.B. Kahng, S. Mantik, A system for automatic recording and prediction of
design quality metrics, in: Proceedings of the IEEE 2001. 2nd International
Symposium on Quality Electronic Design, 2001, pp. 81â€“86.
[213] A.B. Kahng, B. Lin, S. Nath, Enhanced metamodeling techniques for highdimensional IC design estimation problems, in: 2013 Design, Automation &
Test in Europe Conference & Exhibition, DATE, 2013, pp. 1861â€“1866, http:
//dx.doi.org/10.7873/DATE.2013.371.
[214] A.B. Kahng, B. Lin, S. Nath, High-dimensional metamodeling for prediction of
clock tree synthesis outcomes, in: 2013 ACM/IEEE International Workshop on
System Level Interconnect Prediction, SLIP, 2013, pp. 1â€“7.
[215] Y. Kwon, J. Jung, I. Han, Y. Shin, Transient clock power estimation of pre-CTS
netlist, in: 2018 IEEE International Symposium on Circuits and Systems, ISCAS,
2018, pp. 1â€“4, http://dx.doi.org/10.1109/ISCAS.2018.8351430.
[216] P. Ray, V.S. Prashant, B.P. Rao, Machine learning based parameter tuning for
performance and power optimization of multisource clock tree synthesis, in:
2022 IEEE 35th International System-on-Chip Conference, SOCC, 2022, pp. 1â€“2,
http://dx.doi.org/10.1109/SOCC56010.2022.9908123.
[217] Y.-C. Lu, J. Lee, A. Agnesina, K. Samadi, S.K. Lim, GAN-CTS: A generative
adversarial framework for clock tree prediction and optimization, in: 2019
IEEE/ACM International Conference on Computer-Aided Design, ICCAD, 2019,
pp. 1â€“8, http://dx.doi.org/10.1109/ICCAD45719.2019.8942063.
[218] Y.-C. Lu, J. Lee, A. Agnesina, K. Samadi, S.K. Lim, A clock tree prediction
and optimization framework using generative adversarial learning, IEEE Trans.
Comput.-Aided Des. Integr. Circuits Syst. 41 (9) (2022) 3104â€“3117, http://dx.
doi.org/10.1109/TCAD.2021.3122109.
[219] S.A. Beheshti-Shirazi, A. Vakil, S. Manoj, I. Savidis, H. Homayoun, A. Sasan, A
reinforced learning solution for clock skew engineering to reduce peak current
and IR drop, in: Proceedings of the 2021 on Great Lakes Symposium on VLSI,
GLSVLSI â€™21, Association for Computing Machinery, New York, NY, USA, 2021,
pp. 181â€“187, http://dx.doi.org/10.1145/3453688.3461754.
[220] L.-T. Wang, Y.-W. Chang, K.-T.T. Cheng, Electronic Design Automation: Synthesis, Verification, and Test, Morgan Kaufmann Publishers Inc, San Francisco,
CA, USA, 2009.
[221] Y. Wei, C. Sze, N. Viswanathan, Z. Li, C.J. Alpert, L. Reddy, A.D. Huber,
G.E. Tellez, D. Keller, S.S. Sapatnekar, Techniques for scalable and effective
routability evaluation, ACM Trans. Des. Autom. Electron. Syst. 19 (2) (2014)
http://dx.doi.org/10.1145/2566663.
[222] G. Udgirkar, G. Indumathi, VLSI global routing algorithms: A survey, in:
2016 3rd International Conference on Computing for Sustainable Global
Development, INDIACom, 2016, pp. 2528â€“2533.
[223] Z. Qi, Y. Cai, Q. Zhou, Z. Li, M. Chen, VFGR: A very fast parallel global router
with accurate congestion modeling, in: 2014 19th Asia and South Pacific Design
Automation Conference, ASP-DAC, 2014, pp. 525â€“530, http://dx.doi.org/10.
1109/ASPDAC.2014.6742945.
[224] J.H. Friedman, Multivariate adaptive regression splines, Ann. Statist. 19 (1)
(1991) 1â€“67, http://dx.doi.org/10.1214/aos/1176347963.
[225] Z. Qi, Y. Cai, Q. Zhou, Accurate prediction of detailed routing congestion
using supervised data learning, in: 2014 IEEE 32nd International Conference
on Computer Design, ICCD, 2014, pp. 97â€“103.
[226] W.J. Chan, Y. Du, A.B. Kahng, S. Nath, K. Samadi, BEOL stack-aware routability
prediction from placement using data mining techniques, in: 2016 IEEE 34th
International Conference on Computer Design, ICCD, 2016, pp. 41â€“48, http:
//dx.doi.org/10.1109/ICCD.2016.7753259.
[227] Z. Xie, Y. Huang, G. Fang, H. Ren, S. Fang, Y. Chen, J. Hu, RouteNet: Routability
prediction for mixed-size designs using convolutional neural network, in: 2018
IEEE/ACM International Conference on Computer-Aided Design, ICCAD, 2018,
pp. 1â€“8, http://dx.doi.org/10.1145/3240765.3240843.
[228] A.F. Tabrizi, N.K. Darav, L. Rakai, A. Kennings, L. Behjat, Detailed routing
violation prediction during placement using machine learning, in: 2017 International Symposium on VLSI Design, Automation and Test, VLSI-DAT, 2017,
pp. 1â€“4, http://dx.doi.org/10.1109/VLSI-DAT.2017.7939657.
[229] L.-C. Chen, C.-C. Huang, Y.-L. Chang, H.-M. Chen, A learning-based methodology for routability prediction in placement, in: 2018 International Symposium
on VLSI Design, Automation and Test, VLSI-DAT, 2018, pp. 1â€“4, http://dx.doi.
org/10.1109/VLSI-DAT.2018.8373272.
[230] Y.-Y. Huang, C.-T. Lin, W.-L. Liang, H.-M. Chen, Learning based placement
refinement to reduce DRC short violations, in: 2021 International Symposium
on VLSI Design, Automation and Test, VLSI-DAT, 2021, pp. 1â€“4, http://dx.doi.
org/10.1109/VLSI-DAT52063.2021.9427321.
[231] J.-R. Gao, P.-C. Wu, T.-C. Wang, A new global router for modern designs, in:
2008 Asia and South Pacific Design Automation Conference, 2008, pp. 232â€“237,
http://dx.doi.org/10.1109/ASPDAC.2008.4483948.
[232] T. Zhang, X. Liu, W. Tang, J. Chen, Z. Xiao, F. Zhang, W. Hu, Z. Zhou, Y. Cheng,
Predicted congestion using a density-based fast neural network algorithm in
global routing, in: 2019 IEEE International Conference on Electron Devices and
Solid-State Circuits, EDSSC, 2019, pp. 1â€“3, http://dx.doi.org/10.1109/EDSSC.
2019.8754196.
[233] Z. Zhou, Z. Zhu, J. Chen, Y. Ma, B. Yu, T.-Y. Ho, G. Lemieux, A. Ivanov,
Congestion-aware global routing using deep convolutional generative adversarial networks, in: 2019 ACM/IEEE 1st Workshop on Machine Learning for
CAD, MLCAD, 2019, pp. 1â€“6, http://dx.doi.org/10.1109/MLCAD48534.2019.
9142082.
[234] A.F. Tabrizi, N.K. Darav, L. Rakai, I. Bustany, A. Kennings, L. Behjat,
Eh?Predictor: A deep learning framework to identify detailed routing short
violations from a placed netlist, IEEE Trans. Comput.-Aided Des. Integr.
Circuits Syst. 39 (6) (2020) 1177â€“1190, http://dx.doi.org/10.1109/TCAD.2019.
2917130.
[235] K.-R. Dai, W.-H. Liu, Y.-L. Li, NCTU-GR: Efficient simulated evolution-based
rerouting and congestion-relaxed layer assignment on 3-D global routing, IEEE
Trans. Very Large Scale Integr. (VLSI) Syst. 20 (3) (2012) 459â€“472, http:
//dx.doi.org/10.1109/TVLSI.2010.2102780.
[236] Y. Pan, Z. Zhou, A. Ivanov, Routability-driven global routing with 3D congestion
estimation using a customized neural network, in: 2022 23rd International
Symposium on Quality Electronic Design, ISQED, 2022, pp. 1â€“6, http://dx.doi.
org/10.1109/ISQED54688.2022.9806228.
[237] J. Liu, C.-W. Pui, F. Wang, E.F.Y. Young, CUGR: Detailed-routability-driven
3D global routing with probabilistic resource model, in: 2020 57th ACM/IEEE
Design Automation Conference, DAC, 2020, pp. 1â€“6, http://dx.doi.org/10.1109/
DAC18072.2020.9218646.
[238] P. Goswami, D. Bhatia, Congestion prediction in FPGA using regression
based learning methods, Electronics 10 (16) (2021) http://dx.doi.org/10.3390/
electronics10161995, URL: https://www.mdpi.com/2079-9292/10/16/1995.
[239] B. Li, P.D. Franzon, Machine learning in physical design, in: 2016 IEEE 25th
Conference on Electrical Performance of Electronic Packaging and Systems,
EPEPS, 2016, pp. 147â€“150.
[240] E.C. Barboza, N. Shukla, Y. Chen, J. Hu, Machine learning-based pre-routing
timing prediction with reduced pessimism, in: 2019 56th ACM/IEEE Design
Automation Conference, DAC, 2019, pp. 1â€“6.
[241] Y.-H. Yeh, S.Y.-H. Chen, H.-M. Chen, D.-Y. Tu, G.-Q. Fang, Y.-C. Kuo, P.-Y.
Chen, Substrate signal routing solution exploration for high-density packages
with machine learning, in: 2022 International Symposium on VLSI Design,
Automation and Test, VLSI-DAT, 2022, pp. 1â€“4, http://dx.doi.org/10.1109/
VLSI-DAT54769.2022.9768081.
[242] R. Kirby, S. Godil, R. Roy, B. Catanzaro, CongestionNet: Routing congestion
prediction using deep graph neural networks, in: 2019 IFIP/IEEE 27th International Conference on Very Large Scale Integration, VLSI-SoC, 2019, pp.
217â€“222, http://dx.doi.org/10.1109/VLSI-SoC.2019.8920342.
[243] X. Chen, Z. Di, W. Wu, Q. Wu, J. Shi, Q. Feng, Detailed routing short violation
prediction using graph-based deep learning model, IEEE Trans. Circuits Syst. II
69 (2) (2022) 564â€“568, http://dx.doi.org/10.1109/TCSII.2021.3093420.
[244] L. Li, Y. Cai, Q. Zhou, A survey on machine learning-based routing for VLSI
physical design, Integr. VLSI J. 86 (C) (2022) 51â€“56, http://dx.doi.org/10.
1016/j.vlsi.2022.05.003.
[245] V.A. Chhabria, W. Jiang, A.B. Kahng, S.S. Sapatnekar, From global route to
detailed route: ML for fast and accurate wire parasitics and timing prediction,
in: 2022 ACM/IEEE 4th Workshop on Machine Learning for CAD, MLCAD, 2022,
pp. 7â€“14, http://dx.doi.org/10.1109/MLCAD55463.2022.9900099.
[246] S.M. Sze, et al., VLSI Technology, McGraw-hill, 1988.
[247] J.N. Helbert, Handbook of VLSI Microlithography, Cambridge University Press,
2001.
[248] M. Phute, A. Sahastrabudhe, S. Pimparkhede, S. Potphode, K. Rengade, S.
Shilaskar, A survey on machine learning in lithography, in: 2021 International
Conference on Artificial Intelligence and Machine Vision, AIMV, 2021, pp. 1â€“6,
http://dx.doi.org/10.1109/AIMV53313.2021.9670977.
[249] A. Gu, A. Zakhor, Optical proximity correction with linear regression, IEEE
Trans. Semicond. Manuf. 21 (2) (2008) 263â€“271.
Integration 93 (2023) 102048
31
D. Amuru et al.
[250] R. Luo, Optical proximity correction using a multilayer perceptron neural
network, J. Opt. 15 (7) (2013) 075708.
[251] T. Matsunawa, B. Yu, D.Z. Pan, Optical proximity correction with hierarchical
bayes model, in: Optical Microlithography XXVIII, Vol. 9426, International
Society for Optics and Photonics, 2015, p. 94260X.
[252] W. Gilks, Markov Chain Monte Carlo. Hoboken, Wiley Online Library, NJ, 2005.
[253] S. Choi, S. Shim, Y. Shin, Machine learning (ML)-guided OPC using basis
functions of polar Fourier transform, in: Optical Microlithography XXIX, Vol.
9780, International Society for Optics and Photonics, 2016, p. 97800H.
[254] L. Pang, Y. Liu, D. Abrams, Inverse lithography technology (ILT): What is
the impact to the photomask industry? in: Photomask and Next-Generation
Lithography Mask Technology XIII, Vol. 6283, International Society for Optics
and Photonics, 2006, p. 62830X.
[255] N. Jia, E.Y. Lam, Machine learning for inverse lithography: using stochastic
gradient descent for robust photomask synthesis, J. Opt. 12 (4) (2010) 045601.
[256] K.-s. Luo, Z. Shi, X.-l. Yan, Z. Geng, SVM based layout retargeting for fast and
regularized inverse lithography, J. Zhejiang Univ. Sci. C 15 (5) (2014) 390â€“400.
[257] X. Shi, Y. Zhao, S. Chen, C. Li, AI computational lithography, in: 2020 China
Semiconductor Technology International Conference, CSTIC, 2020, pp. 1â€“4,
http://dx.doi.org/10.1109/CSTIC49141.2020.9282529.
[258] X. Shi, Y. Yan, T. Zhou, X. Yu, C. Li, S. Chen, Y. Zhao, Fast and accurate
machine learning inverse lithography using physics-based feature maps and specially designed DCNN, in: 2020 International Workshop on Advanced Patterning
Solutions, IWAPS, 2020, pp. 1â€“3, http://dx.doi.org/10.1109/IWAPS51164.
2020.9286814.
[259] X. Xu, Y. Lin, M. Li, T. Matsunawa, S. Nojima, C. Kodama, T. Kotani, D.Z.
Pan, Subresolution assist feature generation with supervised data learning, IEEE
Trans. Comput.-Aided Des. Integr. Circuits Syst. 37 (6) (2017) 1225â€“1236.
[260] S. Shim, S. Choi, Y. Shin, Machine learning (ML)-based lithography optimizations, in: 2016 IEEE Asia Pacific Conference on Circuits and Systems, APCCAS,
2016, pp. 530â€“533.
[261] S. Shim, Y. Shin, Machine learning-guided etch proximity correction, IEEE
Trans. Semicond. Manuf. 30 (1) (2017) 1â€“7, http://dx.doi.org/10.1109/TSM.
2016.2626304.
[262] R. Chen, H. Hu, X. Li, Y. Chen, X. SU, L. Dong, L. Qu, C. Li, J. Yan, Y.
Wei, ETCH model based on machine learning, in: 2020 China Semiconductor
Technology International Conference, CSTIC, 2020, pp. 1â€“4, http://dx.doi.org/
10.1109/CSTIC49141.2020.9282462.
[263] Y. Meng, Y.-C. Kim, S. Guo, Z. Shu, Y. Zhang, Q. Liu, Machine learning models
for edge placement error based etch bias, IEEE Trans. Semicond. Manuf. 34 (1)
(2021) 42â€“48, http://dx.doi.org/10.1109/TSM.2020.3042803.
[264] Y. Lin, M. Li, Y. Watanabe, T. Kimura, T. Matsunawa, S. Nojima, D.Z. Pan,
Data efficient lithography modeling with transfer learning and active data
selection, IEEE Trans. Comput.-Aided Des. Integr. Circuits Syst. 38 (10) (2019)
1900â€“1913, http://dx.doi.org/10.1109/TCAD.2018.2864251.
[265] H. Yang, S. Li, Y. Ma, B. Yu, E.F. Young, GAN-OPC: Mask optimization with
lithography-guided generative adversarial nets, in: Proceedings of the 55th
Annual Design Automation Conference, 2018, pp. 1â€“6.
[266] M.B. Alawieh, Y. Lin, Z. Zhang, M. Li, Q. Huang, D.Z. Pan, GAN-SRAF: Subresolution assist feature generation using conditional generative adversarial
networks, in: Proceedings of the 56th Annual Design Automation Conference
2019, 2019, pp. 1â€“6.
[267] W. Ye, M.B. Alawieh, Y. Lin, D.Z. Pan, Lithogan: End-to-end lithography
modeling with generative adversarial networks, in: 2019 56th ACM/IEEE Design
Automation Conference, DAC, IEEE, 2019, pp. 1â€“6.
[268] H. Yang, W. Zhong, Y. Ma, H. Geng, R. Chen, W. Chen, B. Yu, VLSI mask
optimization: From shallow to deep learning, in: 2020 25th Asia and South
Pacific Design Automation Conference, ASP-DAC, IEEE, 2020, pp. 434â€“439.
[269] H. Yang, S. Li, C. Tabery, B. Lin, B. Yu, Bridging the gap between layout
pattern sampling and hotspot detection via batch active learning, IEEE Trans.
Comput.-Aided Des. Integr. Circuits Syst. 40 (7) (2021) 1464â€“1475, http://dx.
doi.org/10.1109/TCAD.2020.3015903.
[270] N. Figueiro, F. Sanchez, R. Koret, M. Shifrin, Y. Etzioni, S. Wolfling, M.
Sendelbach, Y. Blancquaert, T. Labbaye, G. Rademaker, J. Pradelles, L. Mourier,
S. Rey, L. Pain, Application of scatterometry-based machine learning to control
multiple electron beam lithography: AM: Advanced metrology, in: 2018 29th
Annual SEMI Advanced Semiconductor Manufacturing Conference, ASMC, 2018,
pp. 328â€“333, http://dx.doi.org/10.1109/ASMC.2018.8373222.
[271] M.P. McLaughlin, A. Stamper, G. Barber, J. Paduano, P. Mennell, E. Benn,
M. Linnane, J. Zwick, C. Khatumria, R.L. Isaacson, N. Hoffman, C. Menser,
Enhanced defect detection in after develop inspection with machine learning
disposition, in: 2021 32nd Annual SEMI Advanced Semiconductor Manufacturing Conference, ASMC, 2021, pp. 1â€“5, http://dx.doi.org/10.1109/ASMC51741.
2021.9435721.
[272] N. Nagase, K. Suzuki, K. Takahashi, M. Minemura, S. Yamauchi, T. Okada, Study
of hot spot detection using neural networks judgment, in: Photomask and NextGeneration Lithography Mask Technology XIV, Vol. 6607, International Society
for Optics and Photonics, 2007, p. 66071B.
[273] D. Ding, X. Wu, J. Ghosh, D.Z. Pan, Machine learning based lithographic
hotspot detection with critical-feature extraction and classification, in: 2009
IEEE International Conference on IC Design and Technology, IEEE, 2009, pp.
219â€“222.
[274] N. Ma, J. Ghan, S. Mishra, C. Spanos, K. Poolla, N. Rodriguez, L. Capodieci,
Automatic hotspot classification using pattern-based clustering, in: Design
for Manufacturability Through Design-Process Integration II, Vol. 6925,
International Society for Optics and Photonics, 2008, 692505.
[275] J. Ghan, N. Ma, S. Mishra, C. Spanos, K. Poolla, N. Rodriguez, L. Capodieci,
Clustering and pattern matching for an automatic hotspot classification and
detection system, in: Design for Manufacturability Through Design-Process
Integration III, Vol. 7275, International Society for Optics and Photonics, 2009,
727516.
[276] D. Ding, J.A. Torres, D.Z. Pan, High performance lithography hotspot detection
with successively refined pattern identifications and machine learning, IEEE
Trans. Comput.-Aided Des. Integr. Circuits Syst. 30 (11) (2011) 1621â€“1634,
http://dx.doi.org/10.1109/TCAD.2011.2164537.
[277] Y.-T. Yu, G.-H. Lin, I.H.-R. Jiang, C. Chiang, Machine-learning-based hotspot
detection using topological classification and critical feature extraction, IEEE
Trans. Comput.-Aided Des. Integr. Circuits Syst. 34 (3) (2015) 460â€“470, http:
//dx.doi.org/10.1109/TCAD.2014.2387858.
[278] D. Ding, B. Yu, J. Ghosh, D.Z. Pan, EPIC: Efficient prediction of IC manufacturing hotspots with a unified meta-classification formulation, in: 17th Asia and
South Pacific Design Automation Conference, IEEE, 2012, pp. 263â€“270.
[279] T. Matsunawa, J.-R. Gao, B. Yu, D.Z. Pan, A new lithography hotspot detection
framework based on AdaBoost classifier and simplified feature extraction,
in: Design-Process-Technology Co-Optimization for Manufacturability IX, Vol.
9427, International Society for Optics and Photonics, 2015, p. 94270S.
[280] Y. Chen, Y. Lin, T. Gai, Y. Su, Y. Wei, D.Z. Pan, Semi-supervised hotspot
detection with self-paced multi-task learning, IEEE Trans. Comput.-Aided Des.
Integr. Circuits Syst. (2019).
[281] M. Shin, J.-H. Lee, Accurate lithography hotspot detection using deep convolutional neural networks, J. Micro/Nanolithogr., MEMS, MOEMS 15 (4) (2016)
043507.
[282] V. Borisov, J. Scheible, Lithography hotspots detection using deep learning,
in: 2018 15th International Conference on Synthesis, Modeling, Analysis and
Simulation Methods and Applications to Circuit Design, SMACD, IEEE, 2018,
pp. 145â€“148.
[283] H. Yang, Y. Lin, B. Yu, E.F. Young, Lithography hotspot detection: From shallow
to deep learning, in: 2017 30th IEEE International System-on-Chip Conference,
SOCC, IEEE, 2017, pp. 233â€“238.
[284] H. Yang, L. Luo, J. Su, C. Lin, B. Yu, Imbalance aware lithography hotspot
detection: a deep learning approach, J. Micro/Nanolithogr. MEMS, MOEMS 16
(3) (2017) 033504.
[285] H. Zhang, B. Yu, E.F. Young, Enabling online learning in lithography hotspot
detection with information-theoretic feature optimization, in: Proceedings of the
35th International Conference on Computer-Aided Design, 2016, pp. 1â€“8.
[286] W. Ye, M.B. Alawieh, M. Li, Y. Lin, D.Z. Pan, Litho-GPA: Gaussian process
assurance for lithography hotspot detection, in: 2019 Design, Automation &
Test in Europe Conference & Exhibition, DATE, IEEE, 2019, pp. 54â€“59.
[287] J.W. Park, A. Torres, X. Song, Litho-aware machine learning for hotspot
detection, IEEE Trans. Comput.-Aided Des. Integr. Circuits Syst. 37 (7) (2018)
1510â€“1514, http://dx.doi.org/10.1109/TCAD.2017.2750068.
[288] K. Madkour, S. Mohamed, D. Tantawy, M. Anis, Hotspot detection using
machine learning, in: 2016 17th International Symposium on Quality Electronic
Design, ISQED, 2016, pp. 405â€“409, http://dx.doi.org/10.1109/ISQED.2016.
7479235.
[289] H. Yang, J. Su, Y. Zou, Y. Ma, B. Yu, E.F.Y. Young, Layout hotspot detection
with feature tensor generation and deep biased learning, IEEE Trans. Comput.-
Aided Des. Integr. Circuits Syst. 38 (6) (2019) 1175â€“1187, http://dx.doi.org/
10.1109/TCAD.2018.2837078.
[290] T. Gai, T. Qu, S. Wang, X. Su, R. Xu, Y. Wang, J. Xue, Y. Su, Y. Wei, T. Ye,
Flexible hotspot detection based on fully convolutional network with transfer
learning, IEEE Trans. Comput.-Aided Des. Integr. Circuits Syst. 41 (11) (2022)
4626â€“4638, http://dx.doi.org/10.1109/TCAD.2021.3135786.
[291] Y. Zhang, C. Zhang, M. Li, L. Zhao, C. Yang, Z. Wang, Modified deep learning
approach for layout hotspot detection, in: 2018 IEEE International Conference
on Electron Devices and Solid-State Circuits, EDSSC, 2018, pp. 1â€“2, http:
//dx.doi.org/10.1109/EDSSC.2018.8487177.
[292] M.T. Ismail, H. Sharara, K. Madkour, K. Seddik, Autoencoder-based data
sampling for machine learning-based lithography hotspot detection, in: 2022
ACM/IEEE 4th Workshop on Machine Learning for CAD, MLCAD, 2022, pp.
91â€“96, http://dx.doi.org/10.1109/MLCAD55463.2022.9900096.
[293] H. Yang, W. Chen, P. Pathak, F. Gennari, Y.-C. Lai, B. Yu, Automatic layout
generation with applications in machine learning engine evaluation, in: 2019
ACM/IEEE 1st Workshop on Machine Learning for CAD, MLCAD, 2019, pp. 1â€“6,
http://dx.doi.org/10.1109/MLCAD48534.2019.9142121.
[294] W. Zhang, K. Chen, X. Li, Y. Ma, C. Zhu, B. Chen, X. Gao, K. Kim, A workflow of
hotspot prediction based on semi-supervised machine learning methodology, in:
2021 International Workshop on Advanced Patterning Solutions, IWAPS, 2021,
pp. 1â€“3, http://dx.doi.org/10.1109/IWAPS54037.2021.9671068.
Integration 93 (2023) 102048
32
D. Amuru et al.
[295] M.B. Alawieh, D.Z. Pan, ADAPT: An adaptive machine learning framework with
application to lithography hotspot detection, in: 2021 ACM/IEEE 3rd Workshop
on Machine Learning for CAD, MLCAD, 2021, pp. 1â€“6, http://dx.doi.org/10.
1109/MLCAD52597.2021.9531210.
[296] D. Schmidt, K. Petrillo, M. Breton, J. Fullam, R. Koret, I. Turovets, A.
Cepler, Advanced EUV resist characterization using scatterometry and machine
learning, in: 2021 32nd Annual SEMI Advanced Semiconductor Manufacturing Conference, ASMC, 2021, pp. 1â€“4, http://dx.doi.org/10.1109/ASMC51741.
2021.9435698.
[297] M.P. McLaughlin, P. Mennell, A. Stamper, G. Barber, J. Paduano, E. Benn,
M. Linnane, J. Zwick, C. Khatumria, R.L. Isaacson, N. Hoffman, C. Menser,
Improved color defect detection with machine learning for after develop inspections in lithography, IEEE Trans. Semicond. Manuf. 35 (3) (2022) 418â€“424,
http://dx.doi.org/10.1109/TSM.2022.3186607.
[298] P. Parashar, C. Akbar, T.S. Rawat, S. Pratik, R. Butola, S.H. Chen, Y.-S.
Chang, S. Nuannimnoi, A.S. Lin, Intelligent photolithography corrections using
dimensionality reductions, IEEE Photonics J. 11 (5) (2019) 1â€“15, http://dx.doi.
org/10.1109/JPHOT.2019.2938536.
[299] T. Zhou, B. Xu, C. Li, X. Diao, Y. Yan, S. Chen, Y. Zhao, K. Zhou, W.
Zhou, X. Zeng, X. Shi, Mining lithography hotspots from massive SEM images
using machine learning model, in: 2021 China Semiconductor Technology
International Conference, CSTIC, 2021, pp. 1â€“3, http://dx.doi.org/10.1109/
CSTIC52283.2021.9461533.
[300] Y.-F. Yang, M. Sun, Hybrid quantum-classical machine learning for lithography hotspot detection, in: 2022 33rd Annual SEMI Advanced Semiconductor
Manufacturing Conference, ASMC, 2022, pp. 1â€“6, http://dx.doi.org/10.1109/
ASMC54647.2022.9792509.
[301] T.C. Tin, S.C. Tan, C.K. Lee, Virtual metrology in semiconductor fabrication foundry using deep learning neural networks, IEEE Access 10 (2022)
81960â€“81973, http://dx.doi.org/10.1109/ACCESS.2022.3193783.
[302] X. Zhang, J. Shiely, E.F. Young, Layout pattern generation and legalization
with generative learning models, in: 2020 IEEE/ACM International Conference
on Computer Aided Design, ICCAD, 2020, pp. 1â€“9.
[303] D.P. Kingma, M. Welling, An introduction to variational autoencoders, Found.
Trends Mach. Learn. 12 (4) (2019) 307â€“392, http://dx.doi.org/10.1561/
2200000056.
[304] M. Mirza, S. Osindero, Conditional generative adversarial nets, 2014, CoRR,
arXiv:1411.1784.
[305] M.B. Alawieh, W. Ye, D.Z. Pan, Re-examining VLSI manufacturing and yield
through the lens of deep learning : (invited talk), in: 2020 IEEE/ACM
International Conference on Computer Aided Design, ICCAD, 2020, pp. 1â€“8.
[306] K.N. Patel, I. Markov, J. Hayes, Evaluating circuit reliability under probabilistic
gate-level fault models, 2003.
[307] S. Krishnaswamy, G.F. Viamontes, I.L. Markov, J.P. Hayes, Accurate reliability
evaluation and enhancement via probabilistic transfer matrices, in: Design,
Automation and Test in Europe, Vol. 1, 2005, pp. 282â€“287, http://dx.doi.org/
10.1109/DATE.2005.47.
[308] M.R. Choudhury, K. Mohanram, Accurate and scalable reliability analysis
of logic circuits, in: 2007 Design, Automation Test in Europe Conference
Exhibition, 2007, pp. 1â€“6, http://dx.doi.org/10.1109/DATE.2007.364503.
[309] A. Beg, F. Awwad, W. Ibrahim, F. Ahmed, On the reliability estimation of
nano-circuits using neural networks, Microprocess. Microsyst. 39 (8) (2015)
674â€“685, http://dx.doi.org/10.1016/j.micpro.2015.09.008, URL: http://www.
sciencedirect.com/science/article/pii/S0141933115001507.
[310] N. Karimi, K. Huang, Prognosis of NBTI aging using a machine learning scheme,
in: 2016 IEEE International Symposium on Defect and Fault Tolerance in VLSI
and Nanotechnology Systems, DFT, 2016, pp. 7â€“10, http://dx.doi.org/10.1109/
DFT.2016.7684060.
[311] S. Bian, M. Hiromoto, M. Shintani, T. Sato, LSTA: Learning-based static timing
analysis for high-dimensional correlated on-chip variations, in: 2017 54th
ACM/EDAC/IEEE Design Automation Conference, DAC, 2017, pp. 1â€“6.
[312] T. Cho, R. Liang, G. Yu, J. Xu, Reliability analysis of P-type SOI FinFETs
with multiple SiGe channels on the degradation of NBTI, in: 2020 IEEE Silicon
Nanoelectronics Workshop, SNW, 2020, pp. 101â€“102.
[313] L. Alrahis, J. Knechtel, F. Klemme, H. Amrouch, O. Sinanoglu, GNN4REL:
Graph neural networks for predicting circuit reliability degradation, IEEE Trans.
Comput.-Aided Des. Integr. Circuits Syst. 41 (11) (2022) 3826â€“3837, http:
//dx.doi.org/10.1109/TCAD.2022.3197521.
[314] S. Peng, W. Jin, L. Chen, S.X.-D. Tan, Data-driven fast electrostatics and TDDB
aging analysis, in: Proceedings of the 2020 ACM/IEEE Workshop on Machine
Learning for CAD, MLCAD â€™20, Association for Computing Machinery, New
York, NY, USA, 2020, pp. 71â€“76, http://dx.doi.org/10.1145/3380446.3430620.
[315] S. Lamichhane, S. Peng, W. Jin, S.X.-D. Tan, Fast electrostatic analysis for
VLSI aging based on generative learning, in: 2021 ACM/IEEE 3rd Workshop
on Machine Learning for CAD, MLCAD, 2021, pp. 1â€“6, http://dx.doi.org/10.
1109/MLCAD52597.2021.9531320.
[316] P.-N. Hsu, K.-C. Shie, K.-P. Chen, J.-C. Tu, C.-C. Wu, N.-T. Tsou, Y.-C. Lo,
N.-Y. Chen, Y.-F. Hsieh, M. Wu, C. Chen, K.-N. Tu, Artificial intelligence
deep learning for 3D IC reliability prediction, Sci. Rep. 12 (1) (2022) 6711,
http://dx.doi.org/10.1038/s41598-022-08179-z.
[317] Y. Pan, Z. Lu, H. Zhang, H. Zhang, M.T. Arafin, Z. Liu, G. Qu, ADLPT:
Improving 3D NAND flash memory reliability by adaptive lifetime prediction
techniques, IEEE Trans. Comput. (2022) 1â€“14, http://dx.doi.org/10.1109/TC.
2022.3214115.
[318] S. Kundu, K. Basu, M. Sadi, T. Titirsha, S. Song, A. Das, U. Guin, Special
session: Reliability analysis for AI/ML hardware, in: 2021 IEEE 39th VLSI Test
Symposium, VTS, 2021, pp. 1â€“10, http://dx.doi.org/10.1109/VTS50974.2021.
9441050.
[319] F. Regazzoni, S. Bhasin, A.A. Pour, I. Alshaer, F. Aydin, A. Aysu, V. Beroulle,
G. Di Natale, P. Franzon, D. Hely, N. Homma, A. Ito, D. Jap, P. Kashyap,
I. Polian, S. Potluri, R. Ueno, E.-I. Vatajelu, V. Yli-MÃ¤yry, Machine learning
and hardware security: Challenges and opportunities -invited talk-, in: 2020
IEEE/ACM International Conference on Computer Aided Design, ICCAD, 2020,
pp. 1â€“6.
[320] L.M. Silva, F.V. Andrade, A.O. Fernandes, L.F.M. Vieira, Arithmetic circuit
classification using convolutional neural networks, in: 2018 International Joint
Conference on Neural Networks, IJCNN, 2018, pp. 1â€“7, http://dx.doi.org/10.
1109/IJCNN.2018.8489382.
[321] X. Hong, T. Lin, Y. Shi, B.H. Gwee, ASIC circuit netlist recognition using graph
neural network, in: 2021 IEEE International Symposium on the Physical and
Failure Analysis of Integrated Circuits, IPFA, 2021, pp. 1â€“5, http://dx.doi.org/
10.1109/IPFA53173.2021.9617311.
[322] G. Ali, L. Bagheriye, H.G. Kerkhoff, On-chip embedded instruments data fusion
and life-time prognostics of dependable VLSI-SoCs using machine-learning, in:
2020 IEEE International Symposium on Circuits and Systems, ISCAS, 2020, pp.
1â€“5, http://dx.doi.org/10.1109/ISCAS45731.2020.9180773.
[323] K.G. Liakos, G.K. Georgakilas, F.C. Plessas, Hardware trojan classification at
gate-level netlists based on area and power machine learning analysis, in: 2021
IEEE Computer Society Annual Symposium on VLSI, ISVLSI, 2021, pp. 412â€“417,
http://dx.doi.org/10.1109/ISVLSI51109.2021.00081.
[324] L. Alrahis, A. Sengupta, J. Knechtel, S. Patnaik, H. Saleh, B. Mohammad, M. AlQutayri, O. Sinanoglu, GNN-RE: Graph neural networks for reverse engineering
of gate-level netlists, IEEE Trans. Comput.-Aided Des. Integr. Circuits Syst. 41
(8) (2022) 2435â€“2448, http://dx.doi.org/10.1109/TCAD.2021.3110807.
[325] L. AmarÃº, P.-E. Gaillardon, G. De Micheli, The EPFL combinational benchmark
suite, in: Proceedings of the 24th International Workshop on Logic & Synthesis,
IWLS, (CONF) 2015.
[326] P.R. Genssler, H. Amrouch, Brain-inspired computing for circuit reliability
characterization, IEEE Trans. Comput. 71 (12) (2022) 3336â€“3348, http://dx.
doi.org/10.1109/TC.2022.3151857.
[327] S.-J. Jang, J.-S. Kim, T.-W. Kim, H.-J. Lee, S. Ko, A wafer map yield prediction
based on machine learning for productivity enhancement, IEEE Trans. Semicond. Manuf. 32 (4) (2019) 400â€“407, http://dx.doi.org/10.1109/TSM.2019.
2945482.
[328] W. Maly, A.J. Strojwas, S.W. Director, VLSI yield prediction and estimation: A
unified framework, IEEE Trans. Comput.-Aided Des. Integr. Circuits Syst. 5 (1)
(1986) 114â€“130.
[329] I. Koren, Z. Koren, Defect tolerance in VLSI circuits: techniques and yield
analysis, Proc. IEEE 86 (9) (1998) 1819â€“1838.
[330] P. Backus, M. Janakiram, S. Mowzoon, C. Runger, A. Bhargava, Factory cycletime prediction with a data-mining approach, IEEE Trans. Semicond. Manuf.
19 (2) (2006) 252â€“258, http://dx.doi.org/10.1109/TSM.2006.873400.
[331] Y. Meidan, B. Lerner, G. Rabinowitz, M. Hassoun, Cycle-time key factor
identification and prediction in semiconductor manufacturing using machine
learning and data mining, IEEE Trans. Semicond. Manuf. 24 (2) (2011)
237â€“248, http://dx.doi.org/10.1109/TSM.2011.2118775.
[332] C.-F. Chien, W.-C. Wang, J.-C. Cheng, Data mining for yield enhancement
in semiconductor manufacturing and an empirical study, Expert Syst. Appl.
33 (1) (2007) 192â€“198, http://dx.doi.org/10.1016/j.eswa.2006.04.014, URL:
https://www.sciencedirect.com/science/article/pii/S095741740600131X.
[333] D. Jiang, W. Lin, N. Raghavan, A Gaussian mixture model clustering ensemble regressor for semiconductor manufacturing final test yield prediction,
IEEE Access 9 (2021) 22253â€“22263, http://dx.doi.org/10.1109/ACCESS.2021.
3055433.
[334] D. Jiang, W. Lin, N. Raghavan, Semiconductor manufacturing final test yield
optimization and wafer acceptance test parameter inverse design using multiobjective optimization algorithms, IEEE Access 9 (2021) 137655â€“137666, http:
//dx.doi.org/10.1109/ACCESS.2021.3117576.
[335] H. Gun Kim, Y.S. Han, J.-H. Lee, Package yield enhancement using machine
learning in semiconductor manufacturing, in: 2015 IEEE Advanced Information
Technology, Electronic and Automation Control Conference, IAEAC, 2015, pp.
316â€“320, http://dx.doi.org/10.1109/IAEAC.2015.7428567.
[336] J.-S. Kim, S.-J. Jang, T.-W. Kim, H.-J. Lee, J.-B. Lee, A productivity-oriented
wafer map optimization using yield model based on machine learning, IEEE
Trans. Semicond. Manuf. 32 (1) (2019) 39â€“47, http://dx.doi.org/10.1109/TSM.
2018.2870253.
[337] C. Mead, L. Conway, Introduction to VLSI Systems, Vol. 1080, Addison-Wesley
Reading, MA, 1980.
[338] D. Price, Pentium FDIV flaw-lessons learned, IEEE Micro 15 (2) (1995) 86â€“88.
Integration 93 (2023) 102048
33
D. Amuru et al.
[339] L.-T. Wang, C.-W. Wu, X. Wen, VLSI Test Principles and Architectures: Design
for Testability (Systems on Silicon), Morgan Kaufmann Publishers Inc, San
Francisco, CA, USA, 2006.
[340] B. Wile, J. Goss, W. Roesner, Comprehensive Functional Verification: The
Complete Industry Cycle, Morgan Kaufmann Publishers Inc, San Francisco, CA,
USA, 2005.
[341] R. Lisanke, F. Brglez, A. de Geus, D. Gregory, Testability-driven random testpattern generation, IEEE Trans. Comput.-Aided Des. Integr. Circuits Syst. 6 (6)
(1987) 1082â€“1087, http://dx.doi.org/10.1109/TCAD.1987.1270348.
[342] C. Fagot, P. Girard, C. Landrault, On using machine learning for logic BIST,
in: Proceedings International Test Conference 1997, 1997, pp. 338â€“346, http:
//dx.doi.org/10.1109/TEST.1997.639635.
[343] S. Fine, A. Ziv, Coverage directed test generation for functional verification
using Bayesian networks, in: Proceedings 2003. Design Automation Conference, IEEE Cat. No.03CH37451, 2003, pp. 286â€“291, http://dx.doi.org/10.1145/
775832.775907.
[344] M. Braun, S. Fine, A. Ziv, Enhancing the efficiency of Bayesian network based
coverage directed test generation, in: Proceedings. Ninth IEEE International
High-Level Design Validation and Test Workshop, IEEE Cat. No.04EX940, 2004,
pp. 75â€“80, http://dx.doi.org/10.1109/HLDVT.2004.1431241.
[345] W. Hughes, S. Srinivasan, R. Suvarna, M. Kulkarni, Optimizing design verification using machine learning: Doing better than random, 2019, CoRR arXiv:
1909.13168.
[346] S. Fine, A. Freund, I. Jaeger, Y. Mansour, Y. Naveh, A. Ziv, Harnessing machine
learning to improve the success rate of stimuli generation, IEEE Trans. Comput.
55 (11) (2006) 1344â€“1355, http://dx.doi.org/10.1109/TC.2006.183.
[347] H. Dhotre, S. EggersglÃ¼ÃŸ, M. Dehbashi, U. Pfannkuchen, R. Drechsler, Machine
learning based test pattern analysis for localizing critical power activity areas,
in: 2017 IEEE International Symposium on Defect and Fault Tolerance in VLSI
and Nanotechnology Systems, DFT, 2017, pp. 1â€“6, http://dx.doi.org/10.1109/
DFT.2017.8244464.
[348] J. Chen, L.-C. Wang, P.-H. Chang, J. Zeng, S. Yu, M. Mateja, Data learning
techniques and methodology for fmax prediction, in: 2009 International Test
Conference, 2009, pp. 1â€“10, http://dx.doi.org/10.1109/TEST.2009.5355620.
[349] L.-C. Wang, Data learning techniques for functional/system Fmax prediction,
in: 2009 24th IEEE International Symposium on Defect and Fault Tolerance in
VLSI Systems, 2009, p. 451, http://dx.doi.org/10.1109/DFT.2009.61.
[350] P. Krishnamurthy, A.B. Chowdhury, B. Tan, F. Khorrami, R. Karri, Explaining
and interpreting machine learning CAD decisions: An IC testing case study, in:
2020 ACM/IEEE 2nd Workshop on Machine Learning for CAD, MLCAD, 2020,
pp. 129â€“134, http://dx.doi.org/10.1145/3380446.3430643.
[351] S. Roy, S.K. Millican, V.D. Agrawal, Training neural network for machine
intelligence in automatic test pattern generator, in: 2021 34th International
Conference on VLSI Design and 2021 20th International Conference on
Embedded Systems, VLSID, 2021, pp. 316â€“321, http://dx.doi.org/10.1109/
VLSID51830.2021.00059.
[352] S. Roy, S.K. Millican, V.D. Agrawal, Multi-heuristic machine intelligence guidance in automatic test pattern generation, in: 2022 IEEE 31st Microelectronics
Design & Test Symposium, MDTS, 2022, pp. 1â€“6, http://dx.doi.org/10.1109/
MDTS54894.2022.9826985.
[353] S. Vasudevan, W.J. Jiang, D. Bieber, R. Singh, h. shojaei, C.R. Ho, C.
Sutton, Learning semantic representations to verify hardware designs, in: M.
Ranzato, A. Beygelzimer, Y. Dauphin, P. Liang, J.W. Vaughan (Eds.), Advances
in Neural Information Processing Systems, Vol. 34, Curran Associates, Inc,
2021, pp. 23491â€“23504, URL: https://proceedings.neurips.cc/paper/2021/file/
c5aa65949d20f6b20e1a922c13d974e7-Paper.pdf.
[354] T. Song, H. Liang, T. Ni, Z. Huang, Y. Lu, J. Wan, A. Yan, Pattern reorder
for test cost reduction through improved SVMRANK algorithm, IEEE Access 8
(2020) 147965â€“147972, http://dx.doi.org/10.1109/ACCESS.2020.3016039.
[355] T. Song, Z. Huang, A. Yan, Machine learning classification algorithm for
VLSI test cost reduction, Integration 87 (2022) 40â€“48, http://dx.doi.org/10.
1016/j.vlsi.2022.06.005, URL: https://www.sciencedirect.com/science/article/
pii/S0167926022000724.
[356] C.-Y. Chen, J.-L. Huang, Reinforcement-learning-based test program generation
for software-based self-test, in: 2019 IEEE 28th Asian Test Symposium, ATS,
2019, pp. 73â€“735, http://dx.doi.org/10.1109/ATS47505.2019.00013.
[357] Y. Maidon, B. Jervis, N. Dutton, S. Lesage, Diagnosis of multifaults in analogue
circuits using multilayer perceptrons, IEE Proc. Circuits, Dev. Syst. 144 (3)
(1997) 149â€“154.
[358] M.A. El-Gamal, M.A. El-Yazeed, A combined clustering and neural network
approach for analog multiple hard fault classification, J. Electron. Test. 14 (3)
(1999) 207â€“217, http://dx.doi.org/10.1023/A:1008353901973.
[359] F. Aminian, M. Aminian, Fault diagnosis of nonlinear analog circuits using
neural networks with wavelet and Fourier transforms as preprocessors, J.
Electron. Test. 17 (6) (2001) 471â€“481.
[360] M. Aminian, F. Aminian, A modular fault-diagnostic system for analog electronic circuits using neural networks with wavelet transform as a preprocessor,
IEEE Trans. Instrum. Meas. 56 (5) (2007) 1546â€“1554.
[361] A. DeOrio, Q. Li, M. Burgess, V. Bertacco, Machine learning-based anomaly
detection for post-silicon bug diagnosis, in: 2013 Design, Automation & Test in
Europe Conference & Exhibition, DATE, 2013, pp. 491â€“496, http://dx.doi.org/
10.7873/DATE.2013.112.
[362] Y. Huang, R. Guo, W.-T. Cheng, J.C.-M. Li, Survey of scan chain diagnosis,
IEEE Des. Test Comput. 25 (3) (2008) 240â€“248, http://dx.doi.org/10.1109/
MDT.2008.83.
[363] Y. Huang, B. Benware, R. Klingenberg, H. Tang, J. Dsouza, W.-T. Cheng, Scan
chain diagnosis based on unsupervised machine learning, in: 2017 IEEE 26th
Asian Test Symposium, ATS, 2017, pp. 225â€“230, http://dx.doi.org/10.1109/
ATS.2017.50.
[364] M. Chern, S.-W. Lee, S.-Y. Huang, Y. Huang, G. Veda, K.-H.H. Tsai, W.-T. Cheng,
Improving scan chain diagnostic accuracy using multi-stage artificial neural
networks, in: Proceedings of the 24th Asia and South Pacific Design Automation
Conference, ASPDAC â€™19, Association for Computing Machinery, New York, NY,
USA, 2019, pp. 341â€“346, http://dx.doi.org/10.1145/3287624.3287692.
[365] H. Lim, T.H. Kim, S. Kim, S. Kang, Diagnosis of scan chain faults based-on
machine-learning, in: 2020 International SoC Design Conference, ISOCC, 2020,
pp. 57â€“58, http://dx.doi.org/10.1109/ISOCC50952.2020.9333074.
[366] Z. Liu, Q. Huang, C. Fang, R.D. Blanton, Improving test chip design efficiency
via machine learning, in: 2019 IEEE International Test Conference, ITC, 2019,
pp. 1â€“10, http://dx.doi.org/10.1109/ITC44170.2019.9000131.
[367] Y.-C. Cheng, P.-Y. Tan, C.-W. Wu, M.-D. Shieh, C.-H. Chuang, G. Liao, A decision
tree-based screening method for improving test quality of memory chips, in:
2022 IEEE International Test Conference in Asia, ITC-Asia, 2022, pp. 19â€“24,
http://dx.doi.org/10.1109/ITCAsia55616.2022.00014.
[368] R. Sleik, M. Glavanovics, Y. Nikitin, M. Di Bernardo, A. Muetze, K. Krischan,
Performance enhancement of a modular test system for power semiconductors
for HTOL testing by use of an embedded system, in: 2017 19th European
Conference on Power Electronics and Applications, EPEâ€™17 ECCE Europe, 2017,
pp. P.1â€“P.8, http://dx.doi.org/10.23919/EPE17ECCEEurope.2017.8098933.
[369] C. Liu, J. Ou, Smart sampling for efficient system level test: A robust machine
learning approach, in: 2021 IEEE International Test Conference, ITC, 2021, pp.
53â€“62, http://dx.doi.org/10.1109/ITC50571.2021.00013.
[370] C. Fang, Q. Huang, R. Blanton, Adaptive test pattern reordering for diagnosis
using k-nearest neighbors, in: 2020 IEEE International Test Conference in
Asia, ITC-Asia, 2020, pp. 59â€“64, http://dx.doi.org/10.1109/ITC-Asia51099.
2020.00022.
[371] M. Liu, K. Chakrabarty, Adaptive methods for machine learning-based testing
of integrated circuits and boards, in: 2021 IEEE International Test Conference,
ITC, 2021, pp. 153â€“162, http://dx.doi.org/10.1109/ITC50571.2021.00023.
[372] A.B. Chowdhury, B. Tan, S. Garg, R. Karri, Robust deep learning for IC test
problems, IEEE Trans. Comput.-Aided Des. Integr. Circuits Syst. 41 (1) (2022)
183â€“195, http://dx.doi.org/10.1109/TCAD.2021.3054808.
[373] H. Amrouch, A.B. Chowdhury, W. Jin, R. Karri, F. Khorrami, P. Krishnamurthy,
I. Polian, V.M. van Santen, B. Tan, S.X.-D. Tan, Special session: Machine
learning for semiconductor test and reliability, in: 2021 IEEE 39th VLSI Test
Symposium, VTS, 2021, pp. 1â€“11, http://dx.doi.org/10.1109/VTS50974.2021.
9441052.
[374] S. Roy, S.K. Millican, V.D. Agrawal, Special session â€“ machine learning in test:
A survey of analog, digital, memory, and RF integrated circuits, in: 2021 IEEE
39th VLSI Test Symposium, VTS, 2021, pp. 1â€“14, http://dx.doi.org/10.1109/
VTS50974.2021.9441051.
[375] E. Sentovich, K. Singh, C. Moon, H. Savoj, R. Brayton, A. SangiovanniVincentelli, Sequential circuit design using synthesis and optimization, in:
Proceedings 1992 IEEE International Conference on Computer Design: VLSI
in Computers Processors, 1992, pp. 328â€“333, http://dx.doi.org/10.1109/ICCD.
1992.276282.
[376] B. Shakya, T. He, H. Salmani, D. Forte, S. Bhunia, M. Tehranipoor, Benchmarking of hardware trojans and maliciously affected circuits, J. Hardw. Syst. Secur.
1 (1) (2017) 85â€“102, http://dx.doi.org/10.1007/s41635-017-0001-6.
[377] IEEE Dataport, 201X, https://ieee-dataport.org/.
[378] E. Ãpek, S.A. McKee, R. Caruana, B.R. de Supinski, M. Schulz, Efficiently
exploring architectural design spaces via predictive modeling, in: Proceedings
of the 12th International Conference on Architectural Support for Programming
Languages and Operating Systems, in: ASPLOS XII, Association for Computing
Machinery, New York, NY, USA, 2006, pp. 195â€“206, http://dx.doi.org/10.1145/
1168857.1168882.
[379] H.-J. Yoo, Mobile/embedded DNN and AI SoCs, in: 2018 International Symposium on VLSI Design, Automation and Test, VLSI-DAT, 2018, p. 1, http:
//dx.doi.org/10.1109/VLSI-DAT.2018.8373285.
[380] J.-T. Kong, CAD for nanometer silicon design challenges and success, IEEE
Trans. Very Large Scale Integr. (VLSI) Syst. 12 (11) (2004) 1132â€“1147, http:
//dx.doi.org/10.1109/TVLSI.2004.836294.
[381] M.T. Bohr, Nanotechnology goals and challenges for electronic applications,
IEEE Trans. Nanotechnol. 1 (1) (2002) 56â€“62, http://dx.doi.org/10.1109/
TNANO.2002.1005426.
Integration 93 (2023) 102048
34
D. Amuru et al.
[382] Y.-W. Lin, Y.-B. Lin, C.-Y. Liu, AItalk: a tutorial to implement AI as IoT
devices, IET Netw. 8 (3) (2019) 195â€“202, http://dx.doi.org/10.1049/ietnet.2018.5182, URL: https://ietresearch.onlinelibrary.wiley.com/doi/abs/10.
1049/iet-net.2018.5182. arXiv:https://ietresearch.onlinelibrary.wiley.com/doi/
pdf/10.1049/iet-net.2018.5182.
[383] M. Song, K. Zhong, J. Zhang, Y. Hu, D. Liu, W. Zhang, J. Wang, T. Li, In-situ AI:
Towards autonomous and incremental deep learning for IoT systems, in: 2018
IEEE International Symposium on High Performance Computer Architecture,
HPCA, 2018, pp. 92â€“103, http://dx.doi.org/10.1109/HPCA.2018.00018.
[384] E. Eleftheriou, â€˜â€˜In-memory computingâ€™â€™: Accelerating AI applications, in: 2018
48th European Solid-State Device Research Conference, ESSDERC, 2018, pp.
4â€“5, http://dx.doi.org/10.1109/ESSDERC.2018.8486900.
[385] B. Yu, D.Z. Pan, T. Matsunawa, X. Zeng, Machine learning and pattern matching
in physical design, in: The 20th Asia and South Pacific Design Automation Conference, 2015, pp. 286â€“293, http://dx.doi.org/10.1109/ASPDAC.2015.
7059020.
[386] H. Iwai, K. Kakushima, H. Wong, Challenges for future semiconductor
manufacturing, Int. J. High Speed Electron. Syst. 16 (01) (2006) 43â€“81.
[387] Vandana, A. Singh, Multi-objective test case minimization using evolutionary
algorithms: A review, in: 2017 International Conference of Electronics, Communication and Aerospace Technology, Vol. 1, ICECA, 2017, pp. 329â€“334,
http://dx.doi.org/10.1109/ICECA.2017.8203698.
[388] V.N. Vapnik, The Nature of Statistical Learning Theory, Springer-Verlag, Berlin,
Heidelberg, 1995.
[389] S. LathuiliÃ¨re, P. Mesejo, X. Alameda-Pineda, R. Horaud, A comprehensive
analysis of deep regression, IEEE Trans. Pattern Anal. Mach. Intell. 42 (9)
(2020) 2065â€“2081, http://dx.doi.org/10.1109/TPAMI.2019.2910523.
[390] S. Obilisetty, Digital intelligence and chip design, in: 2018 International
Symposium on VLSI Design, Automation and Test, VLSI-DAT, 2018, pp. 1â€“4,
http://dx.doi.org/10.1109/VLSI-DAT.2018.8373256.
[391] M. Shafique, R. Hafiz, M.U. Javed, S. Abbas, L. Sekanina, Z. Vasicek, V. Mrazek,
Adaptive and energy-efficient architectures for machine learning: Challenges,
opportunities, and research roadmap, in: 2017 IEEE Computer Society Annual
Symposium on VLSI, ISVLSI, 2017, pp. 627â€“632, http://dx.doi.org/10.1109/
ISVLSI.2017.124.
